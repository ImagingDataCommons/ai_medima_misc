{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImagingDataCommons/ai_medima_misc/blob/main/nnunet/notebooks/inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pmb_HiJv7Sit"
      },
      "source": [
        "# **IDC nnUNet Use-case: Data Inference**\n",
        "\n",
        "... and Brief Description here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZnoRi9Y7nEB"
      },
      "source": [
        "## **Environment Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "non5qVLIcG4M",
        "outputId": "2ba3bab9-e8da-42c1-c2ee-80e71429328f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar 21 10:11:22 2022\n",
            "\n",
            "Current directory : /content\n",
            "Hostname          : 1ae7446a8879\n",
            "Username          : root\n",
            "Python version    : 3.7.12 (default, Jan 15 2022, 18:48:18) \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import yaml\n",
        "\n",
        "import time\n",
        "import tqdm\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# useful information\n",
        "curr_dir = !pwd\n",
        "curr_droid = !hostname\n",
        "curr_pilot = !whoami\n",
        "\n",
        "print(time.asctime(time.localtime()))\n",
        "\n",
        "print(\"\\nCurrent directory :\", curr_dir[-1])\n",
        "print(\"Hostname          :\", curr_droid[-1])\n",
        "print(\"Username          :\", curr_pilot[-1])\n",
        "\n",
        "print(\"Python version    :\", sys.version.split('\\n')[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2DMUqTOVF5WP"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9Y43S1F35h8m"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "bucket_name = 'idc-medima-paper'\n",
        "project_name = \"idc-sandbox-000\"\n",
        "\n",
        "# location where to store the data (and check if a patient was processed already)\n",
        "# if a patient was processed already, copy over the segmentation and run only\n",
        "# the post-processing (split the masks, etc.)\n",
        "bucket_base_uri = \"gs://%s/\"%(bucket_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1GgmTW5Mq6A",
        "outputId": "c0bab501-9f03-4e8c-d5cd-16c9337048b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'src/dicomsort'...\n",
            "remote: Enumerating objects: 126, done.\u001b[K\n",
            "remote: Total 126 (delta 0), reused 0 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (126/126), 37.03 KiB | 6.17 MiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n",
            "Cloning into 'src/pyplastimatch'...\n",
            "remote: Enumerating objects: 333, done.\u001b[K\n",
            "remote: Counting objects: 100% (333/333), done.\u001b[K\n",
            "remote: Compressing objects: 100% (314/314), done.\u001b[K\n",
            "remote: Total 333 (delta 31), reused 302 (delta 12), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (333/333), 55.56 MiB | 26.22 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p src\n",
        "\n",
        "!git clone https://github.com/pieper/dicomsort src/dicomsort\n",
        "!git clone https://github.com/AIM-Harvard/pyplastimatch src/pyplastimatch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT7coYaGfd4t"
      },
      "source": [
        "Install Plastimatch [...] and check the process was successful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6NLhlFqyEgBf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install plastimatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ILwxgbcfcpr",
        "outputId": "9cf5923c-a84b-4386-8bf2-078ac7127daa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "plastimatch version 1.7.0\n"
          ]
        }
      ],
      "source": [
        "# check plastimatch was correctly installed\n",
        "!plastimatch --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ztrKrzSm1WT"
      },
      "source": [
        "Download and unpack DCMQI:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYf72nA8m0wh",
        "outputId": "43b1a962-e4dc-48b2-8cca-f5e56d47da01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-21 10:12:30--  https://github.com/QIICR/dcmqi/releases/download/v1.2.4/dcmqi-1.2.4-linux.tar.gz\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/50675718/04f07880-81ee-11eb-92ec-30c7426dae5d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220321%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220321T101231Z&X-Amz-Expires=300&X-Amz-Signature=f45133540b1c1ea10740a51cfa858f7ec5a44061049e866d4490ea3fafb237bb&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=50675718&response-content-disposition=attachment%3B%20filename%3Ddcmqi-1.2.4-linux.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-03-21 10:12:31--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/50675718/04f07880-81ee-11eb-92ec-30c7426dae5d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220321%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220321T101231Z&X-Amz-Expires=300&X-Amz-Signature=f45133540b1c1ea10740a51cfa858f7ec5a44061049e866d4490ea3fafb237bb&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=50675718&response-content-disposition=attachment%3B%20filename%3Ddcmqi-1.2.4-linux.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21561544 (21M) [application/octet-stream]\n",
            "Saving to: ‘/content/dcmqi-1.2.4-linux.tar.gz’\n",
            "\n",
            "/content/dcmqi-1.2. 100%[===================>]  20.56M  12.0MB/s    in 1.7s    \n",
            "\n",
            "2022-03-21 10:12:33 (12.0 MB/s) - ‘/content/dcmqi-1.2.4-linux.tar.gz’ saved [21561544/21561544]\n",
            "\n",
            "dcmqi-1.2.4-linux/share/\n",
            "dcmqi-1.2.4-linux/share/doc/\n",
            "dcmqi-1.2.4-linux/share/doc/ITK-4.10/\n",
            "dcmqi-1.2.4-linux/share/doc/ITK-4.10/itksys/\n",
            "dcmqi-1.2.4-linux/share/doc/ITK-4.10/itksys/Copyright.txt\n",
            "dcmqi-1.2.4-linux/bin/\n",
            "dcmqi-1.2.4-linux/bin/itkimage2segimage.xml\n",
            "dcmqi-1.2.4-linux/bin/itkimage2paramap\n",
            "dcmqi-1.2.4-linux/bin/segimage2itkimage.xml\n",
            "dcmqi-1.2.4-linux/bin/tid1500reader.xml\n",
            "dcmqi-1.2.4-linux/bin/paramap2itkimage\n",
            "dcmqi-1.2.4-linux/bin/segimage2itkimage\n",
            "dcmqi-1.2.4-linux/bin/tid1500writer\n",
            "dcmqi-1.2.4-linux/bin/paramap2itkimage.xml\n",
            "dcmqi-1.2.4-linux/bin/itkimage2segimage\n",
            "dcmqi-1.2.4-linux/bin/tid1500writer.xml\n",
            "dcmqi-1.2.4-linux/bin/tid1500reader\n",
            "dcmqi-1.2.4-linux/bin/itkimage2paramap.xml\n"
          ]
        }
      ],
      "source": [
        "# FIXME: always parse the latest?\n",
        "dcmqi_release_url = \"https://github.com/QIICR/dcmqi/releases/download/v1.2.4/dcmqi-1.2.4-linux.tar.gz\"\n",
        "dcmqi_download_path = \"/content/dcmqi-1.2.4-linux.tar.gz\"\n",
        "dcmqi_path = \"/content/dcmqi-1.2.4-linux\"\n",
        "\n",
        "!wget -O $dcmqi_download_path $dcmqi_release_url\n",
        "\n",
        "!tar -xvf $dcmqi_download_path\n",
        "\n",
        "!mv $dcmqi_path/bin/* /bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JBJmF7rmz1S"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CthT1kRuywEh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pydicom SimpleITK nnunet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nKUQPEaHFxne"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import SimpleITK as sitk\n",
        "\n",
        "import src.pyplastimatch.pyplastimatch.pyplastimatch as pypla\n",
        "\n",
        "from google.cloud import bigquery as bq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0dkieZXt-lUC"
      },
      "outputs": [],
      "source": [
        "# FIXME: for development purposes - we will switch to a proper query soon!\n",
        "\n",
        "# name of the project\n",
        "PROJECT_NAME = \"idc-sandbox-000\"\n",
        "\n",
        "# name of the BQ dataset where the table is copied\n",
        "DATASET_NAME = \"dennis_cohorts\"\n",
        "\n",
        "# name of the BQ table copied from IDC to the user's own project\n",
        "TABLE_NAME = \"nsclc-nnunet-he\"\n",
        "\n",
        "# Table ID to use with the BQ command\n",
        "TABLE_ID = \"%s.%s.%s\"%(PROJECT_NAME, DATASET_NAME, TABLE_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m9b0Zbzri0DU"
      },
      "outputs": [],
      "source": [
        "# the query we are going to execute to gather data about the selected cohort\n",
        "query_str = \"SELECT * FROM `%s`\"%(TABLE_ID)\n",
        "\n",
        "# init the BQ client\n",
        "client = bq.Client(project = \"idc-sandbox-000\")\n",
        "\n",
        "# run the query\n",
        "query_job = client.query(query_str)\n",
        "\n",
        "# convert the results to a Pandas dataframe\n",
        "cohort_df = query_job.to_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GNutD-SRN52T"
      },
      "outputs": [],
      "source": [
        "# create the directory tree\n",
        "!mkdir -p data models output\n",
        "\n",
        "!mkdir -p data/raw \n",
        "!mkdir -p data/raw/tmp data/raw/nsclc-radiomics\n",
        "!mkdir -p data/raw/nsclc-radiomics/dicom\n",
        "\n",
        "!mkdir -p data/processed\n",
        "!mkdir -p data/processed/nsclc-radiomics\n",
        "!mkdir -p data/processed/nsclc-radiomics/nrrd\n",
        "!mkdir -p data/processed/nsclc-radiomics/nii\n",
        "!mkdir -p data/processed/nsclc-radiomics/dicomseg\n",
        "\n",
        "!mkdir -p data/model_input/\n",
        "!mkdir -p data/nnunet_output/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1GXUtRRrIlT"
      },
      "source": [
        "Copy the JSON metadata file (generated using [...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQiQ9Zw9rMoD",
        "outputId": "71cfdf50-5653-49ac-c4a6-e4ead9aab505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://idc-medima-paper/nnunet/data/dicomseg_metadata.json...\n",
            "/ [1 files][  2.6 KiB/  2.6 KiB]                                                \n",
            "Operation completed over 1 objects/2.6 KiB.                                      \n"
          ]
        }
      ],
      "source": [
        "bucket_data_base_uri = os.path.join(bucket_base_uri, \"nnunet/data\")\n",
        "dicomseg_json_uri = os.path.join(bucket_data_base_uri, \"dicomseg_metadata.json\")\n",
        "dicomseg_json_path = \"/content/data/dicomseg_metadata.json\"\n",
        "\n",
        "!gsutil cp $dicomseg_json_uri $dicomseg_json_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7y3FRa7rbyr"
      },
      "source": [
        "Download the segmentation models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ79vTL5ef11",
        "outputId": "bce60f05-0994-4a5f-f67f-a0e09db12761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-21 10:13:07--  https://www.dropbox.com/s/m7es2ojn8h0ybhv/Task055_SegTHOR.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/m7es2ojn8h0ybhv/Task055_SegTHOR.zip [following]\n",
            "--2022-03-21 10:13:08--  https://www.dropbox.com/s/raw/m7es2ojn8h0ybhv/Task055_SegTHOR.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc956005e362f2400eb0bcd7b531.dl.dropboxusercontent.com/cd/0/inline/Bh5Jpco9bWMiRJLoXHxGOQ7hVEfkecnyja49xYtrafhAJ1mfBkf3GKq1DCtWSjsqH787JdRk_QfxdOULcJQJ6T32xa9olqC4c3zPuWx42ImrWhOsWZq8pXeCet5LoTae92ied9Qvhk2vuY7yiDKO3es5vez5eDl-d6V8seMjW0OnMg/file# [following]\n",
            "--2022-03-21 10:13:08--  https://uc956005e362f2400eb0bcd7b531.dl.dropboxusercontent.com/cd/0/inline/Bh5Jpco9bWMiRJLoXHxGOQ7hVEfkecnyja49xYtrafhAJ1mfBkf3GKq1DCtWSjsqH787JdRk_QfxdOULcJQJ6T32xa9olqC4c3zPuWx42ImrWhOsWZq8pXeCet5LoTae92ied9Qvhk2vuY7yiDKO3es5vez5eDl-d6V8seMjW0OnMg/file\n",
            "Resolving uc956005e362f2400eb0bcd7b531.dl.dropboxusercontent.com (uc956005e362f2400eb0bcd7b531.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to uc956005e362f2400eb0bcd7b531.dl.dropboxusercontent.com (uc956005e362f2400eb0bcd7b531.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Bh5W9PFdOqRQdr51dqUgmRrNYu07v8TKQSFHfxTauRjzSmT9PikhsPLy-JX7YToSe_Gq1F9OPCoe7AtEpw_1OnZ4ADm489Jrgk6bShUAE51K250adubR_pwPMW40a3ksjpQ5cdQVJVjwyOYV2_m9Q5mqkzzrJj3kEtJ3Ii3q2FxYIxiCq1FSbn4Wt0wKdrz4ZEGQGv5QTmDLuSoEyOti7iWl5S8FK8yd9RMCS-fGxrPQ6uYN3SFhCR-Csf6BWuPsCDJO2vmfEN8T0RMXYz0Atj522kfZi_S8ilK2xXdlQNzEWiuoR0q1_F_4YLsiy6NvtLZaLlJl5rgvXd46Gh5DN-xr82GTbAVnTfto5WY0qZnKcjYdY_DjJVe8b8g-_KA6DTmNcEn_Uj3q1Bgm_PYIJB6EBnu-bvLlwavLkGta6IN7zg/file [following]\n",
            "--2022-03-21 10:13:08--  https://uc956005e362f2400eb0bcd7b531.dl.dropboxusercontent.com/cd/0/inline2/Bh5W9PFdOqRQdr51dqUgmRrNYu07v8TKQSFHfxTauRjzSmT9PikhsPLy-JX7YToSe_Gq1F9OPCoe7AtEpw_1OnZ4ADm489Jrgk6bShUAE51K250adubR_pwPMW40a3ksjpQ5cdQVJVjwyOYV2_m9Q5mqkzzrJj3kEtJ3Ii3q2FxYIxiCq1FSbn4Wt0wKdrz4ZEGQGv5QTmDLuSoEyOti7iWl5S8FK8yd9RMCS-fGxrPQ6uYN3SFhCR-Csf6BWuPsCDJO2vmfEN8T0RMXYz0Atj522kfZi_S8ilK2xXdlQNzEWiuoR0q1_F_4YLsiy6NvtLZaLlJl5rgvXd46Gh5DN-xr82GTbAVnTfto5WY0qZnKcjYdY_DjJVe8b8g-_KA6DTmNcEn_Uj3q1Bgm_PYIJB6EBnu-bvLlwavLkGta6IN7zg/file\n",
            "Reusing existing connection to uc956005e362f2400eb0bcd7b531.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5019434005 (4.7G) [application/zip]\n",
            "Saving to: ‘/content/models/Task055_SegTHOR.zip’\n",
            "\n",
            "/content/models/Tas 100%[===================>]   4.67G  63.6MB/s    in 69s     \n",
            "\n",
            "2022-03-21 10:14:18 (69.3 MB/s) - ‘/content/models/Task055_SegTHOR.zip’ saved [5019434005/5019434005]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# FIXME: download from pvt Dropbox to speed up the development\n",
        "#        the final notebook should use the official resources only (Zenodo)\n",
        "seg_model_url = \"https://www.dropbox.com/s/m7es2ojn8h0ybhv/Task055_SegTHOR.zip?dl=0\"\n",
        "model_download_path = \"/content/models/Task055_SegTHOR.zip\"\n",
        "\n",
        "!wget -O $model_download_path $seg_model_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bne4bNo5fAsx"
      },
      "source": [
        "Initialize a few environment variables [...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IIIjeFdUe1EA"
      },
      "outputs": [],
      "source": [
        "os.environ[\"RESULTS_FOLDER\"] = \"/content/data/nnunet_output/\"\n",
        "os.environ[\"WEIGHTS_FOLDER\"] = \"/content/data/nnunet_output/nnUNet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Wx8bWGVvew3P"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!nnUNet_install_pretrained_model_from_zip $model_download_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Z2S0OdAtkz"
      },
      "source": [
        "---\n",
        "\n",
        "# **Function Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtbQY2pf6UlF"
      },
      "source": [
        "## **Data Download and Preparation**\n",
        "\n",
        "The following function handles the download of a single patient data from the IDC buckets using `gsutil cp`. Furthermore, to organise the data in a more human-understandable and, above all, standardized fashion, the function makes use of [DICOMSort](https://github.com/pieper/dicomsort).\n",
        "\n",
        "DICOMSort is an open source tool for custom sorting and renaming of dicom files based on their specific DICOM tags. In our case, we will exploit DICOMSort to organise the DICOM data by `PatientID` and `Modality` - so that the final directory will look like the following:\n",
        "\n",
        "```\n",
        "raw/nsclc-radiomics/dicom/$PatientID\n",
        " └─── CT\n",
        "       ├─── $SOPInstanceUID_slice0.dcm\n",
        "       ├─── $SOPInstanceUID_slice1.dcm\n",
        "       ├───  ...\n",
        "       │\n",
        "      RTSTRUCT \n",
        "       ├─── $SOPInstanceUID_RTSTRUCT.dcm\n",
        "      SEG\n",
        "       └─── $SOPInstanceUID_RTSEG.dcm\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mAkq03N9A0np"
      },
      "outputs": [],
      "source": [
        "def download_patient_data(raw_base_path, sorted_base_path,\n",
        "                          patient_df, remove_raw = True):\n",
        "\n",
        "  \"\"\"\n",
        "  Download raw DICOM data and run dicomsort to standardise the input format.\n",
        "\n",
        "  Arguments:\n",
        "    raw_base_path    : required - path to the folder where the raw data will be stored.\n",
        "    sorted_base_path : required - path to the folder where the sorted data will be stored.\n",
        "    patient_df       : required - Pandas dataframe (returned from BQ) storing all the\n",
        "                                  patient information required to pull data from the IDC buckets.\n",
        "    remove_raw       : optional - whether to remove or not the raw non-sorted data\n",
        "                                  (after sorting with dicomsort). Defaults to True.\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # FIXME: this gets overwritten every single time; use `tempfile` library?\n",
        "  gs_file_path = \"gcs_paths.txt\"\n",
        "  patient_df[\"gcs_url\"].to_csv(gs_file_path, header = False, index = False)\n",
        "\n",
        "  pat_id = patient_df[\"PatientID\"].values[0]\n",
        "  download_path = os.path.join(raw_base_path, pat_id)\n",
        "\n",
        "  if not os.path.exists(download_path):\n",
        "    os.mkdir(download_path)\n",
        "\n",
        "  # FIXME: ok for a notebook; for scripting, change this to `subprocess`\n",
        "\n",
        "  start_time = time.time()\n",
        "  print(\"Copying files from IDC buckets to %s...\"%(download_path))\n",
        "  !cat $gs_file_path | gsutil -q -m cp -Ir $download_path\n",
        "  elapsed = time.time() - start_time\n",
        "  print(\"Done in %g seconds.\"%elapsed)\n",
        "\n",
        "  start_time = time.time()\n",
        "  print(\"\\nSorting DICOM files...\" )\n",
        "  !python src/dicomsort/dicomsort.py -u $download_path $sorted_base_path/%PatientID/%Modality/%SOPInstanceUID.dcm\n",
        "  elapsed = time.time() - start_time\n",
        "  print(\"Done in %g seconds.\"%elapsed)\n",
        "\n",
        "  print(\"Sorted DICOM data saved at: %s\"%(os.path.join(sorted_base_path, pat_id)))\n",
        "\n",
        "  # get rid of the temporary folder, storing the unsorted DICOM data \n",
        "  if remove_raw:\n",
        "    print(\"Removing un-sorted data at %s...\"%(download_path))\n",
        "    !rm -r $download_path\n",
        "    print(\"... Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLGqxh6063lE"
      },
      "source": [
        "---\n",
        "\n",
        "## **Data Preprocessing**\n",
        "\n",
        "Brief description here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "S1L6gQ0A_gr_"
      },
      "outputs": [],
      "source": [
        "def pypla_dicom_ct_to_nrrd(sorted_base_path, processed_nrrd_path,\n",
        "                           pat_id, verbose = True):\n",
        "  \n",
        "  \"\"\"\n",
        "  Sorted DICOM patient data to NRRD file (CT volume).\n",
        "\n",
        "  Arguments:\n",
        "    sorted_base_path    : required - path to the folder where the sorted data should be stored.\n",
        "    processed_nrrd_path : required - path to the folder where the preprocessed NRRD data are stored\n",
        "    remove_raw          : required - patient ID (used for naming purposes).\n",
        "    verbose             : optional - whether to run pyplastimatch in verbose mode. Defaults to true.\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # given that everything is standardised already, compute the paths\n",
        "  path_to_dicom_ct_folder = os.path.join(sorted_base_path, pat_id, \"CT\")\n",
        "  \n",
        "  # sanity check\n",
        "  assert(os.path.exists(path_to_dicom_ct_folder))\n",
        "  \n",
        "  pat_dir_nrrd_path = os.path.join(processed_nrrd_path, pat_id)\n",
        "  if not os.path.exists(pat_dir_nrrd_path):\n",
        "    os.mkdir(pat_dir_nrrd_path)\n",
        "\n",
        "  # output NRRD CT\n",
        "  ct_nrrd_path = os.path.join(pat_dir_nrrd_path, pat_id + \"_CT.nrrd\")\n",
        "\n",
        "  # logfile for the plastimatch conversion\n",
        "  log_file_path = os.path.join(pat_dir_nrrd_path, pat_id + '_pypla.log')\n",
        "\n",
        "  # DICOM CT to NRRD conversion (if the file doesn't exist yet)\n",
        "  if not os.path.exists(ct_nrrd_path):\n",
        "    convert_args_ct = {\"input\" : path_to_dicom_ct_folder,\n",
        "                       \"output-img\" : ct_nrrd_path}\n",
        "\n",
        "    # clean old log file if it exist\n",
        "    if os.path.exists(log_file_path): os.remove(log_file_path)\n",
        "    \n",
        "    pypla.convert(verbose = verbose,\n",
        "                  path_to_log_file = log_file_path,\n",
        "                  **convert_args_ct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIs8aIubGq9X"
      },
      "source": [
        "---\n",
        "\n",
        "Brief description here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1hBTKAfp_bnM"
      },
      "outputs": [],
      "source": [
        "def pypla_dicom_ct_to_nifti(sorted_base_path, processed_nifti_path,\n",
        "                            pat_id, verbose = True):\n",
        "  \n",
        "  \"\"\"\n",
        "  Sorted DICOM patient data to NIfTI file (CT volume).\n",
        "\n",
        "  Arguments:\n",
        "    sorted_base_path     : required - path to the folder where the sorted data should be stored.\n",
        "    processed_nifti_path : required - path to the folder where the preprocessed NIfTI data are stored\n",
        "    remove_raw           : required - patient ID (used for naming purposes).\n",
        "    verbose              : optional - whether to run pyplastimatch in verbose mode. Defaults to true.\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # given that everything is standardised already, compute the paths\n",
        "  path_to_dicom_ct_folder = os.path.join(sorted_base_path, pat_id, \"CT\")\n",
        "  \n",
        "  # sanity check\n",
        "  assert(os.path.exists(path_to_dicom_ct_folder))\n",
        "  \n",
        "  pat_dir_nifti_path = os.path.join(processed_nifti_path, pat_id)\n",
        "  if not os.path.exists(pat_dir_nifti_path):\n",
        "    os.mkdir(pat_dir_nifti_path)\n",
        "\n",
        "  # output NRRD CT\n",
        "  ct_nifti_path = os.path.join(pat_dir_nifti_path, pat_id + \"_CT.nii.gz\")\n",
        "\n",
        "  # logfile for the plastimatch conversion\n",
        "  log_file_path = os.path.join(pat_dir_nifti_path, pat_id + '_pypla.log')\n",
        "\n",
        "  # DICOM CT to NRRD conversion (if the file doesn't exist yet)\n",
        "  if not os.path.exists(ct_nifti_path):\n",
        "    convert_args_ct = {\"input\" : path_to_dicom_ct_folder,\n",
        "                       \"output-img\" : ct_nifti_path}\n",
        "\n",
        "    # clean old log file if it exist\n",
        "    if os.path.exists(log_file_path): os.remove(log_file_path)\n",
        "    \n",
        "    pypla.convert(verbose = verbose,\n",
        "                  path_to_log_file = log_file_path,\n",
        "                  **convert_args_ct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSMNA0ESH3O6"
      },
      "source": [
        "---\n",
        "\n",
        "Brief description here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LVeSbb7-_kXS"
      },
      "outputs": [],
      "source": [
        "def pypla_dicom_rtstruct_to_nrrd(sorted_base_path, processed_nrrd_path,\n",
        "                                 pat_id, verbose = True):\n",
        "  \n",
        "  \"\"\"\n",
        "  Sorted DICOM patient data to NRRD file (RTSTRUCT).\n",
        "\n",
        "  Arguments:\n",
        "    sorted_base_path    : required - path to the folder where the sorted data should be stored.\n",
        "    processed_nrrd_path : required - path to the folder where the preprocessed NRRD data are stored\n",
        "    remove_raw          : required - patient ID (used for naming purposes).\n",
        "    verbose             : optional - whether to run pyplastimatch in verbose mode. Defaults to true.\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # given that everything is standardised already, compute the paths\n",
        "  path_to_dicom_ct_folder = os.path.join(sorted_base_path, pat_id, \"CT\")\n",
        "  path_to_dicom_rt_folder = os.path.join(sorted_base_path, pat_id, \"RTSTRUCT\")\n",
        "\n",
        "  pat_dir_nrrd_path = os.path.join(processed_nrrd_path, pat_id)\n",
        "\n",
        "  # sanity check\n",
        "  assert(os.path.exists(path_to_dicom_rt_folder))\n",
        "  assert(os.path.exists(pat_dir_nrrd_path))\n",
        "\n",
        "  # output NRRD CT\n",
        "  rt_folder_path = os.path.join(pat_dir_nrrd_path, \"rt_segmasks\")\n",
        "  rt_list_path = os.path.join(rt_folder_path, pat_id + \"_rt_list.txt\")\n",
        "\n",
        "  # path to the file storing the names of the exported segmentation masks\n",
        "  # (from the DICOM RTSTRUCT)\n",
        "  log_file_path = os.path.join(pat_dir_nrrd_path, pat_id + '_pypla.log')\n",
        "\n",
        "  # DICOM CT to NRRD conversion (if the file doesn't exist yet)\n",
        "  if not os.path.exists(rt_folder_path):\n",
        "    convert_args_rt = {\"input\" : path_to_dicom_rt_folder, \n",
        "                       \"referenced-ct\" : path_to_dicom_ct_folder,\n",
        "                       \"output-prefix\" : rt_folder_path,\n",
        "                       \"prefix-format\" : 'nrrd',\n",
        "                       \"output-ss-list\" : rt_list_path}\n",
        "\n",
        "    \n",
        "    pypla.convert(verbose = verbose,\n",
        "                  path_to_log_file = log_file_path,\n",
        "                  **convert_args_rt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQoFWJasfror"
      },
      "source": [
        "---\n",
        "\n",
        "Brief description here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9I0X9fFNfvWG"
      },
      "outputs": [],
      "source": [
        "def prep_input_data(processed_nifti_path, model_input_folder, pat_id):\n",
        "  \n",
        "  \"\"\"\n",
        "  Sorted DICOM patient data to NRRD file (RTSTRUCT).\n",
        "\n",
        "  Arguments:\n",
        "    src_folder : required - path to the folder where the sorted data should be stored.\n",
        "    dst_folder : required - path to the folder where the preprocessed NRRD data are stored\n",
        "    pat_id     : required - patient ID (used for naming purposes).\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # FIXME: ok for a notebook; for scripting, change this to `shutil`\n",
        "\n",
        "  pat_dir_nifti_path = os.path.join(processed_nifti_path, pat_id)\n",
        "  ct_nifti_path = os.path.join(pat_dir_nifti_path, pat_id + \"_CT.nii.gz\")\n",
        "  \n",
        "  copy_to_path = os.path.join(model_input_folder, pat_id + \"_0000.nii.gz\")\n",
        "    \n",
        "  # copy NIfTI to the right dir for nnU-Net processing\n",
        "  if not os.path.exists(copy_to_path):\n",
        "    print(\"Copying %s\\nto %s...\"%(ct_nifti_path, copy_to_path))\n",
        "    !cp $ct_nifti_path $copy_to_path\n",
        "    print(\"... Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-7IE4iEIBpD"
      },
      "source": [
        "---\n",
        "\n",
        "## **Data Processing**\n",
        "\n",
        "Brief description here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZINfBYSxIKH8"
      },
      "outputs": [],
      "source": [
        "def process_patient_nnunet(model_input_folder, model_output_folder, \n",
        "                           nnunet_model, use_tta = False, export_prob_maps = False,\n",
        "                           verbose = False):\n",
        "\n",
        "  \"\"\"\n",
        "  Infer the thoracic organs at risk segmentation maps using one of the nnU-Net models.\n",
        "\n",
        "  Arguments:\n",
        "    model_input_folder  : required - path to the folder where the data to be inferred should be stored.\n",
        "    model_output_folder : required - path to the folder where the inferred segmentation masks will be stored.\n",
        "    nnunet_model        : required - pre-trained nnU-Net model to use during the inference phase.\n",
        "    use_tta             : optional - whether to use or not test time augmentation (TTA). Defaults to False.\n",
        "    export_prob_maps    : optional - whether to export or not softmax probabilities. Defaults to False.\n",
        "    verbose             : optional - whether to output text from `nnUNet_predict` or not. Defaults to False.\n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "  \n",
        "  export_prob_maps = \"--save_npz\" if export_prob_maps == True else \"\"\n",
        "  direct_to = \"\" if verbose == True else \"> /dev/null\"\n",
        "  use_tta = \"\" if use_tta == True else \"--disable_tta\"\n",
        "\n",
        "  assert(nnunet_model in [\"2d\", \"3d_lowres\", \"3d_fullres\", \"3d_cascade_fullres\"])\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  print(\"Running `nnUNet_predict` with `%s` model...\"%(nnunet_model))\n",
        "\n",
        "  pat_fn_list = sorted([f for f in os.listdir(model_input_folder) if \".nii.gz\" in f])\n",
        "  pat_fn_path = os.path.join(model_input_folder, pat_fn_list[-1])\n",
        "\n",
        "  print(\"Processing file at %s...\"%(pat_fn_path))\n",
        "\n",
        "  # run the inference phase\n",
        "  # accepted options for --model are: 2d, 3d_lowres, 3d_fullres or 3d_cascade_fullres\n",
        "  !nnUNet_predict --input_folder $model_input_folder \\\n",
        "                  --output_folder $model_output_folder \\\n",
        "                  --task_name \"Task055_SegTHOR\" \\\n",
        "                  --model $nnunet_model $use_tta $direct_to $export_prob_maps\n",
        "\n",
        "  elapsed = time.time() - start_time\n",
        "\n",
        "  print(\"Done in %g seconds.\"%elapsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iENa_4flTd1m"
      },
      "source": [
        "---\n",
        "\n",
        "## **Data Postprocessing**\n",
        "\n",
        "Description here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jHe5LSJ1f8pv"
      },
      "outputs": [],
      "source": [
        "def pypla_nifti_to_nrrd(pred_nifti_path, processed_nrrd_path,\n",
        "                        pat_id, verbose = True):\n",
        "  \n",
        "  \"\"\"\n",
        "  Sorted DICOM patient data to NRRD file (RTSTRUCT).\n",
        "\n",
        "  Arguments:\n",
        "    src_folder : required - path to the folder where the sorted data should be stored.\n",
        "    dst_folder : required - path to the folder where the preprocessed NRRD data are stored\n",
        "    pat_id     : required - patient ID (used for naming purposes).\n",
        "  \n",
        "  Returns:\n",
        "    pred_nrrd_path - \n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  pred_nrrd_path = os.path.join(processed_nrrd_path, pat_id, pat_id + \"_pred_segthor.nrrd\")\n",
        "  log_file_path = os.path.join(processed_nrrd_path, pat_id, pat_id + \"_pypla.log\")\n",
        "  \n",
        "  # Inferred NIfTI segmask to NRRD\n",
        "  convert_args_pred = {\"input\" : pred_nifti_path, \n",
        "                       \"output-img\" : pred_nrrd_path}\n",
        "\n",
        "  pypla.convert(verbose = verbose,\n",
        "                path_to_log_file = log_file_path,\n",
        "                **convert_args_pred)\n",
        "  \n",
        "  return pred_nrrd_path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Description here."
      ],
      "metadata": {
        "id": "t9XIdgwicapX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cTFv-slKU039"
      },
      "outputs": [],
      "source": [
        "def pypla_postprocess(processed_nrrd_path, model_output_folder, pat_id):\n",
        "\n",
        "  \"\"\"\n",
        "  Sorted DICOM patient data to NRRD file (RTSTRUCT).\n",
        "\n",
        "  Arguments:\n",
        "    processed_nrrd_path  : required - path to the folder where the sorted data should be stored.\n",
        "    model_output_folder  : required - path to the folder where the inferred segmentation masks should be stored.\n",
        "    pat_id               : required - patient ID (used for naming purposes). \n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  pred_nifti_fn = pat_id + \".nii.gz\"\n",
        "  pred_nifti_path = os.path.join(model_output_folder, pred_nifti_fn)\n",
        "\n",
        "  # parse NRRD file - we will make use of if to populate the header of the\n",
        "  # NRRD mask we are going to get from the inferred segmentation mask\n",
        "  ct_nrrd_path = os.path.join(processed_nrrd_path, pat_id, pat_id + \"_CT.nrrd\")\n",
        "  sitk_ct = sitk.ReadImage(ct_nrrd_path)\n",
        "\n",
        "  pred_nrrd_path = pypla_nifti_to_nrrd(pred_nifti_path = pred_nifti_path,\n",
        "                                       processed_nrrd_path = processed_nrrd_path,\n",
        "                                       pat_id = pat_id, verbose = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Description here."
      ],
      "metadata": {
        "id": "Wvf4yN-DcT3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eVYThboonIpe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numpy_to_nrrd(model_output_folder, processed_nrrd_path, pat_id,\n",
        "                  output_folder_name = \"pred_softmax\", output_dtype = \"uint8\",\n",
        "                  structure_list = [\"Background\", \"Esophagus\",\n",
        "                                    \"Heart\", \"Trachea\", \"Aorta\"]):\n",
        "\n",
        "  \"\"\"\n",
        "  Convert softmax probability maps to NRRD. For simplicity, the probability maps\n",
        "  are converted by default to UInt8\n",
        "\n",
        "  Arguments:\n",
        "    model_output_folder : required - path to the folder where the inferred segmentation masks should be stored.\n",
        "    processed_nrrd_path : required - path to the folder where the preprocessed NRRD data are stored.\n",
        "    pat_id              : required - patient ID (used for naming purposes).\n",
        "    output_folder_name  : optional - name of the subfolder under the patient directory \n",
        "                                     (under `processed_nrrd_path`) where the softmax NRRD\n",
        "                                     files will be saved. Defaults to \"pred_softmax\".\n",
        "    output_dtype        : optional - output data type. Float16 is not supported by the NRRD standard,\n",
        "                                     so the choice should be between uint8, uint16 or float32.\n",
        "                                     Please note this will greatly impact the size of the DICOM PM\n",
        "                                     file that will be generated.\n",
        "    structure_list      : optional - list of the structures whose probability maps are stored in the \n",
        "                                     first channel of the `.npz` file (output from the nnU-Net pipeline\n",
        "                                     when `export_prob_maps` is set to True). Defaults to the structure\n",
        "                                     list for the SegTHOR challenge (background = 0 included).\n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  pred_softmax_fn = pat_id + \".npz\"\n",
        "  pred_softmax_path = os.path.join(model_output_folder, pred_softmax_fn)\n",
        "\n",
        "  # parse NRRD file - we will make use of if to populate the header of the\n",
        "  # NRRD mask we are going to get from the inferred segmentation mask\n",
        "  ct_nrrd_path = os.path.join(processed_nrrd_path, pat_id, pat_id + \"_CT.nrrd\")\n",
        "  sitk_ct = sitk.ReadImage(ct_nrrd_path)\n",
        "\n",
        "  output_folder_path = os.path.join(processed_nrrd_path, pat_id, output_folder_name)\n",
        "  \n",
        "  if not os.path.exists(output_folder_path):\n",
        "    os.mkdir(output_folder_path)\n",
        "\n",
        "  pred_softmax_all = np.load(pred_softmax_path)[\"softmax\"]\n",
        "\n",
        "  for channel, structure in enumerate(structure_list):\n",
        "\n",
        "    # FIXME: NRRD does not support float16 tensors. For now, convert to a float32. \n",
        "    #        Then replace with a direct conversion to DICOM?\n",
        "\n",
        "    pred_softmax_segmask = pred_softmax_all[channel].astype(dtype = np.float32)\n",
        "\n",
        "    assert(output_dtype in [\"uint8\", \"uint16\", \"float32\"])      \n",
        "\n",
        "    if output_dtype == \"float32\":\n",
        "      # no rescale needed - the values will be between 0 and 1\n",
        "      # set SITK image dtype to Float32\n",
        "      sitk_dtype = sitk.sitkFloat32\n",
        "\n",
        "    elif output_dtype == \"uint8\":\n",
        "      # rescale between 0 and 255, quantize\n",
        "      pred_softmax_segmask = (255*pred_softmax_segmask).astype(np.int)\n",
        "      # set SITK image dtype to UInt8\n",
        "      sitk_dtype = sitk.sitkUInt8\n",
        "\n",
        "    elif output_dtype == \"uint16\":\n",
        "      # rescale between 0 and 65536\n",
        "      pred_softmax_segmask = (65536*pred_softmax_segmask).astype(int)\n",
        "      # set SITK image dtype to UInt16\n",
        "      sitk_dtype = sitk.sitkUInt16\n",
        "    \n",
        "    pred_softmax_segmask_sitk = sitk.GetImageFromArray(pred_softmax_segmask)\n",
        "    pred_softmax_segmask_sitk.CopyInformation(sitk_ct)\n",
        "    pred_softmax_segmask_sitk = sitk.Cast(pred_softmax_segmask_sitk, sitk_dtype)\n",
        "\n",
        "    output_fn = \"%s.nrrd\"%(structure)\n",
        "    output_path = os.path.join(output_folder_path, output_fn)\n",
        "\n",
        "    writer = sitk.ImageFileWriter()\n",
        "\n",
        "    writer.UseCompressionOn()\n",
        "    writer.SetFileName(output_path)\n",
        "    writer.Execute(pred_softmax_segmask_sitk)"
      ],
      "metadata": {
        "id": "3lixiZVRcQAZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Description here."
      ],
      "metadata": {
        "id": "7gSPfp96cRYK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7WiApMZAjupQ"
      },
      "outputs": [],
      "source": [
        "def nrrd_to_dicomseg(sorted_base_path, processed_base_path,\n",
        "                     dicomseg_json_path, pat_id, skip_empty_slices = True):\n",
        "\n",
        "  \"\"\"\n",
        "  Export DICOM SEG object from segmentation masks stored in NRRD files.\n",
        "\n",
        "  Arguments:\n",
        "    sorted_base_path    : required - path to the folder where the sorted data should be stored.\n",
        "    processed_base_path : required - path to the folder where the preprocessed NRRD data are stored\n",
        "    dicomseg_json_path  : required - ...\n",
        "    pat_id              : required - patient ID (used for naming purposes). \n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  path_to_ct_dir = os.path.join(sorted_base_path, pat_id, \"CT\")\n",
        "\n",
        "  processed_dicomseg_path = os.path.join(processed_base_path, \"dicomseg\")\n",
        "  pat_dir_dicomseg_path = os.path.join(processed_dicomseg_path, pat_id)\n",
        "\n",
        "  if not os.path.exists(pat_dir_dicomseg_path):\n",
        "    os.mkdir(pat_dir_dicomseg_path)\n",
        "\n",
        "  pred_segmasks_nrrd = os.path.join(processed_nrrd_path, pat_id, pat_id + \"_pred_segthor.nrrd\")\n",
        "\n",
        "  dicom_seg_out_path = os.path.join(pat_dir_dicomseg_path, pat_id + \"_SEG.dcm\")\n",
        "\n",
        "  # transform from bool to int according to `itkimage2segimage` requirements\n",
        "  skip_flag = \"--skip\" if skip_empty_slices == True else \"\"\n",
        "\n",
        "  !itkimage2segimage --inputImageList $pred_segmasks_nrrd \\\n",
        "                     --inputDICOMDirectory $path_to_ct_dir \\\n",
        "                     --outputDICOM $dicom_seg_out_path \\\n",
        "                     --inputMetadata $dicomseg_json_path $skip_flag"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "LwYHPQwsj620"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKIzPeXuog9K"
      },
      "source": [
        "---\n",
        "\n",
        "## **General Utilities**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uM4vXFnxolA-"
      },
      "outputs": [],
      "source": [
        "def file_exists_in_bucket(project_name, bucket_name, file_gs_uri):\n",
        "  \n",
        "  \"\"\"\n",
        "  Check whether a file exists in the specified Google Cloud Storage Bucket.\n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    bucket_name  : required - name of the bucket (without gs://)\n",
        "    file_gs_uri  : required - file GS URI\n",
        "  \n",
        "  Returns:\n",
        "    file_exists : boolean variable, True if the file exists in the specified,\n",
        "                  bucket, at the specified location; False if it doesn't.\n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  storage_client = storage.Client(project = project_name)\n",
        "  bucket = storage_client.get_bucket(bucket_name)\n",
        "  \n",
        "  bucket_gs_url = \"gs://%s/\"%(bucket_name)\n",
        "  path_to_file_relative = file_gs_uri.split(bucket_gs_url)[-1]\n",
        "\n",
        "  print(\"Searching `%s` for: \\n%s\\n\"%(bucket_gs_url, path_to_file_relative))\n",
        "\n",
        "  file_exists = bucket.blob(path_to_file_relative).exists(storage_client)\n",
        "\n",
        "  return file_exists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS6WfCfgiS-b"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "JwQKxMXchi0h"
      },
      "outputs": [],
      "source": [
        "def listdir_bucket(project_name, bucket_name, dir_gs_uri):\n",
        "  \n",
        "  \"\"\"\n",
        "  Export DICOM SEG object from segmentation masks stored in NRRD files.\n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    bucket_name  : required - name of the bucket (without gs://)\n",
        "    file_gs_uri  : required - directory GS URI\n",
        "  \n",
        "  Returns:\n",
        "    file_list : list of files in the specified GCS bucket.\n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  storage_client = storage.Client(project = project_name)\n",
        "  bucket = storage_client.get_bucket(bucket_name)\n",
        "  \n",
        "  bucket_gs_url = \"gs://%s/\"%(bucket_name)\n",
        "  path_to_dir_relative = dir_gs_uri.split(bucket_gs_url)[-1]\n",
        "\n",
        "\n",
        "  print(\"Getting the list of files at `%s`...\"%(dir_gs_uri))\n",
        "\n",
        "  file_list = list()\n",
        "\n",
        "  for blob in storage_client.list_blobs(bucket_name,  prefix = path_to_dir_relative):\n",
        "    fn = os.path.basename(blob.name)\n",
        "    file_list.append(fn)\n",
        "\n",
        "  return file_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cya9qL50iU1S"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "EgYBIF9Y8NX8"
      },
      "outputs": [],
      "source": [
        "def format_dict(input_dict):\n",
        "  \n",
        "  \"\"\"\n",
        "  Format dictionary [...]\n",
        "\n",
        "  Arguments:\n",
        "    input_dict : required - \n",
        "    \n",
        "  Returns:\n",
        "    output_df : \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  output_df = pd.DataFrame.from_dict(data = input_dict, orient = \"index\")\n",
        "\n",
        "  output_df = output_df.reset_index()\n",
        "  output_df = output_df.rename(columns = {\"index\" : \"PatientID\", \n",
        "                                          0 : \"inference_time\"}) \n",
        "  \n",
        "  return output_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5Ftc1_AZVUP"
      },
      "source": [
        "---\n",
        "\n",
        "# **Putting Everything Together**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qu6eXpHx6iV"
      },
      "source": [
        "## **Parsing Cohort Information from BigQuery Tables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "id": "V27IJihCi9Kv",
        "outputId": "b99c7ba1-8e8a-4896-e19e-760aa0bce010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique Patient IDs: 357\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 44157 entries, 0 to 44156\n",
            "Data columns (total 11 columns):\n",
            " #   Column              Non-Null Count  Dtype \n",
            "---  ------              --------------  ----- \n",
            " 0   PatientID           44157 non-null  object\n",
            " 1   collection_id       44157 non-null  object\n",
            " 2   source_DOI          44157 non-null  object\n",
            " 3   StudyInstanceUID    44157 non-null  object\n",
            " 4   SeriesInstanceUID   44157 non-null  object\n",
            " 5   SOPInstanceUID      44157 non-null  object\n",
            " 6   crdc_study_uuid     44157 non-null  object\n",
            " 7   crdc_series_uuid    44157 non-null  object\n",
            " 8   crdc_instance_uuid  44157 non-null  object\n",
            " 9   gcs_url             44157 non-null  object\n",
            " 10  idc_version         44157 non-null  object\n",
            "dtypes: object(11)\n",
            "memory usage: 3.7+ MB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   PatientID    collection_id                     source_DOI  \\\n",
              "0  LUNG1-389  nsclc_radiomics  10.7937/K9/TCIA.2015.PF0M9REI   \n",
              "1  LUNG1-389  nsclc_radiomics  10.7937/K9/TCIA.2015.PF0M9REI   \n",
              "2  LUNG1-389  nsclc_radiomics  10.7937/K9/TCIA.2015.PF0M9REI   \n",
              "3  LUNG1-389  nsclc_radiomics  10.7937/K9/TCIA.2015.PF0M9REI   \n",
              "4  LUNG1-389  nsclc_radiomics  10.7937/K9/TCIA.2015.PF0M9REI   \n",
              "\n",
              "                                    StudyInstanceUID  \\\n",
              "0  1.3.6.1.4.1.32722.99.99.3103908869703256044664...   \n",
              "1  1.3.6.1.4.1.32722.99.99.3103908869703256044664...   \n",
              "2  1.3.6.1.4.1.32722.99.99.3103908869703256044664...   \n",
              "3  1.3.6.1.4.1.32722.99.99.3103908869703256044664...   \n",
              "4  1.3.6.1.4.1.32722.99.99.3103908869703256044664...   \n",
              "\n",
              "                                   SeriesInstanceUID  \\\n",
              "0  1.3.6.1.4.1.32722.99.99.2352658665619663766339...   \n",
              "1  1.3.6.1.4.1.32722.99.99.2352658665619663766339...   \n",
              "2  1.3.6.1.4.1.32722.99.99.2352658665619663766339...   \n",
              "3  1.3.6.1.4.1.32722.99.99.2352658665619663766339...   \n",
              "4  1.3.6.1.4.1.32722.99.99.2352658665619663766339...   \n",
              "\n",
              "                                      SOPInstanceUID  \\\n",
              "0  1.3.6.1.4.1.32722.99.99.2414242025218459804872...   \n",
              "1  1.3.6.1.4.1.32722.99.99.2648221267343378996555...   \n",
              "2  1.3.6.1.4.1.32722.99.99.2483895798395217229931...   \n",
              "3  1.3.6.1.4.1.32722.99.99.2829070204870971800854...   \n",
              "4  1.3.6.1.4.1.32722.99.99.3684779892051361103024...   \n",
              "\n",
              "                        crdc_study_uuid                      crdc_series_uuid  \\\n",
              "0  0184ddf7-bf1c-4812-ae58-37f0f62eef51  b3324cee-7e85-4b2c-ae2d-4b89a8da50ca   \n",
              "1  0184ddf7-bf1c-4812-ae58-37f0f62eef51  b3324cee-7e85-4b2c-ae2d-4b89a8da50ca   \n",
              "2  0184ddf7-bf1c-4812-ae58-37f0f62eef51  b3324cee-7e85-4b2c-ae2d-4b89a8da50ca   \n",
              "3  0184ddf7-bf1c-4812-ae58-37f0f62eef51  b3324cee-7e85-4b2c-ae2d-4b89a8da50ca   \n",
              "4  0184ddf7-bf1c-4812-ae58-37f0f62eef51  b3324cee-7e85-4b2c-ae2d-4b89a8da50ca   \n",
              "\n",
              "                     crdc_instance_uuid  \\\n",
              "0  034324d3-0089-4726-bfbb-cd754febde6c   \n",
              "1  053d8545-4adf-44b9-9776-2f149b255796   \n",
              "2  065427e7-c0de-41f1-9cdb-2cd28ee2009c   \n",
              "3  0799aa73-2587-4214-a22a-95d6bb323466   \n",
              "4  0caa0508-e7f0-4ab6-b514-1c035d1c34d9   \n",
              "\n",
              "                                             gcs_url idc_version  \n",
              "0  gs://idc-dev-cr/034324d3-0089-4726-bfbb-cd754f...         7.0  \n",
              "1  gs://idc-dev-cr/053d8545-4adf-44b9-9776-2f149b...         7.0  \n",
              "2  gs://idc-dev-cr/065427e7-c0de-41f1-9cdb-2cd28e...         7.0  \n",
              "3  gs://idc-dev-cr/0799aa73-2587-4214-a22a-95d6bb...         7.0  \n",
              "4  gs://idc-dev-cr/0caa0508-e7f0-4ab6-b514-1c035d...         7.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57b94831-dd0a-475b-af13-4a7061668a3a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PatientID</th>\n",
              "      <th>collection_id</th>\n",
              "      <th>source_DOI</th>\n",
              "      <th>StudyInstanceUID</th>\n",
              "      <th>SeriesInstanceUID</th>\n",
              "      <th>SOPInstanceUID</th>\n",
              "      <th>crdc_study_uuid</th>\n",
              "      <th>crdc_series_uuid</th>\n",
              "      <th>crdc_instance_uuid</th>\n",
              "      <th>gcs_url</th>\n",
              "      <th>idc_version</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LUNG1-389</td>\n",
              "      <td>nsclc_radiomics</td>\n",
              "      <td>10.7937/K9/TCIA.2015.PF0M9REI</td>\n",
              "      <td>1.3.6.1.4.1.32722.99.99.3103908869703256044664...</td>\n",
              "      <td>1.3.6.1.4.1.32722.99.99.2352658665619663766339...</td>\n",
              "      <td>1.3.6.1.4.1.32722.99.99.2414242025218459804872...</td>\n",
              "      <td>0184ddf7-bf1c-4812-ae58-37f0f62eef51</td>\n",
              "      <td>b3324cee-7e85-4b2c-ae2d-4b89a8da50ca</td>\n",
              "      <td>034324d3-0089-4726-bfbb-cd754febde6c</td>\n",
              "      <td>gs://idc-dev-cr/034324d3-0089-4726-bfbb-cd754f...</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LUNG1-389</td>\n",
              "      <td>nsclc_radiomics</td>\n",
              "      <td>10.7937/K9/TCIA.2015.PF0M9REI</td>\n",
              "      <td>1.3.6.1.4.1.32722.99.99.3103908869703256044664...</td>\n",
              "      <td>1.3.6.1.4.1.32722.99.99.2352658665619663766339...</td>\n",
              "      <td>1.3.6.1.4.1.32722.99.99.2648221267343378996555...</td>\n",
              "      <td>0184ddf7-bf1c-4812-ae58-37f0f62eef51</td>\n",
              "      <td>b3324cee-7e85-4b2c-ae2d-4b89a8da50ca</td>\n",
              "      <td>053d8545-4adf-44b9-9776-2f149b255796</td>\n",
              "      <td>gs://idc-dev-cr/053d8545-4adf-44b9-9776-2f149b...</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LUNG1-389</td>\n",
              "      <td>nsclc_radiomics</td>\n",
              "      <td>10.7937/K9/TCIA.2015.PF0M9REI</td>\n",
              "      <td>1.3.6.1.4.1.32722.99.99.3103908869703256044664...</td>\n",
              "      <td>1.3.6.1.4.1.32722.99.99.2352658665619663766339...</td>\n",
              "      <td>1.3.6.1.4.1.32722.99.99.2483895798395217229931...</td>\n",
              "      <td>0184ddf7-bf1c-4812-ae58-37f0f62eef51</td>\n",
              "      <td>b3324cee-7e85-4b2c-ae2d-4b89a8da50ca</td>\n",
              "      <td>065427e7-c0de-41f1-9cdb-2cd28ee2009c</td>\n",
              "      <td>gs://idc-dev-cr/065427e7-c0de-41f1-9cdb-2cd28e...</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LUNG1-389</td>\n",
              "      <td>nsclc_radiomics</td>\n",
              "      <td>10.7937/K9/TCIA.2015.PF0M9REI</td>\n",
              "      <td>1.3.6.1.4.1.32722.99.99.3103908869703256044664...</td>\n",
              "      <td>1.3.6.1.4.1.32722.99.99.2352658665619663766339...</td>\n",
              "      <td>1.3.6.1.4.1.32722.99.99.2829070204870971800854...</td>\n",
              "      <td>0184ddf7-bf1c-4812-ae58-37f0f62eef51</td>\n",
              "      <td>b3324cee-7e85-4b2c-ae2d-4b89a8da50ca</td>\n",
              "      <td>0799aa73-2587-4214-a22a-95d6bb323466</td>\n",
              "      <td>gs://idc-dev-cr/0799aa73-2587-4214-a22a-95d6bb...</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LUNG1-389</td>\n",
              "      <td>nsclc_radiomics</td>\n",
              "      <td>10.7937/K9/TCIA.2015.PF0M9REI</td>\n",
              "      <td>1.3.6.1.4.1.32722.99.99.3103908869703256044664...</td>\n",
              "      <td>1.3.6.1.4.1.32722.99.99.2352658665619663766339...</td>\n",
              "      <td>1.3.6.1.4.1.32722.99.99.3684779892051361103024...</td>\n",
              "      <td>0184ddf7-bf1c-4812-ae58-37f0f62eef51</td>\n",
              "      <td>b3324cee-7e85-4b2c-ae2d-4b89a8da50ca</td>\n",
              "      <td>0caa0508-e7f0-4ab6-b514-1c035d1c34d9</td>\n",
              "      <td>gs://idc-dev-cr/0caa0508-e7f0-4ab6-b514-1c035d...</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57b94831-dd0a-475b-af13-4a7061668a3a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-57b94831-dd0a-475b-af13-4a7061668a3a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-57b94831-dd0a-475b-af13-4a7061668a3a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# FIXME: for debug purposes, only process a handful of patients for now\n",
        "pat_id_list = sorted(list(set(cohort_df[\"PatientID\"].values)))\n",
        "\n",
        "print(\"Total number of unique Patient IDs:\", len(pat_id_list))\n",
        "\n",
        "display(cohort_df.info())\n",
        "\n",
        "display(cohort_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AolLtXOLVt7D"
      },
      "source": [
        "---\n",
        "\n",
        "## **Set Run Parameters**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRmrt00589lv",
        "outputId": "18c9c39a-0aae-436d-ebad-666eb603296c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file:///content/data/run_params.yaml [Content-Type=application/octet-stream]...\n",
            "/ [1/1 files][  635.0 B/  635.0 B] 100% Done                                    \n",
            "Operation completed over 1 objects/635.0 B.                                      \n"
          ]
        }
      ],
      "source": [
        "data_base_path = \"/content/data\"\n",
        "raw_base_path = \"/content/data/raw/tmp\"\n",
        "sorted_base_path = \"/content/data/raw/nsclc-radiomics/dicom\"\n",
        "\n",
        "processed_base_path = \"/content/data/processed/nsclc-radiomics/\"\n",
        "processed_nrrd_path = os.path.join(processed_base_path, \"nrrd\")\n",
        "processed_nifti_path = os.path.join(processed_base_path, \"nii\")\n",
        "\n",
        "processed_dicomseg_path = os.path.join(processed_base_path, \"dicomseg\")\n",
        "processed_dicompm_path = os.path.join(processed_base_path, \"dicompm\")\n",
        "\n",
        "model_input_folder = \"/content/data/model_input/\"\n",
        "model_output_folder = \"/content/data/nnunet_output/\"\n",
        "\n",
        "bucket_output_base_uri = os.path.join(bucket_base_uri, \"nnunet/nnunet_output\")\n",
        "\n",
        "# -----------------\n",
        "# nnU-Net pipeline parameters\n",
        "\n",
        "# choose from: \"2d\", \"3d_lowres\", \"3d_fullres\", \"3d_cascade_fullres\"\n",
        "nnunet_model = \"3d_fullres\"\n",
        "use_tta = True\n",
        "export_prob_maps = True\n",
        "\n",
        "experiment_folder_name = nnunet_model + \"-tta\" if use_tta == True else + nnunet_model + \"-no_tta\"\n",
        "bucket_experiment_folder_uri = os.path.join(bucket_output_base_uri, experiment_folder_name)\n",
        "\n",
        "bucket_log_folder_uri = os.path.join(bucket_experiment_folder_uri, 'log')\n",
        "\n",
        "bucket_nifti_folder_uri = os.path.join(bucket_experiment_folder_uri, 'nii')\n",
        "bucket_softmax_pred_folder_uri = os.path.join(bucket_experiment_folder_uri, 'softmax_pred')\n",
        "\n",
        "bucket_dicomseg_folder_uri = os.path.join(bucket_experiment_folder_uri, 'dicomseg')\n",
        "\n",
        "# -----------------\n",
        "# save run information\n",
        "\n",
        "yaml_fn = \"run_params.yaml\"\n",
        "yaml_out_path = os.path.join(data_base_path, yaml_fn)\n",
        "\n",
        "settings_dict = dict()\n",
        "settings_dict[\"bucket\"] = dict()\n",
        "settings_dict[\"bucket\"][\"name\"] = bucket_name\n",
        "settings_dict[\"bucket\"][\"base_uri\"] = bucket_base_uri\n",
        "settings_dict[\"bucket\"][\"output_base_uri\"] = bucket_output_base_uri\n",
        "settings_dict[\"bucket\"][\"experiment_folder_uri\"] = bucket_experiment_folder_uri\n",
        "settings_dict[\"bucket\"][\"nifti_folder_uri\"] = bucket_nifti_folder_uri\n",
        "settings_dict[\"bucket\"][\"softmax_pred_folder_uri\"] = bucket_softmax_pred_folder_uri\n",
        "settings_dict[\"bucket\"][\"dicomseg_folder_uri\"] = bucket_dicomseg_folder_uri\n",
        "settings_dict[\"bucket\"][\"log_folder_uri\"] = bucket_log_folder_uri\n",
        "\n",
        "settings_dict[\"inference\"] = dict()\n",
        "settings_dict[\"inference\"][\"model\"] = nnunet_model\n",
        "settings_dict[\"inference\"][\"use_tta\"] = use_tta\n",
        "settings_dict[\"inference\"][\"export_prob_maps\"] = export_prob_maps\n",
        "\n",
        "with open(yaml_out_path, 'w') as fp:\n",
        "  yaml.dump(settings_dict, fp, default_flow_style = False)\n",
        "\n",
        "gs_uri_yaml_file = os.path.join(bucket_log_folder_uri, yaml_fn)\n",
        "\n",
        "!gsutil -m cp $yaml_out_path $gs_uri_yaml_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TID5qTiMgspa",
        "outputId": "89a48fa4-8f62-4fe4-896f-a75805775ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting the list of files at `gs://idc-medima-paper/nnunet/nnunet_output/3d_fullres-tta/dicomseg`...\n",
            "\n",
            "Found 5 patients already processed.\n",
            "Moving on with the remaining 352...\n"
          ]
        }
      ],
      "source": [
        "# exclude from processing all the patients for which a DICOM SEG object was exported already\n",
        "# (stored in the specified Google Cloud Storage Bucket)\n",
        "dicomseg_bucket_list = listdir_bucket(project_name = project_name,\n",
        "                                      bucket_name = bucket_name,\n",
        "                                      dir_gs_uri = bucket_dicomseg_folder_uri)\n",
        "\n",
        "already_processed_id_list = [f.split(\"_SEG\")[0] for f in dicomseg_bucket_list]\n",
        "\n",
        "print(\"\\nFound %g patients already processed.\"%(len(already_processed_id_list)))\n",
        "\n",
        "pat_to_process_id_list = sorted(list(set(pat_id_list) - set(already_processed_id_list)))\n",
        "\n",
        "print(\"Moving on with the remaining %g...\"%(len(pat_to_process_id_list)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n0qjPp1B_mv"
      },
      "source": [
        "## **Running the Per-patient Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpfMEkfvB8oZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a506bb3-5e6f-465f-8b0d-ab8719368aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1/352) Processing patient LUNG1-007\n",
            "Searching `gs://idc-medima-paper/` for: \n",
            "nnunet/nnunet_output/3d_fullres-tta/nii/LUNG1-007.nii.gz\n",
            "\n",
            "Copying files from IDC buckets to /content/data/raw/tmp/LUNG1-007...\n",
            "Done in 11.7594 seconds.\n",
            "\n",
            "Sorting DICOM files...\n",
            "100% 131/131 [00:01<00:00, 69.42it/s] \n",
            "Files sorted\n",
            "Done in 2.45961 seconds.\n",
            "Sorted DICOM data saved at: /content/data/raw/nsclc-radiomics/dicom/LUNG1-007\n",
            "Removing un-sorted data at /content/data/raw/tmp/LUNG1-007...\n",
            "... Done.\n",
            "\n",
            "Running 'plastimatch convert' with the specified arguments:\n",
            "  --input /content/data/raw/nsclc-radiomics/dicom/LUNG1-007/CT\n",
            "  --output-img /content/data/processed/nsclc-radiomics/nrrd/LUNG1-007/LUNG1-007_CT.nrrd\n",
            "... Done.\n",
            "\n",
            "Running 'plastimatch convert' with the specified arguments:\n",
            "  --input /content/data/raw/nsclc-radiomics/dicom/LUNG1-007/CT\n",
            "  --output-img /content/data/processed/nsclc-radiomics/nii/LUNG1-007/LUNG1-007_CT.nii.gz\n",
            "... Done.\n",
            "Copying /content/data/processed/nsclc-radiomics/nii/LUNG1-007/LUNG1-007_CT.nii.gz\n",
            "to /content/data/model_input/LUNG1-007_0000.nii.gz...\n",
            "... Done.\n",
            "Running `nnUNet_predict` with `3d_fullres` model...\n",
            "Processing file at /content/data/model_input/LUNG1-007_0000.nii.gz...\n"
          ]
        }
      ],
      "source": [
        "for idx, pat_id in enumerate(pat_to_process_id_list):\n",
        "\n",
        "  # -----------------\n",
        "  # init\n",
        "\n",
        "  start_total = time.time()\n",
        "\n",
        "  # init every single time, as the most recent logs are loaded from the bucket\n",
        "  inference_time_dict = dict()\n",
        "  total_time_dict = dict()\n",
        "\n",
        "  clear_output(wait = True)\n",
        "\n",
        "  print(\"(%g/%g) Processing patient %s\"%(idx + 1, len(pat_to_process_id_list), pat_id))\n",
        "\n",
        "  patient_df = cohort_df[cohort_df[\"PatientID\"] == pat_id]\n",
        "\n",
        "  has_segmask_already = False\n",
        "\n",
        "  dicomseg_fn = pat_id + \"_SEG.dcm\"\n",
        "\n",
        "  input_nifti_fn = pat_id + \"_0000.nii.gz\"\n",
        "  input_nifti_path = os.path.join(model_input_folder, input_nifti_fn)\n",
        "\n",
        "  pred_nifti_fn = pat_id + \".nii.gz\"\n",
        "  pred_nifti_path = os.path.join(model_output_folder, pred_nifti_fn)\n",
        "\n",
        "  pred_softmax_folder_name = \"pred_softmax\"\n",
        "  pred_softmax_folder_path = os.path.join(processed_nrrd_path, pat_id, pred_softmax_folder_name)\n",
        "  \n",
        "  # -----------------\n",
        "  # GS URI definition\n",
        "\n",
        "  # gs URI at which the *nii.gz object is or will be stored in the bucket\n",
        "  gs_uri_nifti_file = os.path.join(bucket_nifti_folder_uri, pred_nifti_fn)\n",
        "\n",
        "  # gs URI at which the folder storing the *.nrrd softmax probabilities is or will be stored in the bucket\n",
        "  gs_uri_softmax_pred_folder = os.path.join(bucket_softmax_pred_folder_uri, pat_id)\n",
        "\n",
        "  # gs URI at which the DICOM SEG object is or will be stored in the bucket\n",
        "  gs_uri_dicomseg_file = os.path.join(bucket_dicomseg_folder_uri, dicomseg_fn)\n",
        "\n",
        "\n",
        "  # -----------------\n",
        "  # cross-load the CT data from the IDC buckets, run the preprocessing\n",
        "\n",
        "  # check whether the NIfTI seg mask exists already\n",
        "  has_segmask_already = file_exists_in_bucket(project_name = project_name,\n",
        "                                              bucket_name = bucket_name,\n",
        "                                              file_gs_uri = gs_uri_nifti_file)\n",
        "\n",
        "  # if the raw segmentation file exists in the output directory but the DICOM SEG\n",
        "  # doesn't, skip the inference phase. Data still need to be downloaded because\n",
        "  # the DICOM folder is essential in the DICOM SEG generation process\n",
        "  download_patient_data(raw_base_path = raw_base_path,\n",
        "                        sorted_base_path = sorted_base_path,\n",
        "                        patient_df = patient_df,\n",
        "                        remove_raw = True)\n",
        "    \n",
        "  # DICOM CT to NRRD - good to have for a number of reasons\n",
        "  pypla_dicom_ct_to_nrrd(sorted_base_path = sorted_base_path,\n",
        "                         processed_nrrd_path = processed_nrrd_path,\n",
        "                         pat_id = pat_id, verbose = True)\n",
        "\n",
        "  # -----------------\n",
        "  # DL-inference\n",
        "  \n",
        "  if has_segmask_already == True:\n",
        "    # copy the mask in the correct folder etc.\n",
        "    print(\"Retrieving the segmentation mask from the specified bucket...\")\n",
        "    print(\"Copying from %s\"%(gs_uri_nifti_file))\n",
        "    !gsutil -m cp $gs_uri_nifti_file $pred_nifti_path\n",
        "\n",
        "  else:\n",
        "\n",
        "    # DICOM CT to NIfTI - required for the processing\n",
        "    pypla_dicom_ct_to_nifti(sorted_base_path = sorted_base_path,\n",
        "                            processed_nifti_path = processed_nifti_path,\n",
        "                            pat_id = pat_id, verbose = True)\n",
        "\n",
        "    # FIXME: could we get rid of these at least in the inference notebook?\n",
        "    # DICOM RTSTRUCT to NRRD - good to have for a number of reasons\n",
        "    \"\"\"\n",
        "    pypla_dicom_rtstruct_to_nrrd(sorted_base_path = sorted_base_path,\n",
        "                                 processed_nrrd_path = processed_nrrd_path,\n",
        "                                 pat_id = pat_id, verbose = True)\n",
        "    \"\"\"\n",
        "\n",
        "    # prepare the `model_input` folder for the inference phase\n",
        "    prep_input_data(processed_nifti_path = processed_nifti_path,\n",
        "                    model_input_folder = model_input_folder,\n",
        "                    pat_id = pat_id)\n",
        "\n",
        "    start_inference = time.time()\n",
        "    # run the DL-based prediction\n",
        "    process_patient_nnunet(model_input_folder = model_input_folder,\n",
        "                           model_output_folder = model_output_folder, \n",
        "                           nnunet_model = nnunet_model, use_tta = use_tta,\n",
        "                           export_prob_maps = export_prob_maps, verbose = False)\n",
        "\n",
        "    elapsed_inference = time.time() - start_inference\n",
        "    inference_time_dict[pat_id] = elapsed_inference\n",
        "\n",
        "    # convert the softmax predictions to NRRD files\n",
        "    numpy_to_nrrd(model_output_folder = model_output_folder,\n",
        "                  processed_nrrd_path = processed_nrrd_path,\n",
        "                  pat_id = pat_id,\n",
        "                  output_folder_name = pred_softmax_folder_name)\n",
        "\n",
        "    # copy the nnU-Net *.npz softmax probabilities in the chosen bucket\n",
        "    !gsutil -m cp $pred_softmax_folder_path/* $gs_uri_softmax_pred_folder\n",
        "\n",
        "    # copy the nnU-Net *.nii.gz binary masks in the chosen bucket\n",
        "    !gsutil -m cp $pred_nifti_path $gs_uri_nifti_file\n",
        "\n",
        "    # remove the NIfTI file the prediction was computed from\n",
        "    !rm $input_nifti_path\n",
        "    \n",
        "\n",
        "  # -----------------\n",
        "  # post-processing\n",
        "  pypla_postprocess(processed_nrrd_path = processed_nrrd_path,\n",
        "                    model_output_folder = model_output_folder,\n",
        "                    pat_id = pat_id)\n",
        "  \n",
        "  nrrd_to_dicomseg(sorted_base_path = sorted_base_path,\n",
        "                   processed_base_path = processed_base_path,\n",
        "                   dicomseg_json_path = dicomseg_json_path,\n",
        "                   pat_id = pat_id)\n",
        "\n",
        "  pred_dicomseg_path = os.path.join(processed_dicomseg_path, pat_id, dicomseg_fn)\n",
        "\n",
        "  !gsutil -m cp $pred_dicomseg_path $gs_uri_dicomseg_file\n",
        "\n",
        "  elapsed_total = time.time() - start_total\n",
        "\n",
        "  if has_segmask_already == False:\n",
        "    total_time_dict[pat_id] = elapsed_total\n",
        "\n",
        "  print(\"End-to-end processing of %s completed in %g seconds.\\n\"%(pat_id, elapsed_total))\n",
        "\n",
        "  # -----------------\n",
        "  # save inference time information - upload after every processing step\n",
        "\n",
        "  csv_fn = \"inference_time.csv\"\n",
        "\n",
        "  csv_path = os.path.join(data_base_path, csv_fn)\n",
        "  gs_uri_csv_file = os.path.join(bucket_log_folder_uri, csv_fn)\n",
        "\n",
        "  has_csv_already = file_exists_in_bucket(project_name = project_name,\n",
        "                                          bucket_name = bucket_name,\n",
        "                                          file_gs_uri = gs_uri_csv_file)\n",
        "\n",
        "  # if the log CSV is found already, append to it\n",
        "  if has_csv_already == True:\n",
        "\n",
        "    # copy the log CSV from bucket and load it as a DataFrame, append to it\n",
        "    !gsutil -m cp $gs_uri_csv_file $csv_path\n",
        "\n",
        "    inference_time_df = pd.read_csv(csv_path, index_col = [0]) \n",
        "    add_to_inference_time_df = format_dict(inference_time_dict) \n",
        "    new_inference_time_df = pd.concat([inference_time_df, add_to_inference_time_df],\n",
        "                                      ignore_index = True)\n",
        "    \n",
        "    # push the updated version to the bucket\n",
        "    new_inference_time_df.to_csv(csv_path)\n",
        "    !gsutil -m cp $csv_path $gs_uri_csv_file\n",
        "\n",
        "  # in the case the log CSV does not exist yet, create a new one from scratch\n",
        "  else:\n",
        "    inference_time_df = format_dict(inference_time_dict) \n",
        "    inference_time_df.to_csv(csv_path)\n",
        "\n",
        "    !gsutil -m cp $csv_path $gs_uri_csv_file\n",
        "\n",
        "  # -----------------\n",
        "  # save total processing time information - upload after every processing step\n",
        "\n",
        "  csv_fn = \"total_processing_time.csv\"\n",
        "\n",
        "  csv_path = os.path.join(data_base_path, csv_fn)\n",
        "  gs_uri_csv_file = os.path.join(bucket_log_folder_uri, csv_fn)\n",
        "\n",
        "  has_csv_already = file_exists_in_bucket(project_name = project_name,\n",
        "                                          bucket_name = bucket_name,\n",
        "                                          file_gs_uri = gs_uri_csv_file)\n",
        "\n",
        "  # if the log CSV is found already, append to it\n",
        "  if has_csv_already == True:\n",
        "\n",
        "    # copy the log CSV from bucket and load it as a DataFrame, append to it\n",
        "    !gsutil -m cp $gs_uri_csv_file $csv_path\n",
        "\n",
        "    total_time_df = pd.read_csv(csv_path, index_col = [0]) \n",
        "    add_to_total_time_df = format_dict(total_time_dict) \n",
        "    new_total_time_df = pd.concat([total_time_df, add_to_total_time_df],\n",
        "                                  ignore_index = True)\n",
        "    \n",
        "    # push the updated version to the bucket\n",
        "    new_total_time_df.to_csv(csv_path)\n",
        "    !gsutil -m cp $csv_path $gs_uri_csv_file\n",
        "\n",
        "  # in the case the log CSV does not exist yet, create a new one from scratch\n",
        "  else:\n",
        "    total_time_df = format_dict(total_time_dict) \n",
        "    total_time_df.to_csv(csv_path)\n",
        "\n",
        "    !gsutil -m cp $csv_path $gs_uri_csv_file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7b4MCk63AhfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aL9_LxuCAwIz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WECSd3mKj_pM"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "giOdpG9-r4eB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "idc_nnunet-infer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPlzjTZ27wRosIUwVwTjK/n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}