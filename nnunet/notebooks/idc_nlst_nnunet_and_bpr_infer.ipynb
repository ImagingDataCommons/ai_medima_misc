{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfHa5lY5Xkw9"
      },
      "source": [
        "This notebook performs inference for both nnUNet and BPR for the NLST collection. This way the conversion of CT data to nii is performed only once for both use cases. As a precaution the nifti files are uploaded to the bucket (in the same folder as the SEG files for now, will change).\n",
        "\n",
        "Notes to do 6-10-22 week: \n",
        "1. Make sure that the 3d model runs, need to delete old files - done \n",
        "2. Fix query per Andrey's suggestions - done \n",
        "3. Resampling - done\n",
        "  - Include resampling of the input data for nnUNet to 1x1x2.5 (this is from the supplementary material on nnUNet). -- done\n",
        "  - Include resampling to the original space for the predicted binary labels nii file. -- done\n",
        "  - Include resampling to the original space for the predicted binary labels nrrd file \n",
        "  - Include resampling to the original space for the nrrd segments for the softmax prob.  -- done\n",
        "4. Still cuda memory error with resampling -- so remove export of prob maps, this seems to solve it.\n",
        "\n",
        "\n",
        "Other to do 6-10-22: \n",
        "3. Misc - Check why two SEG files of same series are of different sizes? \n",
        "4. Misc - collapse nulls in BQ table \n",
        "5. Misc - redo profiling times \n",
        "\n",
        "Notes to do from week 6-6 to 6-10: \n",
        "1. Make changes to per series loop to run over multiple models (2d, 3d etc). \n",
        "2. Modify BQ tables to save all inference/total times to the same table. \n",
        "3. Do some quick plotting to see differences in the time (or DataStudio dashboard), see effect of num instances per time \n",
        "4. Figure out what to change the SegmentAlgorithmName to in the meta json file \n",
        "5. Maybe pyradiomics? \n",
        "\n",
        "Notes to do misc:\n",
        "1. How to get latest dcmqi?? \n",
        "2. Need to move functions to git repo!\n",
        "\n",
        "Notes to discuss: \n",
        "1. New user needs to set up aws credentials for using s5cmd \n",
        "2. We can probably remove the dicom sorting step.\n",
        "3. Dennis - in nrrd_to_dicomseg the processed_nrrd_path doesn't exist\n",
        "4. Should we use dicom2nifti or another program instead of plastimatch?\n",
        "5. Optimization  \n",
        "\n",
        "Notes on changes: \n",
        "1. The NLST query is updated to not include instances where there are multiple patient positions or orientations. \n",
        "2. Uploads the nifti file per series to the bucket as a precaution \n",
        "3. Uses the faster s5cmd instead of gsutil for download of DICOM (did not use yet for download/upload of other files)\n",
        "4. Saves the inference and total time values to tables instead of to a csv \n",
        "5. Uses s5cmd instead of gsutil for all "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Dwe3nYylG_2"
      },
      "source": [
        "Profile results summary from late May? \n",
        "\n",
        "This was using 2d, tta=True and export prob maps = True! \n",
        "Need to redo to set tta=False and export prob maps = False!\n",
        "And change all gsutil to s5cmd. \n",
        "\n",
        "Min # slices: \n",
        "- download_series_data_s5cmd = 2.7% of time\n",
        "- pypla_dicom_ct_to_nrrd = 5.7% of time \n",
        "- pypla_dicom_ct_to_nifti = 5.0% of time \n",
        "- upload ct nii to bucket (gsutil) = 5.7% of time \n",
        "- process_patient_nnunet = 42.2% of time \n",
        "- numpy_to_nrrd = 1.6%\n",
        "- copy nnunet softmax prob (gsutil) = 3.0%\n",
        "- copy nnunet pred (gsutil) = 3.1%\n",
        "- adding two values to table = total ~4%\n",
        "\n",
        "Median # slices:\n",
        "- download_series_data_s5cmd = 1.3% of time \n",
        "- pypla_dicom_ct_to_nrrd = 6.9% of time \n",
        "- pypla_dicom_ct_to_nifti = 6.9% of time \n",
        "- uplaod ct nii to bucket (gsutil) = 2.3% of time\n",
        "- process_patient_nnunet = 58.9% of time \n",
        "- numpy_to_nrrd = 2.2 %\n",
        "- copy unet softmax prob (gsutil) = 4.8% \n",
        "- copy nnunet pred (gsutil) = 1.6%\n",
        "- adding two values to table = total ~4%\n",
        "\n",
        "Max # slices: \n",
        "- waiting over 1 hour for nnUnet prediction -- I did not wait for it to complete "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkKOcbyxs__u"
      },
      "source": [
        "# **Parameterization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "aFr36lYatRU_"
      },
      "outputs": [],
      "source": [
        "# SPECIFY: target table of results \n",
        "\n",
        "# Table and view names for holding cohort information \n",
        "project_name = 'idc-external-018'    # project_name = \"idc-sandbox-000\"\n",
        "bucket_name = 'idc-medima-paper-dk'  # bucket_name = 'idc-medima-paper'\n",
        "location_id = 'us-central1'\n",
        "dataset_id = 'dataset_nlst' \n",
        "table_id_name = 'table_nlst' # holds the cohort information \n",
        "table_view_id_name = 'nlst_revised_series_selection' \n",
        "\n",
        "# Tables for inference and total times \n",
        "nnunet_table_id = 'nlst_nnunet_time' # results over all runs will be added to this table. \n",
        "bpr_table_id = 'nlst_bpr_time'       # results over all runs will be added to this table. \n",
        "nnunet_table_id_fullname = '.'.join([project_name, dataset_id, nnunet_table_id])\n",
        "bpr_table_id_fullname = '.'.join([project_name, dataset_id, bpr_table_id])\n",
        "\n",
        "# Bucket subdirectory to hold results \n",
        "# different bucket per run, for now. \n",
        "# nlst_sub = 'nlst_25_series_06_15_22' \n",
        "# nlst_sub = 'nlst_25_series_06_16_22' \n",
        "nlst_sub = 'nlst_25_series' # CHANGE THIS if you want.\n",
        "\n",
        "# name of DICOM datastore \n",
        "# different datastore per run, for now. \n",
        "# datastore_id = 'datastore_nlst_25_series_06_15_22' \n",
        "# datastore_id = 'datastore_nlst_25_series_06_16_22' \n",
        "datastore_id = 'datastore_nlst_25_series' # CHANGE THIS if you want.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zgjKUG-ds-8O"
      },
      "outputs": [],
      "source": [
        "# SPECIFY: model list\n",
        "\n",
        "# choose from: \"2d\", \"3d_lowres\", \"3d_fullres\", \"3d_cascade_fullres\"\n",
        "model_list = ['2d', '3d_fullres']\n",
        "use_tta = True\n",
        "# export_prob_maps = True\n",
        "export_prob_maps = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uyB6gRyCx-WG"
      },
      "outputs": [],
      "source": [
        "# SPECIFY: sample to be analyzed \n",
        "\n",
        "# Choose 25 patients (=series) to analyze each time \n",
        "# Define the patient_offset to change the starting index. \n",
        "patient_offset = 25 # CHANGE THIS VALUE "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZnoRi9Y7nEB"
      },
      "source": [
        "# **Environment Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1T8sB7YgjZza",
        "outputId": "9dfe1aa4-227e-428a-fd88-b9d488df3bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jun 16 14:14:08 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8FLzDDB9dEae"
      },
      "outputs": [],
      "source": [
        "# These need to be upgraded first. \n",
        "\n",
        "%%capture\n",
        "\n",
        "!pip uninstall -y pandas==1.1.5 \n",
        "!pip install pandas==1.2.1\n",
        "\n",
        "!pip install --upgrade google-cloud-bigquery\n",
        "!pip install --upgrade db-dtypes\n",
        "!pip install --upgrade pandas-gbq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "non5qVLIcG4M",
        "outputId": "c79e1888-1562-4428-ebf7-3abd303c6040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jun 16 14:14:40 2022\n",
            "\n",
            "Current directory : /content\n",
            "Hostname          : 613f06ebef6b\n",
            "Username          : root\n",
            "Python version    : 3.7.13 (default, Apr 24 2022, 01:04:09) \n"
          ]
        }
      ],
      "source": [
        "# Import packages for both use cases. \n",
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import yaml\n",
        "import time\n",
        "import tqdm\n",
        "import copy\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# useful information\n",
        "curr_dir = !pwd\n",
        "curr_droid = !hostname\n",
        "curr_pilot = !whoami\n",
        "\n",
        "print(time.asctime(time.localtime()))\n",
        "\n",
        "print(\"\\nCurrent directory :\", curr_dir[-1])\n",
        "print(\"Hostname          :\", curr_droid[-1])\n",
        "print(\"Username          :\", curr_pilot[-1])\n",
        "\n",
        "print(\"Python version    :\", sys.version.split('\\n')[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BByKS66FjKOm"
      },
      "outputs": [],
      "source": [
        "# Import packages etc that nnUnet depends on. \n",
        "\n",
        "%%capture\n",
        "!pip install SimpleITK\n",
        "import SimpleITK as sitk\n",
        "!pip install nnunet\n",
        "\n",
        "# Import packages/repos that BPR depends on. \n",
        "\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd \n",
        "import pandas_gbq\n",
        "import db_dtypes\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "if os.path.isdir('/content/BodyPartRegression'):\n",
        "  try:\n",
        "    shutil.rmtree('/content/BodyPartRegression')\n",
        "  except OSError as err:\n",
        "    print(\"Error: %s : %s\" % (\"/content/BodyPartRegression\", err.strerror)) \n",
        "\n",
        "!pip install torch==1.8.1 pytorch-lightning==1.2.10 torchtext==0.9.1 torchvision==0.9.1 torchaudio==0.8.1 dataclasses==0.6\n",
        "!pip install bpreg\n",
        "!git clone https://github.com/MIC-DKFZ/BodyPartRegression.git\n",
        "#!pip install SimpleITK\n",
        "!pip install pydicom\n",
        "\n",
        "import bpreg \n",
        "import seaborn as sb \n",
        "#import SimpleITK as sitk\n",
        "import glob\n",
        "import matplotlib.pyplot as plt \n",
        "import pydicom\n",
        "\n",
        "!pip install opencv-python-headless==4.1.2.30 \n",
        "\n",
        "from BodyPartRegression.docs.notebooks.utils import * \n",
        "from bpreg.scripts.bpreg_inference import bpreg_inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2DMUqTOVF5WP"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9Y43S1F35h8m"
      },
      "outputs": [],
      "source": [
        "# Set the fields for storage. \n",
        "\n",
        "from google.cloud import storage\n",
        "# bucket_name = 'idc-medima-paper'\n",
        "# project_name = \"idc-sandbox-000\"\n",
        "# bucket_name = 'idc-medima-paper-dk'\n",
        "# project_name = \"idc-external-018\"\n",
        "\n",
        "# location where to store the data (and check if a patient was processed already)\n",
        "# if a patient was processed already, copy over the segmentation and run only\n",
        "# the post-processing (split the masks, etc.)\n",
        "# bucket_base_uri = \"gs://%s/\"%(bucket_name)\n",
        "bucket_base_uri =  \"s3://%s/\"%(bucket_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szXo5-sWOIc9"
      },
      "source": [
        "Set up AWS. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxoklXn7dbdF",
        "outputId": "12c6fa71-1291-4f98-859c-5d367438358e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "--2022-06-16 14:18:20--  https://github.com/peak/s5cmd/releases/download/v2.0.0-beta/s5cmd_2.0.0-beta_Linux-64bit.tar.gz\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/73909333/aafb8c9b-5844-4d77-bd36-a58662d19c98?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220616%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220616T141821Z&X-Amz-Expires=300&X-Amz-Signature=66a04603a58fc161a482122c74639f5bec01b740dfa46719a0e36c3bdaa23c45&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=73909333&response-content-disposition=attachment%3B%20filename%3Ds5cmd_2.0.0-beta_Linux-64bit.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-06-16 14:18:21--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/73909333/aafb8c9b-5844-4d77-bd36-a58662d19c98?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220616%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220616T141821Z&X-Amz-Expires=300&X-Amz-Signature=66a04603a58fc161a482122c74639f5bec01b740dfa46719a0e36c3bdaa23c45&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=73909333&response-content-disposition=attachment%3B%20filename%3Ds5cmd_2.0.0-beta_Linux-64bit.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4271712 (4.1M) [application/octet-stream]\n",
            "Saving to: ‘s5cmd_2.0.0-beta_Linux-64bit.tar.gz’\n",
            "\n",
            "s5cmd_2.0.0-beta_Li 100%[===================>]   4.07M  14.0MB/s    in 0.3s    \n",
            "\n",
            "2022-06-16 14:18:22 (14.0 MB/s) - ‘s5cmd_2.0.0-beta_Linux-64bit.tar.gz’ saved [4271712/4271712]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Setting up aws -- This needs to be explained to the user. \n",
        "\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!mkdir -p ~/.aws\n",
        "!cp /content/gdrive/MyDrive/aws/credentials ~/.aws\n",
        "# Get s5cmd \n",
        "!wget https://github.com/peak/s5cmd/releases/download/v2.0.0-beta/s5cmd_2.0.0-beta_Linux-64bit.tar.gz\n",
        "!tar zxf s5cmd_2.0.0-beta_Linux-64bit.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "n0zBdBJKDYFl"
      },
      "outputs": [],
      "source": [
        "# Unmount drive as we have already copied the file \n",
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg82vxFyPmuP"
      },
      "source": [
        "\n",
        "We will next install a number of packages needed for organizing and converting DICOM files:\n",
        "\n",
        "1.    `dicomsort`, a package for sorting DICOM files into a directory tree using specific DICOM fields. \n",
        "2.   `plastimatch`, a package used to convert RTSTRUCT DICOM files to nrrd. \n",
        "3.   `dcmqi`, a package which converts SEG DICOM files to nrrd.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4mHluV-LPolH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "start_time=time.time()\n",
        "\n",
        "# dicomsort \n",
        "if os.path.isdir('/content/src/dicomsort'):\n",
        "  try:\n",
        "    shutil.rmtree('/content/src/dicomsort')\n",
        "  except OSError as err:\n",
        "    print(\"Error: %s : %s\" % (\"dicomsort\", err.strerror)) \n",
        "# !git clone https://github.com/pieper/dicomsort.git \n",
        "!git clone https://github.com/pieper/dicomsort.git src/dicomsort\n",
        "\n",
        "# plastimatch and pyplastimatch \n",
        "!sudo apt install plastimatch \n",
        "if os.path.isdir('/content/src/pyplastimatch'):\n",
        "  try:\n",
        "    shutil.rmtree('/content/src/pyplastimatch')\n",
        "  except OSError as err:\n",
        "    print(\"Error: %s : %s\" % (\"pyplastimatch\", err.strerror)) \n",
        "# !git clone https://github.com/AIM-Harvard/pyplastimatch.git \n",
        "# from pyplastimatch.pyplastimatch import pyplastimatch as pypla\n",
        "!git clone https://github.com/AIM-Harvard/pyplastimatch src/pyplastimatch\n",
        "import src.pyplastimatch.pyplastimatch.pyplastimatch as pypla\n",
        "\n",
        "# dcmqi \n",
        "# !wget https://github.com/QIICR/dcmqi/releases/download/v1.2.4/dcmqi-1.2.4-linux.tar.gz\n",
        "# !tar zxvf dcmqi-1.2.4-linux.tar.gz\n",
        "# !cp dcmqi-1.2.4-linux/bin/* /usr/local/bin/\n",
        "\n",
        "# !wget https://github.com/QIICR/dcmqi/releases/download/latest/dcmqi-1.2.4-linux-20220312-eb8bc91.tar.gz\n",
        "# !tar zxvf dcmqi-1.2.4-linux-20220312-eb8bc91.tar.gz\n",
        "# !cp dcmqi-1.2.4-linux-20220312-eb8bc91/bin/* /usr/local/bin/\n",
        "\n",
        "# !wget https://github.com/QIICR/dcmqi/releases/download/latest/dcmqi-1.2.4-linux-20220418-7f45450.tar.gz\n",
        "# !tar zxvf dcmqi-1.2.4-linux-20220418-7f45450.tar.gz\n",
        "# !cp dcmqi-1.2.4-linux-20220418-7f45450/bin/* /usr/local/bin/\n",
        "\n",
        "!wget https://github.com/QIICR/dcmqi/releases/download/latest/dcmqi-1.2.4-linux-20220526-e25cb30.tar.gz\n",
        "!tar zxvf dcmqi-1.2.4-linux-20220526-e25cb30.tar.gz\n",
        "!cp dcmqi-1.2.4-linux-20220526-e25cb30/bin/* /usr/local/bin/\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print ('time to install: ' + str(end_time-start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKTSJWVjQlvF"
      },
      "source": [
        "Create directory tree for both"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7KYrV2xGQnTJ"
      },
      "outputs": [],
      "source": [
        "# create the directory tree for both \n",
        "\n",
        "!mkdir -p data models output\n",
        "\n",
        "!mkdir -p data/raw \n",
        "!mkdir -p data/raw/tmp \n",
        "!mkdir -p data/raw/nlst\n",
        "!mkdir -p data/raw/nlst/dicom\n",
        "\n",
        "# create the directory tree for nnUnet\n",
        "\n",
        "!mkdir -p data/processed\n",
        "!mkdir -p data/processed/nlst\n",
        "!mkdir -p data/processed/nlst/nii\n",
        "!mkdir -p data/processed/nlst/nnunet/nrrd\n",
        "!mkdir -p data/processed/nlst/nnunet/dicomseg\n",
        "\n",
        "!mkdir -p data/nnunet/model_input/\n",
        "!mkdir -p data/nnunet/nnunet_output/\n",
        "\n",
        "# create the directory free for bpr \n",
        "\n",
        "!mkdir -p data/bpr/model_input/\n",
        "!mkdir -p data/bpr/model_input_tmp/\n",
        "!mkdir -p data/bpr/bpr_output/\n",
        "!mkdir -p data/bpr/bpr_output_tmp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzeguuFbQiG1"
      },
      "source": [
        "### nnUnet "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1GXUtRRrIlT"
      },
      "source": [
        "Copy the JSON metadata file (generated using [...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQiQ9Zw9rMoD",
        "outputId": "6fa0a170-3769-4251-d31b-fc0c245a4708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp s3://idc-medima-paper/nnunet/data/dicomseg_metadata.json /content/data/dicomseg_metadata.json\n"
          ]
        }
      ],
      "source": [
        "bucket_data_base_uri = os.path.join(bucket_base_uri, \"nnunet/data\")\n",
        "# dicomseg_json_uri = os.path.join(bucket_data_base_uri, \"dicomseg_metadata.json\")\n",
        "# temporary \n",
        "# dicomseg_json_uri = \"gs://idc-medima-paper/nnunet/data/dicomseg_metadata.json\"\n",
        "dicomseg_json_uri = \"s3://idc-medima-paper/nnunet/data/dicomseg_metadata.json\"\n",
        "dicomseg_json_path = \"/content/data/dicomseg_metadata.json\"\n",
        "\n",
        "# !gsutil cp $dicomseg_json_uri $dicomseg_json_path\n",
        "!/content/s5cmd --endpoint-url https://storage.googleapis.com cp $dicomseg_json_uri $dicomseg_json_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7y3FRa7rbyr"
      },
      "source": [
        "Download the segmentation models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ79vTL5ef11",
        "outputId": "06f51c18-a769-4b95-d4fc-e7abf88b0dc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp s3://idc-medima-paper/nnunet/model/Task055_SegTHOR.zip /content/models/Task055_SegTHOR.zip\n"
          ]
        }
      ],
      "source": [
        "# FIXME: download from pvt Dropbox to speed up the development\n",
        "#        the final notebook should use the official resources only (Zenodo)\n",
        "# s5cmd download from bucket is much faster than Dropbox - switched\n",
        "seg_model_url = \"https://www.dropbox.com/s/m7es2ojn8h0ybhv/Task055_SegTHOR.zip?dl=0\"\n",
        "model_download_path = \"/content/models/Task055_SegTHOR.zip\"\n",
        "\n",
        "#!wget -O $model_download_path $seg_model_url\n",
        "!/content/s5cmd --endpoint-url https://storage.googleapis.com cp s3://idc-medima-paper/nnunet/model/Task055_SegTHOR.zip $model_download_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bne4bNo5fAsx"
      },
      "source": [
        "Initialize a few environment variables [...]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IIIjeFdUe1EA"
      },
      "outputs": [],
      "source": [
        "os.environ[\"RESULTS_FOLDER\"] = \"/content/data/nnunet/nnunet_output/\"\n",
        "os.environ[\"WEIGHTS_FOLDER\"] = \"/content/data/nnunet/nnunet_output/nnUNet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx8bWGVvew3P",
        "outputId": "0ad1dffb-687e-4235-e3e6-650e6d0b0437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
            "\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "\n",
            "nnUNet_raw_data_base is not defined and nnU-Net can only be used on data for which preprocessed files are already present on your system. nnU-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\n",
            "nnUNet_preprocessed is not defined and nnU-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "!nnUNet_install_pretrained_model_from_zip $model_download_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFw5dkW3QpwA"
      },
      "source": [
        "### BPR "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwxFv0pAOFRw",
        "outputId": "8cbc3ecd-dad4-4bb6-c6d3-e1eee575de71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp s3://idc-medima-paper/bpr/model/bpr_model.zip /content/models/bpr_model.zip\n",
            "Archive:  /content/models/bpr_model.zip\n",
            "   creating: /content/models/bpr_model/public_bpr_model/\n",
            "  inflating: /content/models/bpr_model/public_bpr_model/reference.xlsx  \n",
            "  inflating: /content/models/bpr_model/public_bpr_model/inference-settings.json  \n",
            "  inflating: /content/models/bpr_model/public_bpr_model/model.pt  \n",
            "  inflating: /content/models/bpr_model/public_bpr_model/config.json  \n"
          ]
        }
      ],
      "source": [
        "# Download BPR model \n",
        "# bpr_model_url = \"https://zenodo.org/record/5113483/files/public_bpr_model.zip?download=1\"\n",
        "bpr_model_url = \"https://zenodo.org/record/5113483/files/public_bpr_model.zip\"\n",
        "model_download_path = \"/content/models/bpr_model.zip\"\n",
        "#!wget -O $model_download_path $bpr_model_url\n",
        "\n",
        "# switched to download from the bucket, as it is much faster\n",
        "!/content/s5cmd --endpoint-url https://storage.googleapis.com cp s3://idc-medima-paper/bpr/model/bpr_model.zip $model_download_path \n",
        "\n",
        "# unzip it \n",
        "model_extract_path = \"/content/models/bpr_model\"\n",
        "!unzip $model_download_path -d $model_extract_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JBJmF7rmz1S"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_Z2S0OdAtkz"
      },
      "source": [
        "---\n",
        "\n",
        "# **Function Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtbQY2pf6UlF"
      },
      "source": [
        "## **Data Download and Preparation**\n",
        "\n",
        "The following function handles the download of a single patient data from the IDC buckets using `gsutil cp`. Furthermore, to organise the data in a more human-understandable and, above all, standardized fashion, the function makes use of [DICOMSort](https://github.com/pieper/dicomsort).\n",
        "\n",
        "DICOMSort is an open source tool for custom sorting and renaming of dicom files based on their specific DICOM tags. In our case, we will exploit DICOMSort to organise the DICOM data by `PatientID` and `Modality` - so that the final directory will look like the following:\n",
        "\n",
        "```\n",
        "raw/nsclc-radiomics/dicom/$PatientID\n",
        " └─── CT\n",
        "       ├─── $SOPInstanceUID_slice0.dcm\n",
        "       ├─── $SOPInstanceUID_slice1.dcm\n",
        "       ├───  ...\n",
        "       │\n",
        "      RTSTRUCT \n",
        "       ├─── $SOPInstanceUID_RTSTRUCT.dcm\n",
        "      SEG\n",
        "       └─── $SOPInstanceUID_RTSEG.dcm\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mAkq03N9A0np"
      },
      "outputs": [],
      "source": [
        "def download_patient_data(raw_base_path, sorted_base_path,\n",
        "                          patient_df, remove_raw = True):\n",
        "\n",
        "  \"\"\"\n",
        "  Download raw DICOM data and run dicomsort to standardise the input format.\n",
        "\n",
        "  Arguments:\n",
        "    raw_base_path    : required - path to the folder where the raw data will be stored.\n",
        "    sorted_base_path : required - path to the folder where the sorted data will be stored.\n",
        "    patient_df       : required - Pandas dataframe (returned from BQ) storing all the\n",
        "                                  patient information required to pull data from the IDC buckets.\n",
        "    remove_raw       : optional - whether to remove or not the raw non-sorted data\n",
        "                                  (after sorting with dicomsort). Defaults to True.\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # FIXME: this gets overwritten every single time; use `tempfile` library?\n",
        "  gs_file_path = \"gcs_paths.txt\"\n",
        "  patient_df[\"gcs_url\"].to_csv(gs_file_path, header = False, index = False)\n",
        "\n",
        "  pat_id = patient_df[\"PatientID\"].values[0]\n",
        "  download_path = os.path.join(raw_base_path, pat_id)\n",
        "\n",
        "  if not os.path.exists(download_path):\n",
        "    os.mkdir(download_path)\n",
        "\n",
        "  # FIXME: ok for a notebook; for scripting, change this to `subprocess`\n",
        "\n",
        "  start_time = time.time()\n",
        "  print(\"Copying files from IDC buckets to %s...\"%(download_path))\n",
        "  !cat $gs_file_path | gsutil -q -m cp -Ir $download_path\n",
        "  elapsed = time.time() - start_time\n",
        "  print(\"Done in %g seconds.\"%elapsed)\n",
        "\n",
        "  start_time = time.time()\n",
        "  print(\"\\nSorting DICOM files...\" )\n",
        "  !python src/dicomsort/dicomsort.py -u -k $download_path $sorted_base_path/%PatientID/%Modality/%SOPInstanceUID.dcm\n",
        "  elapsed = time.time() - start_time\n",
        "  print(\"Done in %g seconds.\"%elapsed)\n",
        "\n",
        "  print(\"Sorted DICOM data saved at: %s\"%(os.path.join(sorted_base_path, pat_id)))\n",
        "\n",
        "  # get rid of the temporary folder, storing the unsorted DICOM data \n",
        "  if remove_raw:\n",
        "    print(\"Removing un-sorted data at %s...\"%(download_path))\n",
        "    !rm -r $download_path\n",
        "    print(\"... Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9KJzUZlvUL6F"
      },
      "outputs": [],
      "source": [
        "def download_series_data(raw_base_path, \n",
        "                         sorted_base_path,\n",
        "                         series_df, \n",
        "                         remove_raw = True):\n",
        "\n",
        "  \"\"\"\n",
        "  Download raw DICOM data and run dicomsort to standardise the input format.\n",
        "\n",
        "  Arguments:\n",
        "    raw_base_path    : required - path to the folder where the raw data will be stored.\n",
        "    sorted_base_path : required - path to the folder where the sorted data will be stored.\n",
        "    series_df        : required - Pandas dataframe (returned from BQ) storing all the\n",
        "                                  series information required to pull data from the IDC buckets.\n",
        "    remove_raw       : optional - whether to remove or not the raw non-sorted data\n",
        "                                  (after sorting with dicomsort). Defaults to True.\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # FIXME: this gets overwritten every single time; use `tempfile` library?\n",
        "  gs_file_path = \"gcs_paths.txt\"\n",
        "  series_df[\"gcs_url\"].to_csv(gs_file_path, header = False, index = False)\n",
        "\n",
        "  series_id = series_df[\"SeriesInstanceUID\"].values[0]\n",
        "  download_path = os.path.join(raw_base_path, series_id)\n",
        "\n",
        "  # series_id = series_df[\"SeriesInstanceUID\"].values[0]\n",
        "  # print ('series_id: ' + str(series_id))\n",
        "  # # get the gcs_url on the fly \n",
        "  # client = bigquery.Client(project=project_id)\n",
        "  # query = \"\"\"\n",
        "  #   SELECT\n",
        "  #     gcs_url, \n",
        "  #   FROM\n",
        "  #     `bigquery-public-data.idc_current.dicom_all` \n",
        "  #   WHERE\n",
        "  #     SeriesInstanceUID IN UNNEST(@series_id)\n",
        "  # \"\"\" \n",
        "  # job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ArrayQueryParameter(\"series_id\", \"STRING\", [series_id])])\n",
        "  # df = client.query(query, job_config=job_config).to_dataframe()\n",
        "\n",
        "  # gs_file_path = \"gcs_paths.txt\"\n",
        "  # df[\"gcs_url\"].to_csv(gs_file_path, header = False, index = False)\n",
        "  # download_path = os.path.join(raw_base_path, series_id)\n",
        "\n",
        "  if not os.path.exists(download_path):\n",
        "    os.mkdir(download_path)\n",
        "\n",
        "  # FIXME: ok for a notebook; for scripting, change this to `subprocess`\n",
        "\n",
        "  start_time = time.time()\n",
        "  print(\"Copying files from IDC buckets to %s...\"%(download_path))\n",
        "  # !cat $gs_file_path | gsutil -q -m cp -Ir $download_path\n",
        "  !cat $gs_file_path | gsutil -q -m cp -I $download_path\n",
        "\n",
        "  elapsed = time.time() - start_time\n",
        "  print(\"Done in %g seconds.\"%elapsed)\n",
        "\n",
        "  start_time = time.time()\n",
        "  print(\"\\nSorting DICOM files...\" )\n",
        "  # !python src/dicomsort/dicomsort.py -u $download_path $sorted_base_path/%PatientID/%Modality/%SOPInstanceUID.dcm\n",
        "  !python src/dicomsort/dicomsort.py -u $download_path $sorted_base_path/%SeriesInstanceUID/%Modality/%SOPInstanceUID.dcm\n",
        "  elapsed = time.time() - start_time\n",
        "  print(\"Done in %g seconds.\"%elapsed)\n",
        "\n",
        "  print(\"Sorted DICOM data saved at: %s\"%(os.path.join(sorted_base_path, series_id)))\n",
        "\n",
        "  # get rid of the temporary folder, storing the unsorted DICOM data \n",
        "  if remove_raw:\n",
        "    print(\"Removing un-sorted data at %s...\"%(download_path))\n",
        "    !rm -r $download_path\n",
        "    print(\"... Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0JDNAPJivEUm"
      },
      "outputs": [],
      "source": [
        "def download_series_data_s5cmd(raw_base_path, \n",
        "                               sorted_base_path,\n",
        "                               series_df, \n",
        "                               remove_raw = True):\n",
        "\n",
        "  \"\"\"\n",
        "  Download raw DICOM data and run dicomsort to standardise the input format.\n",
        "  Uses s5cmd instead of gsutil which allows for a faster download. \n",
        "\n",
        "  Arguments:\n",
        "    raw_base_path    : required - path to the folder where the raw data will be stored.\n",
        "    sorted_base_path : required - path to the folder where the sorted data will be stored.\n",
        "    series_df        : required - Pandas dataframe (returned from BQ) storing all the\n",
        "                                  series information required to pull data from the IDC buckets.\n",
        "    remove_raw       : optional - whether to remove or not the raw non-sorted data\n",
        "                                  (after sorting with dicomsort). Defaults to True.\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # Get and create the download path \n",
        "  series_id = series_df[\"SeriesInstanceUID\"].values[0]\n",
        "  download_path = os.path.join(raw_base_path, series_id)\n",
        "  if not os.path.exists(download_path):\n",
        "    os.mkdir(download_path)\n",
        "\n",
        "  # Create the text file to hold gsc_url \n",
        "  gcsurl_temp = \"cp \" + series_df[\"gcs_url\"].str.replace(\"gs://\",\"s3://\") + \" \" + download_path \n",
        "  gs_file_path = \"gcs_paths.txt\"\n",
        "  gcsurl_temp.to_csv(gs_file_path, header = False, index = False)\n",
        "\n",
        "  # Download using s5cmd \n",
        "  start_time = time.time()\n",
        "  download_cmd = [\"/content/s5cmd\",\"--endpoint-url\", \"https://storage.googleapis.com\", \"run\", gs_file_path]\n",
        "  proc = subprocess.Popen(download_cmd)\n",
        "  proc.wait()\n",
        "  elapsed = time.time() - start_time \n",
        "  print (\"Done download in %g seconds.\"%elapsed)\n",
        "\n",
        "  # Sort files \n",
        "  start_time = time.time()\n",
        "  print(\"\\nSorting DICOM files...\" )\n",
        "  # !python src/dicomsort/dicomsort.py -u $download_path $sorted_base_path/%PatientID/%Modality/%SOPInstanceUID.dcm\n",
        "  !python src/dicomsort/dicomsort.py -u $download_path $sorted_base_path/%SeriesInstanceUID/%Modality/%SOPInstanceUID.dcm\n",
        "  elapsed = time.time() - start_time\n",
        "  print(\"Done sorting in %g seconds.\"%elapsed)\n",
        "\n",
        "  print(\"Sorted DICOM data saved at: %s\"%(os.path.join(sorted_base_path, series_id)))\n",
        "\n",
        "  # get rid of the temporary folder, storing the unsorted DICOM data \n",
        "  if remove_raw:\n",
        "    print(\"Removing un-sorted data at %s...\"%(download_path))\n",
        "    !rm -r $download_path\n",
        "    print(\"... Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLGqxh6063lE"
      },
      "source": [
        "---\n",
        "\n",
        "## **Data Preprocessing**\n",
        "\n",
        "Brief description here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "S1L6gQ0A_gr_"
      },
      "outputs": [],
      "source": [
        "def pypla_dicom_ct_to_nrrd(sorted_base_path, processed_nrrd_path,\n",
        "                           pat_id, verbose = True):\n",
        "  \n",
        "  \"\"\"\n",
        "  Sorted DICOM patient data to NRRD file (CT volume).\n",
        "\n",
        "  Arguments:\n",
        "    sorted_base_path    : required - path to the folder where the sorted data should be stored.\n",
        "    processed_nrrd_path : required - path to the folder where the preprocessed NRRD data are stored\n",
        "    remove_raw          : required - patient ID (used for naming purposes).\n",
        "    verbose             : optional - whether to run pyplastimatch in verbose mode. Defaults to true.\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # given that everything is standardised already, compute the paths\n",
        "  path_to_dicom_ct_folder = os.path.join(sorted_base_path, pat_id, \"CT\")\n",
        "  \n",
        "  # sanity check\n",
        "  assert(os.path.exists(path_to_dicom_ct_folder))\n",
        "  \n",
        "  pat_dir_nrrd_path = os.path.join(processed_nrrd_path, pat_id)\n",
        "  if not os.path.exists(pat_dir_nrrd_path):\n",
        "    os.mkdir(pat_dir_nrrd_path)\n",
        "\n",
        "  # output NRRD CT\n",
        "  ct_nrrd_path = os.path.join(pat_dir_nrrd_path, pat_id + \"_CT.nrrd\")\n",
        "\n",
        "  # logfile for the plastimatch conversion\n",
        "  log_file_path = os.path.join(pat_dir_nrrd_path, pat_id + '_pypla.log')\n",
        "\n",
        "  # DICOM CT to NRRD conversion (if the file doesn't exist yet)\n",
        "  if not os.path.exists(ct_nrrd_path):\n",
        "    convert_args_ct = {\"input\" : path_to_dicom_ct_folder,\n",
        "                       \"output-img\" : ct_nrrd_path}\n",
        "\n",
        "    # clean old log file if it exist\n",
        "    if os.path.exists(log_file_path): os.remove(log_file_path)\n",
        "    \n",
        "    pypla.convert(verbose = verbose,\n",
        "                  path_to_log_file = log_file_path,\n",
        "                  **convert_args_ct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "o-pd_33z-vWy"
      },
      "outputs": [],
      "source": [
        "def pypla_dicom_ct_to_nrrd_resample(sorted_base_path, processed_nrrd_path,\n",
        "                                    pat_id, verbose = True):\n",
        "  \n",
        "  \"\"\"\n",
        "  Sorted DICOM patient data to NRRD file (CT volume).\n",
        "  The nrrd volume is resampled to 1x1x2.5, from the supplementary segThor material. \n",
        "  The header of the nrrd volume is just used to create the nrrd files of the individual \n",
        "  segments, it is not used otherwise. \n",
        "\n",
        "  Arguments:\n",
        "    sorted_base_path    : required - path to the folder where the sorted data should be stored.\n",
        "    processed_nrrd_path : required - path to the folder where the preprocessed NRRD data are stored\n",
        "    remove_raw          : required - patient ID (used for naming purposes).\n",
        "    verbose             : optional - whether to run pyplastimatch in verbose mode. Defaults to true.\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # given that everything is standardised already, compute the paths\n",
        "  path_to_dicom_ct_folder = os.path.join(sorted_base_path, pat_id, \"CT\")\n",
        "  \n",
        "  # sanity check\n",
        "  assert(os.path.exists(path_to_dicom_ct_folder))\n",
        "  \n",
        "  pat_dir_nrrd_path = os.path.join(processed_nrrd_path, pat_id)\n",
        "  if not os.path.exists(pat_dir_nrrd_path):\n",
        "    os.mkdir(pat_dir_nrrd_path)\n",
        "\n",
        "  # output NRRD CT\n",
        "  # ct_nrrd_path = os.path.join(pat_dir_nrrd_path, pat_id + \"_CT.nrrd\")\n",
        "  ct_nrrd_path = os.path.join(pat_dir_nrrd_path, pat_id + \"_CT_orig.nrrd\")\n",
        "\n",
        "  # logfile for the plastimatch conversion\n",
        "  log_file_path = os.path.join(pat_dir_nrrd_path, pat_id + '_pypla.log')\n",
        "\n",
        "  # DICOM CT to NRRD conversion (if the file doesn't exist yet)\n",
        "  if not os.path.exists(ct_nrrd_path):\n",
        "    convert_args_ct = {\"input\" : path_to_dicom_ct_folder,\n",
        "                       \"output-img\" : ct_nrrd_path}\n",
        "\n",
        "    # clean old log file if it exist\n",
        "    if os.path.exists(log_file_path): os.remove(log_file_path)\n",
        "    \n",
        "    pypla.convert(verbose = verbose,\n",
        "                  path_to_log_file = log_file_path,\n",
        "                  **convert_args_ct)\n",
        "    \n",
        "  # Perform resampling of the original CT nrrd file and save as _CT.nrrd \n",
        "  # The header of this is used to create the individual nrrd segments. \n",
        "  ct_nrrd_path_resample = os.path.join(pat_dir_nrrd_path, pat_id + \"_CT.nrrd\") \n",
        "  log_file_path = os.path.join(pat_dir_nrrd_path, pat_id + '_pypla_resample.log')\n",
        "  resample_args_ct_nrrd = {\"input\" : ct_nrrd_path,\n",
        "                           \"output\" : ct_nrrd_path_resample,\n",
        "                           \"spacing\": \"1 1 2.5\"}\n",
        "  pypla.resample(verbose = verbose,\n",
        "                path_to_log_file = log_file_path,\n",
        "                **resample_args_ct_nrrd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIs8aIubGq9X"
      },
      "source": [
        "---\n",
        "\n",
        "Brief description here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1hBTKAfp_bnM"
      },
      "outputs": [],
      "source": [
        "def pypla_dicom_ct_to_nifti(sorted_base_path, processed_nifti_path,\n",
        "                            pat_id, verbose = True):\n",
        "  \n",
        "  \"\"\"\n",
        "  Sorted DICOM patient data to NIfTI file (CT volume).\n",
        "\n",
        "  Arguments:\n",
        "    sorted_base_path     : required - path to the folder where the sorted data should be stored.\n",
        "    processed_nifti_path : required - path to the folder where the preprocessed NIfTI data are stored\n",
        "    remove_raw           : required - patient ID (used for naming purposes).\n",
        "    verbose              : optional - whether to run pyplastimatch in verbose mode. Defaults to true.\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # given that everything is standardised already, compute the paths\n",
        "  path_to_dicom_ct_folder = os.path.join(sorted_base_path, pat_id, \"CT\")\n",
        "  \n",
        "  # sanity check\n",
        "  assert(os.path.exists(path_to_dicom_ct_folder))\n",
        "  \n",
        "  pat_dir_nifti_path = os.path.join(processed_nifti_path, pat_id)\n",
        "  if not os.path.exists(pat_dir_nifti_path):\n",
        "    os.mkdir(pat_dir_nifti_path)\n",
        "\n",
        "  # output nii CT\n",
        "  ct_nifti_path = os.path.join(pat_dir_nifti_path, pat_id + \"_CT.nii.gz\")\n",
        "\n",
        "  # logfile for the plastimatch conversion\n",
        "  log_file_path = os.path.join(pat_dir_nifti_path, pat_id + '_pypla.log')\n",
        "\n",
        "  # DICOM CT to nii conversion (if the file doesn't exist yet)\n",
        "  if not os.path.exists(ct_nifti_path):\n",
        "    convert_args_ct = {\"input\" : path_to_dicom_ct_folder,\n",
        "                       \"output-img\" : ct_nifti_path}\n",
        "\n",
        "    # clean old log file if it exist\n",
        "    if os.path.exists(log_file_path): os.remove(log_file_path)\n",
        "    \n",
        "    pypla.convert(verbose = verbose,\n",
        "                  path_to_log_file = log_file_path,\n",
        "                  **convert_args_ct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSMNA0ESH3O6"
      },
      "source": [
        "---\n",
        "\n",
        "Brief description here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LVeSbb7-_kXS"
      },
      "outputs": [],
      "source": [
        "def pypla_dicom_rtstruct_to_nrrd(sorted_base_path, processed_nrrd_path,\n",
        "                                 pat_id, verbose = True):\n",
        "  \n",
        "  \"\"\"\n",
        "  Sorted DICOM patient data to NRRD file (RTSTRUCT).\n",
        "\n",
        "  Arguments:\n",
        "    sorted_base_path    : required - path to the folder where the sorted data should be stored.\n",
        "    processed_nrrd_path : required - path to the folder where the preprocessed NRRD data are stored\n",
        "    remove_raw          : required - patient ID (used for naming purposes).\n",
        "    verbose             : optional - whether to run pyplastimatch in verbose mode. Defaults to true.\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # given that everything is standardised already, compute the paths\n",
        "  path_to_dicom_ct_folder = os.path.join(sorted_base_path, pat_id, \"CT\")\n",
        "  path_to_dicom_rt_folder = os.path.join(sorted_base_path, pat_id, \"RTSTRUCT\")\n",
        "\n",
        "  pat_dir_nrrd_path = os.path.join(processed_nrrd_path, pat_id)\n",
        "\n",
        "  # sanity check\n",
        "  assert(os.path.exists(path_to_dicom_rt_folder))\n",
        "  assert(os.path.exists(pat_dir_nrrd_path))\n",
        "\n",
        "  # output NRRD CT\n",
        "  rt_folder_path = os.path.join(pat_dir_nrrd_path, \"rt_segmasks\")\n",
        "  rt_list_path = os.path.join(rt_folder_path, pat_id + \"_rt_list.txt\")\n",
        "\n",
        "  # path to the file storing the names of the exported segmentation masks\n",
        "  # (from the DICOM RTSTRUCT)\n",
        "  log_file_path = os.path.join(pat_dir_nrrd_path, pat_id + '_pypla.log')\n",
        "\n",
        "  # DICOM CT to NRRD conversion (if the file doesn't exist yet)\n",
        "  if not os.path.exists(rt_folder_path):\n",
        "    convert_args_rt = {\"input\" : path_to_dicom_rt_folder, \n",
        "                       \"referenced-ct\" : path_to_dicom_ct_folder,\n",
        "                       \"output-prefix\" : rt_folder_path,\n",
        "                       \"prefix-format\" : 'nrrd',\n",
        "                       \"output-ss-list\" : rt_list_path}\n",
        "\n",
        "    \n",
        "    pypla.convert(verbose = verbose,\n",
        "                  path_to_log_file = log_file_path,\n",
        "                  **convert_args_rt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQoFWJasfror"
      },
      "source": [
        "---\n",
        "\n",
        "Brief description here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "9I0X9fFNfvWG"
      },
      "outputs": [],
      "source": [
        "def prep_input_data(processed_nifti_path, model_input_folder, pat_id):\n",
        "  \n",
        "  \"\"\"\n",
        "  Sorted DICOM patient data to NRRD file (RTSTRUCT).\n",
        "\n",
        "  Arguments:\n",
        "    src_folder : required - path to the folder where the sorted data should be stored.\n",
        "    dst_folder : required - path to the folder where the preprocessed NRRD data are stored\n",
        "    pat_id     : required - patient ID (used for naming purposes).\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # FIXME: ok for a notebook; for scripting, change this to `shutil`\n",
        "\n",
        "  pat_dir_nifti_path = os.path.join(processed_nifti_path, pat_id)\n",
        "  ct_nifti_path = os.path.join(pat_dir_nifti_path, pat_id + \"_CT.nii.gz\")\n",
        "  \n",
        "  copy_to_path = os.path.join(model_input_folder, pat_id + \"_0000.nii.gz\")\n",
        "    \n",
        "  # copy NIfTI to the right dir for nnU-Net processing\n",
        "  if not os.path.exists(copy_to_path):\n",
        "    print(\"Copying %s\\nto %s...\"%(ct_nifti_path, copy_to_path))\n",
        "    !cp $ct_nifti_path $copy_to_path\n",
        "    print(\"... Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1VRruV6ISj0B"
      },
      "outputs": [],
      "source": [
        "def prep_input_data_resample(processed_nifti_path, model_input_folder, pat_id, verbose = True):\n",
        "  \n",
        "  \"\"\"\n",
        "  After converting the CT to nifti, this function resamples the CT volume and \n",
        "  copies the resampled one to the model_input_folder. \n",
        "\n",
        "  Arguments:\n",
        "    processed_nifti_path : path where the converted nifti file is stored \n",
        "    model_input_folder   : input folder for nnuNet \n",
        "    pat_id               : required - patient ID (used for naming purposes).\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # FIXME: ok for a notebook; for scripting, change this to `shutil`\n",
        "\n",
        "  pat_dir_nifti_path = os.path.join(processed_nifti_path, pat_id)\n",
        "  ct_nifti_path = os.path.join(pat_dir_nifti_path, pat_id + \"_CT.nii.gz\")\n",
        "\n",
        "  # Resample, and then copy \n",
        "  ct_nifti_path_resample = os.path.join(pat_dir_nifti_path, pat_id + \"_CT_resample.nii.gz\")\n",
        "  log_file_path = os.path.join(pat_dir_nifti_path, 'resample_log.log')\n",
        "  convert_args_ct = {\"input\" : ct_nifti_path,\n",
        "                      \"output\" : ct_nifti_path_resample,\n",
        "                      \"spacing\": \"1 1 2.5\"}\n",
        "  pypla.resample(verbose = verbose,\n",
        "                path_to_log_file = log_file_path,\n",
        "                **convert_args_ct)\n",
        "\n",
        "  copy_to_path = os.path.join(model_input_folder, pat_id + \"_0000.nii.gz\")\n",
        "    \n",
        "  # copy NIfTI to the right dir for nnU-Net processing\n",
        "  # delete the resampled file \n",
        "  if not os.path.exists(copy_to_path):\n",
        "    print(\"Copying %s\\nto %s...\"%(ct_nifti_path_resample, copy_to_path))\n",
        "    !cp $ct_nifti_path_resample $copy_to_path\n",
        "    # !rm $ct_nifti_path_resample \n",
        "    print(\"... Done.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "v0lqtQ8vNlkW"
      },
      "outputs": [],
      "source": [
        "def prep_input_data_bpr(processed_nifti_path, model_input_folder, pat_id):\n",
        "  \n",
        "  \"\"\"\n",
        "  Copies the nifti file to the appropriate model_input_folder\n",
        "\n",
        "  Arguments:\n",
        "    processed_nifti_path : required - path to the folder where the input nifti are stored \n",
        "    model_input_folder   : required - path to the folder where the nifti files for BPR will be stored\n",
        "    pat_id               : required - patient ID (used for naming purposes).\n",
        "  \n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  # FIXME: ok for a notebook; for scripting, change this to `shutil`\n",
        "\n",
        "  pat_dir_nifti_path = os.path.join(processed_nifti_path, pat_id)\n",
        "  ct_nifti_path = os.path.join(pat_dir_nifti_path, pat_id + \"_CT.nii.gz\")\n",
        "  \n",
        "  # copy_to_path = os.path.join(model_input_folder, pat_id + \"_0000.nii.gz\")\n",
        "  # copy_to_path = os.path.join(model_input_folder, pat_id + \"_CT.nii.gz\")\n",
        "  copy_to_path = os.path.join(model_input_folder, pat_id + \".nii.gz\")\n",
        "    \n",
        "  # copy NIfTI to the right dir for bpr processing\n",
        "  if not os.path.exists(copy_to_path):\n",
        "    print(\"Copying %s\\nto %s...\"%(ct_nifti_path, copy_to_path))\n",
        "    !cp $ct_nifti_path $copy_to_path\n",
        "    print(\"... Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-7IE4iEIBpD"
      },
      "source": [
        "---\n",
        "\n",
        "## **Data Processing**\n",
        "\n",
        "Brief description here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZINfBYSxIKH8"
      },
      "outputs": [],
      "source": [
        "def process_patient_nnunet(model_input_folder, model_output_folder, \n",
        "                           nnunet_model, use_tta = False, export_prob_maps = False,\n",
        "                           verbose = False):\n",
        "\n",
        "  \"\"\"\n",
        "  Infer the thoracic organs at risk segmentation maps using one of the nnU-Net models.\n",
        "\n",
        "  Arguments:\n",
        "    model_input_folder  : required - path to the folder where the data to be inferred should be stored.\n",
        "    model_output_folder : required - path to the folder where the inferred segmentation masks will be stored.\n",
        "    nnunet_model        : required - pre-trained nnU-Net model to use during the inference phase.\n",
        "    use_tta             : optional - whether to use or not test time augmentation (TTA). Defaults to False.\n",
        "    export_prob_maps    : optional - whether to export or not softmax probabilities. Defaults to False.\n",
        "    verbose             : optional - whether to output text from `nnUNet_predict` or not. Defaults to False.\n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "  \n",
        "  export_prob_maps = \"--save_npz\" if export_prob_maps == True else \"\"\n",
        "  direct_to = \"\" if verbose == True else \"> /dev/null\"\n",
        "  use_tta = \"\" if use_tta == True else \"--disable_tta\"\n",
        "\n",
        "  assert(nnunet_model in [\"2d\", \"3d_lowres\", \"3d_fullres\", \"3d_cascade_fullres\"])\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  print(\"Running `nnUNet_predict` with `%s` model...\"%(nnunet_model))\n",
        "\n",
        "  pat_fn_list = sorted([f for f in os.listdir(model_input_folder) if \".nii.gz\" in f])\n",
        "  pat_fn_path = os.path.join(model_input_folder, pat_fn_list[-1])\n",
        "\n",
        "  print(\"Processing file at %s...\"%(pat_fn_path))\n",
        "\n",
        "  # run the inference phase\n",
        "  # accepted options for --model are: 2d, 3d_lowres, 3d_fullres or 3d_cascade_fullres\n",
        "  !nnUNet_predict --input_folder $model_input_folder \\\n",
        "                  --output_folder $model_output_folder \\\n",
        "                  --task_name \"Task055_SegTHOR\" \\\n",
        "                  --num_threads_preprocessing 1 \\\n",
        "                  --num_threads_nifti_save 1 \\\n",
        "                  --model $nnunet_model $use_tta $direct_to $export_prob_maps\n",
        "\n",
        "  elapsed = time.time() - start_time\n",
        "\n",
        "  print(\"Done in %g seconds.\"%elapsed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "vIwS_f9gRou0"
      },
      "outputs": [],
      "source": [
        "def process_patient_bpr(model_input_folder, model_output_folder, model):\n",
        "  \n",
        "  \"\"\"\n",
        "  Predict the body part region scores from the model_input_folder. \n",
        "\n",
        "  Arguments:\n",
        "    model_input_folder  : required - path to the folder where the data to be inferred should be stored.\n",
        "    model_output_folder : required - path to the folder where the output json files will be stored.\n",
        "\n",
        "  Outputs:\n",
        "    This function produces a json file per input nifti that stores the body part scores. \n",
        "  \"\"\"\n",
        "\n",
        "  start_time = time.time()\n",
        "  # bpreg_inference(model_input_folder, model_output_folder)\n",
        "  bpreg_inference(model_input_folder, model_output_folder, model)\n",
        "  end_time = time.time()\n",
        "  elapsed = end_time-start_time \n",
        "  print(\"Done in %g seconds.\"%elapsed)\n",
        "\n",
        "\n",
        "  return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iENa_4flTd1m"
      },
      "source": [
        "---\n",
        "\n",
        "## **Data Postprocessing**\n",
        "\n",
        "Description here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "jHe5LSJ1f8pv"
      },
      "outputs": [],
      "source": [
        "def pypla_nifti_to_nrrd(pred_nifti_path, processed_nrrd_path,\n",
        "                        pat_id, verbose = True):\n",
        "  \n",
        "  \"\"\"\n",
        "  Sorted DICOM patient data to NRRD file (RTSTRUCT).\n",
        "\n",
        "  Arguments:\n",
        "    src_folder : required - path to the folder where the sorted data should be stored.\n",
        "    dst_folder : required - path to the folder where the preprocessed NRRD data are stored\n",
        "    pat_id     : required - patient ID (used for naming purposes).\n",
        "  \n",
        "  Returns:\n",
        "    pred_nrrd_path - \n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  pred_nrrd_path = os.path.join(processed_nrrd_path, pat_id, pat_id + \"_pred_segthor.nrrd\")\n",
        "  log_file_path = os.path.join(processed_nrrd_path, pat_id, pat_id + \"_pypla.log\")\n",
        "  \n",
        "  # Inferred NIfTI segmask to NRRD\n",
        "  convert_args_pred = {\"input\" : pred_nifti_path, \n",
        "                       \"output-img\" : pred_nrrd_path}\n",
        "\n",
        "  pypla.convert(verbose = verbose,\n",
        "                path_to_log_file = log_file_path,\n",
        "                **convert_args_pred)\n",
        "  \n",
        "  return pred_nrrd_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9XIdgwicapX"
      },
      "source": [
        "---\n",
        "\n",
        "Description here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "cTFv-slKU039"
      },
      "outputs": [],
      "source": [
        "def pypla_postprocess(processed_nrrd_path, model_output_folder, pat_id):\n",
        "\n",
        "  \"\"\"\n",
        "  Sorted DICOM patient data to NRRD file.\n",
        "\n",
        "  Arguments:\n",
        "    processed_nrrd_path  : required - path to the folder where the sorted data should be stored.\n",
        "    model_output_folder  : required - path to the folder where the inferred segmentation masks should be stored.\n",
        "    pat_id               : required - patient ID (used for naming purposes). \n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  pred_nifti_fn = pat_id + \".nii.gz\"\n",
        "  pred_nifti_path = os.path.join(model_output_folder, pred_nifti_fn)\n",
        "\n",
        "  pred_nrrd_path = pypla_nifti_to_nrrd(pred_nifti_path = pred_nifti_path,\n",
        "                                       processed_nrrd_path = processed_nrrd_path,\n",
        "                                       pat_id = pat_id, verbose = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvf4yN-DcT3N"
      },
      "source": [
        "---\n",
        "\n",
        "Description here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3lixiZVRcQAZ"
      },
      "outputs": [],
      "source": [
        "def numpy_to_nrrd(model_output_folder, processed_nrrd_path, pat_id,\n",
        "                  output_folder_name = \"pred_softmax\", output_dtype = \"uint8\",\n",
        "                  structure_list = [\"Background\", \"Esophagus\",\n",
        "                                    \"Heart\", \"Trachea\", \"Aorta\"]):\n",
        "\n",
        "  \"\"\"\n",
        "  Convert softmax probability maps to NRRD. For simplicity, the probability maps\n",
        "  are converted by default to UInt8\n",
        "\n",
        "  Arguments:\n",
        "    model_output_folder : required - path to the folder where the inferred segmentation masks should be stored.\n",
        "    processed_nrrd_path : required - path to the folder where the preprocessed NRRD data are stored.\n",
        "    pat_id              : required - patient ID (used for naming purposes).\n",
        "    output_folder_name  : optional - name of the subfolder under the patient directory \n",
        "                                     (under `processed_nrrd_path`) where the softmax NRRD\n",
        "                                     files will be saved. Defaults to \"pred_softmax\".\n",
        "    output_dtype        : optional - output data type. Float16 is not supported by the NRRD standard,\n",
        "                                     so the choice should be between uint8, uint16 or float32.\n",
        "                                     Please note this will greatly impact the size of the DICOM PM\n",
        "                                     file that will be generated.\n",
        "    structure_list      : optional - list of the structures whose probability maps are stored in the \n",
        "                                     first channel of the `.npz` file (output from the nnU-Net pipeline\n",
        "                                     when `export_prob_maps` is set to True). Defaults to the structure\n",
        "                                     list for the SegTHOR challenge (background = 0 included).\n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  pred_softmax_fn = pat_id + \".npz\"\n",
        "  pred_softmax_path = os.path.join(model_output_folder, pred_softmax_fn)\n",
        "\n",
        "  # parse NRRD file - we will make use of if to populate the header of the\n",
        "  # NRRD mask we are going to get from the inferred segmentation mask\n",
        "  ct_nrrd_path = os.path.join(processed_nrrd_path, pat_id, pat_id + \"_CT.nrrd\")\n",
        "  sitk_ct = sitk.ReadImage(ct_nrrd_path)\n",
        "\n",
        "  output_folder_path = os.path.join(processed_nrrd_path, pat_id, output_folder_name)\n",
        "  \n",
        "  if not os.path.exists(output_folder_path):\n",
        "    os.mkdir(output_folder_path)\n",
        "\n",
        "  pred_softmax_all = np.load(pred_softmax_path)[\"softmax\"]\n",
        "\n",
        "  for channel, structure in enumerate(structure_list):\n",
        "\n",
        "    # FIXME: NRRD does not support float16 tensors. For now, convert to a float32. \n",
        "    #        Then replace with a direct conversion to DICOM?\n",
        "\n",
        "    pred_softmax_segmask = pred_softmax_all[channel].astype(dtype = np.float32)\n",
        "\n",
        "    assert(output_dtype in [\"uint8\", \"uint16\", \"float32\"])      \n",
        "\n",
        "    if output_dtype == \"float32\":\n",
        "      # no rescale needed - the values will be between 0 and 1\n",
        "      # set SITK image dtype to Float32\n",
        "      sitk_dtype = sitk.sitkFloat32\n",
        "\n",
        "    elif output_dtype == \"uint8\":\n",
        "      # rescale between 0 and 255, quantize\n",
        "      pred_softmax_segmask = (255*pred_softmax_segmask).astype(np.int)\n",
        "      # set SITK image dtype to UInt8\n",
        "      sitk_dtype = sitk.sitkUInt8\n",
        "\n",
        "    elif output_dtype == \"uint16\":\n",
        "      # rescale between 0 and 65536\n",
        "      pred_softmax_segmask = (65536*pred_softmax_segmask).astype(int)\n",
        "      # set SITK image dtype to UInt16\n",
        "      sitk_dtype = sitk.sitkUInt16\n",
        "    \n",
        "    pred_softmax_segmask_sitk = sitk.GetImageFromArray(pred_softmax_segmask)\n",
        "    pred_softmax_segmask_sitk.CopyInformation(sitk_ct)\n",
        "    pred_softmax_segmask_sitk = sitk.Cast(pred_softmax_segmask_sitk, sitk_dtype)\n",
        "\n",
        "    output_fn = \"%s.nrrd\"%(structure)\n",
        "    output_path = os.path.join(output_folder_path, output_fn)\n",
        "\n",
        "    writer = sitk.ImageFileWriter()\n",
        "\n",
        "    writer.UseCompressionOn()\n",
        "    writer.SetFileName(output_path)\n",
        "    writer.Execute(pred_softmax_segmask_sitk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gSPfp96cRYK"
      },
      "source": [
        "---\n",
        "\n",
        "Description here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7WiApMZAjupQ"
      },
      "outputs": [],
      "source": [
        "def nrrd_to_dicomseg(sorted_base_path, processed_base_path,\n",
        "                     dicomseg_json_path, pat_id, skip_empty_slices = True):\n",
        "\n",
        "  \"\"\"\n",
        "  Export DICOM SEG object from segmentation masks stored in NRRD files.\n",
        "\n",
        "  Arguments:\n",
        "    sorted_base_path    : required - path to the folder where the sorted data should be stored.\n",
        "    processed_base_path : required - path to the folder where the preprocessed NRRD data are stored\n",
        "    dicomseg_json_path  : required - ...\n",
        "    pat_id              : required - patient ID (used for naming purposes). \n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  path_to_ct_dir = os.path.join(sorted_base_path, pat_id, \"CT\")\n",
        "\n",
        "  # processed_dicomseg_path = os.path.join(processed_base_path, \"dicomseg\")\n",
        "  processed_dicomseg_path = os.path.join(processed_base_path, \"nnunet\", \"dicomseg\")\n",
        "  pat_dir_dicomseg_path = os.path.join(processed_dicomseg_path, pat_id)\n",
        "\n",
        "  if not os.path.exists(pat_dir_dicomseg_path):\n",
        "    os.mkdir(pat_dir_dicomseg_path)\n",
        "\n",
        "  # DK added for now\n",
        "  # same as processed_nrrd_path_nnunet\n",
        "  processed_nrrd_path = os.path.join(processed_base_path, \"nnunet\", \"nrrd\")\n",
        "\n",
        "  pred_segmasks_nrrd = os.path.join(processed_nrrd_path, pat_id, pat_id + \"_pred_segthor.nrrd\")\n",
        "\n",
        "  dicom_seg_out_path = os.path.join(pat_dir_dicomseg_path, pat_id + \"_SEG.dcm\")\n",
        "\n",
        "  # transform from bool to int according to `itkimage2segimage` requirements\n",
        "  skip_flag = \"--skip\" if skip_empty_slices == True else \"\"\n",
        "\n",
        "  !itkimage2segimage --inputImageList $pred_segmasks_nrrd \\\n",
        "                     --inputDICOMDirectory $path_to_ct_dir \\\n",
        "                     --outputDICOM $dicom_seg_out_path \\\n",
        "                     --inputMetadata $dicomseg_json_path $skip_flag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OEovDWuW232V"
      },
      "outputs": [],
      "source": [
        "def nrrd_to_dicomseg_resampled(sorted_base_path, processed_base_path,\n",
        "                     dicomseg_json_path, pat_id, skip_empty_slices = True):\n",
        "\n",
        "  \"\"\"\n",
        "  Export DICOM SEG object from segmentation masks stored in NRRD files.\n",
        "  Takes as input the resampled segthor_nrrd file, which is resampled to the \n",
        "  original image spacing. \n",
        "\n",
        "  Arguments:\n",
        "    sorted_base_path    : required - path to the folder where the sorted data should be stored.\n",
        "    processed_base_path : required - path to the folder where the preprocessed NRRD data are stored\n",
        "    dicomseg_json_path  : required - ...\n",
        "    pat_id              : required - patient ID (used for naming purposes). \n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  path_to_ct_dir = os.path.join(sorted_base_path, pat_id, \"CT\")\n",
        "\n",
        "  # processed_dicomseg_path = os.path.join(processed_base_path, \"dicomseg\")\n",
        "  processed_dicomseg_path = os.path.join(processed_base_path, \"nnunet\", \"dicomseg\")\n",
        "  pat_dir_dicomseg_path = os.path.join(processed_dicomseg_path, pat_id)\n",
        "\n",
        "  if not os.path.exists(pat_dir_dicomseg_path):\n",
        "    os.mkdir(pat_dir_dicomseg_path)\n",
        "\n",
        "  # DK added for now\n",
        "  # same as processed_nrrd_path_nnunet\n",
        "  processed_nrrd_path = os.path.join(processed_base_path, \"nnunet\", \"nrrd\")\n",
        "\n",
        "  pred_segmasks_nrrd = os.path.join(processed_nrrd_path, pat_id, pat_id + \"_pred_segthor_resampled.nrrd\") # instead of _pred_segthor.nrrd \n",
        "\n",
        "  dicom_seg_out_path = os.path.join(pat_dir_dicomseg_path, pat_id + \"_SEG.dcm\")\n",
        "\n",
        "  # transform from bool to int according to `itkimage2segimage` requirements\n",
        "  skip_flag = \"--skip\" if skip_empty_slices == True else \"\"\n",
        "\n",
        "  !itkimage2segimage --inputImageList $pred_segmasks_nrrd \\\n",
        "                     --inputDICOMDirectory $path_to_ct_dir \\\n",
        "                     --outputDICOM $dicom_seg_out_path \\\n",
        "                     --inputMetadata $dicomseg_json_path $skip_flag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwYHPQwsj620"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKIzPeXuog9K"
      },
      "source": [
        "---\n",
        "\n",
        "## **General Utilities**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "uM4vXFnxolA-"
      },
      "outputs": [],
      "source": [
        "def file_exists_in_bucket(project_name, bucket_name, file_gs_uri):\n",
        "  \n",
        "  \"\"\"\n",
        "  Check whether a file exists in the specified Google Cloud Storage Bucket.\n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    bucket_name  : required - name of the bucket (without gs://)\n",
        "    file_gs_uri  : required - file GS URI\n",
        "  \n",
        "  Returns:\n",
        "    file_exists : boolean variable, True if the file exists in the specified,\n",
        "                  bucket, at the specified location; False if it doesn't.\n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  storage_client = storage.Client(project = project_name)\n",
        "  bucket = storage_client.get_bucket(bucket_name)\n",
        "  \n",
        "  # bucket_gs_url = \"gs://%s/\"%(bucket_name)\n",
        "  bucket_gs_url = \"s3://%s/\"%(bucket_name)\n",
        "  path_to_file_relative = file_gs_uri.split(bucket_gs_url)[-1]\n",
        "\n",
        "  print(\"Searching `%s` for: \\n%s\\n\"%(bucket_gs_url, path_to_file_relative))\n",
        "\n",
        "  file_exists = bucket.blob(path_to_file_relative).exists(storage_client)\n",
        "\n",
        "  return file_exists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS6WfCfgiS-b"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "JwQKxMXchi0h"
      },
      "outputs": [],
      "source": [
        "def listdir_bucket(project_name, bucket_name, dir_gs_uri):\n",
        "  \n",
        "  \"\"\"\n",
        "  Export DICOM SEG object from segmentation masks stored in NRRD files.\n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    bucket_name  : required - name of the bucket (without gs://)\n",
        "    file_gs_uri  : required - directory GS URI\n",
        "  \n",
        "  Returns:\n",
        "    file_list : list of files in the specified GCS bucket.\n",
        "\n",
        "  Outputs:\n",
        "    This function [...]\n",
        "  \"\"\"\n",
        "\n",
        "  storage_client = storage.Client(project = project_name)\n",
        "  bucket = storage_client.get_bucket(bucket_name)\n",
        "  \n",
        "  # bucket_gs_url = \"gs://%s/\"%(bucket_name)\n",
        "  bucket_gs_url = \"s3://%s/\"%(bucket_name)\n",
        "  path_to_dir_relative = dir_gs_uri.split(bucket_gs_url)[-1]\n",
        "\n",
        "\n",
        "  print(\"Getting the list of files at `%s`...\"%(dir_gs_uri))\n",
        "\n",
        "  file_list = list()\n",
        "\n",
        "  for blob in storage_client.list_blobs(bucket_name,  prefix = path_to_dir_relative):\n",
        "    fn = os.path.basename(blob.name)\n",
        "    file_list.append(fn)\n",
        "\n",
        "  return file_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "vm-gnaS8JqRB"
      },
      "outputs": [],
      "source": [
        "def create_dataset(project_name, dataset_id):\n",
        "\n",
        "  \"\"\"\n",
        "  Create a dataset that will store the cohort_df table \n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    dataset_id   : required - name of the dataset to create\n",
        "  \n",
        "  Returns:\n",
        "    dataset : returns the dataset created \n",
        "  \"\"\"\n",
        "\n",
        "  # Construct a BigQuery client object.\n",
        "  client = bigquery.Client(project=project_name)\n",
        "\n",
        "  # Construct a full Dataset object to send to the API.\n",
        "  dataset_id_full = \".\".join([project_name, dataset_id])\n",
        "  dataset = bigquery.Dataset(dataset_id_full)\n",
        "\n",
        "  # TODO(developer): Specify the geographic location where the dataset should reside.\n",
        "  dataset.location = \"US\"\n",
        "\n",
        "  # Send the dataset to the API for creation, with an explicit timeout.\n",
        "  # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
        "  # exists within the project.\n",
        "  # dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
        "  dataset = client.create_dataset(dataset)  # Make an API request.\n",
        "\n",
        "  print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
        "\n",
        "  return dataset \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "zLz4e7-E7xqt"
      },
      "outputs": [],
      "source": [
        "def dataset_exists_in_project(project_id, dataset_id):\n",
        "  \"\"\"Check if a dataset exists in a project\"\"\"\n",
        "\n",
        "  from google.cloud import bigquery\n",
        "  from google.cloud.exceptions import NotFound\n",
        "\n",
        "  client = bigquery.Client()\n",
        "  dataset_id_full = '.'.join([project_id, dataset_id])\n",
        "\n",
        "  try:\n",
        "      client.get_dataset(dataset_id_full)  \n",
        "      return True \n",
        "  except NotFound:\n",
        "      return False "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ewKnaXcF72iz"
      },
      "outputs": [],
      "source": [
        "def create_dataset(project_name, dataset_id):\n",
        "\n",
        "  \"\"\"\n",
        "  Create a dataset that will store the cohort_df table \n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    dataset_id   : required - name of the dataset to create\n",
        "  \n",
        "  Returns:\n",
        "    dataset : returns the dataset created \n",
        "  \"\"\"\n",
        "\n",
        "  # Construct a BigQuery client object.\n",
        "  client = bigquery.Client(project=project_name)\n",
        "\n",
        "  # Construct a full Dataset object to send to the API.\n",
        "  dataset_id_full = \".\".join([project_name, dataset_id])\n",
        "  dataset = bigquery.Dataset(dataset_id_full)\n",
        "\n",
        "  # TODO(developer): Specify the geographic location where the dataset should reside.\n",
        "  dataset.location = \"US\"\n",
        "\n",
        "  # Send the dataset to the API for creation, with an explicit timeout.\n",
        "  # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
        "  # exists within the project.\n",
        "  # dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
        "  dataset = client.create_dataset(dataset)  # Make an API request.\n",
        "\n",
        "  print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
        "\n",
        "  return dataset \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "wfojbZ4GTuEf"
      },
      "outputs": [],
      "source": [
        "def get_dataframe_from_table(project_name, dataset_id, table_id_name):\n",
        "  \n",
        "  \"\"\"\n",
        "  Gets the pandas dataframe from the saved big query table \n",
        "\n",
        "  Arguments:\n",
        "    project_name : required - name of the GCP project.\n",
        "    dataset_id   : required - name of the dataset already created \n",
        "    table_id     : required - name of the table to create \n",
        "  \n",
        "  Returns: \n",
        "    cohort_df    : the cohort as a pandas dataframe\n",
        "  \"\"\"\n",
        "\n",
        "  table_id = \"%s.%s.%s\"%(project_name, dataset_id, table_id_name)\n",
        "\n",
        "  # the query we are going to execute to gather data about the selected cohort\n",
        "  query_str = \"SELECT * FROM `%s`\"%(table_id)\n",
        "\n",
        "  # init the BQ client\n",
        "  client = bigquery.Client(project = project_name)\n",
        "\n",
        "  # run the query\n",
        "  query_job = client.query(query_str)\n",
        "\n",
        "  # convert the results to a Pandas dataframe\n",
        "  # cohort_df = query_job.to_dataframe()\n",
        "  cohort_df = query_job.to_dataframe(create_bqstorage_client=True)\n",
        "\n",
        "\n",
        "  return cohort_df \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cya9qL50iU1S"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "EgYBIF9Y8NX8"
      },
      "outputs": [],
      "source": [
        "def format_dict(input_dict):\n",
        "  \n",
        "  \"\"\"\n",
        "  Format dictionary [...]\n",
        "\n",
        "  Arguments:\n",
        "    input_dict : required - \n",
        "    \n",
        "  Returns:\n",
        "    output_df : \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  output_df = pd.DataFrame.from_dict(data = input_dict, orient = \"index\")\n",
        "\n",
        "  output_df = output_df.reset_index()\n",
        "  output_df = output_df.rename(columns = {\"index\" : \"PatientID\", \n",
        "                                          0 : \"inference_time\"}) \n",
        "  \n",
        "  return output_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "pstlLcWdLAFo"
      },
      "outputs": [],
      "source": [
        "# def format_dict_DK(input_dict, column_name):\n",
        "  \n",
        "#   \"\"\"\n",
        "#   Format dictionary [...]\n",
        "\n",
        "#   Arguments:\n",
        "#     input_dict : required - \n",
        "    \n",
        "#   Returns:\n",
        "#     output_df : \n",
        "\n",
        "#   \"\"\"\n",
        "\n",
        "#   output_df = pd.DataFrame.from_dict(data = input_dict, orient = \"index\")\n",
        "\n",
        "#   output_df = output_df.reset_index()\n",
        "#   output_df = output_df.rename(columns = {\"index\" : \"PatientID\", \n",
        "#                                           0 : column_name}) \n",
        "  \n",
        "#   return output_df\n",
        "\n",
        "def format_dict_columns(input_dict, index_name, column_name):\n",
        "  \n",
        "  \"\"\"\n",
        "  Formats the dictionary \n",
        "\n",
        "  Arguments:\n",
        "    input_dict  : input dictionary  \n",
        "    index_name  : name of index column \n",
        "    column_name : name of column with value to hold \n",
        "    \n",
        "  Returns:\n",
        "    output_df : formatted dataframe \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  output_df = pd.DataFrame.from_dict(data = input_dict, orient = \"index\")\n",
        "\n",
        "  output_df = output_df.reset_index()\n",
        "  output_df = output_df.rename(columns = {\"index\" : index_name, \n",
        "                                          0 : column_name}) \n",
        "  \n",
        "  return output_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "cellView": "code",
        "id": "GCCDBUJOKsIE"
      },
      "outputs": [],
      "source": [
        "def add_value_to_table(project_name, dataset_id, table_id, column_name, pat_id, value_name, value_to_add, dict_to_add):\n",
        "\n",
        "  \"\"\"\n",
        "  This function adds a value to a table - per PatientID, SeriesInstanceUID, any other field\n",
        "  First check if the table exists. If it does, check if the id exists. If the id/value exists, \n",
        "  replace with the current values. If the id does not exist, append to the table. \n",
        "\n",
        "  Arguments:\n",
        "    project_name : name of project\n",
        "    dataset_id   : name of dataset \n",
        "    table_id     : name of table\n",
        "    column_name  : column name that the table will have - e.g. PatientID, SeriesInstanceUID \n",
        "    pat_id       : the patient id, series id, or any other id for which the value will be added\n",
        "    value_name   : the name of the value - inference time, elapsed time \n",
        "    value_to_add : the value to add \n",
        "    dict_to_add  : the original dictionary that holds the value per id \n",
        "    \n",
        "  Returns:\n",
        "    \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  \n",
        "  # ----------------- Save value in a table instead of csv  ----------- #\n",
        "\n",
        "  ##################################### check if table exists ########################\n",
        "\n",
        "  # Check if the table exists \n",
        "  from google.cloud.exceptions import NotFound\n",
        "  table_id_fullname = '.'.join([project_name, dataset_id, table_id])\n",
        "  client = bigquery.Client(project=project_name)\n",
        "  try:\n",
        "    client.get_table(table_id_fullname) \n",
        "    table_found = True\n",
        "  except NotFound:\n",
        "    table_found = False\n",
        "\n",
        "  print ('table_found: ' + str(table_found))\n",
        "\n",
        "  ############################################################\n",
        "  ### If the table is found, check if the pat_id exists ###\n",
        "  ############################################################\n",
        "\n",
        "  if (table_found):\n",
        "    # do this for now \n",
        "    if (column_name==\"PatientID\"):\n",
        "      query = f\"\"\"\n",
        "        SELECT \n",
        "          COUNT(PatientID) AS num_instances\n",
        "        FROM \n",
        "          {table_id_fullname}\n",
        "        WHERE \n",
        "          PatientID IN UNNEST (@pat_id_change);\n",
        "      \"\"\"\n",
        "    elif (column_name==\"SeriesInstanceUID\"):\n",
        "      query = f\"\"\"\n",
        "        SELECT \n",
        "          COUNT(SeriesInstanceUID) AS num_instances\n",
        "        FROM \n",
        "          {table_id_fullname}\n",
        "        WHERE \n",
        "          SeriesInstanceUID IN UNNEST (@pat_id_change);\n",
        "      \"\"\"\n",
        "    job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ArrayQueryParameter(\"pat_id_change\", \"STRING\", [pat_id])])\n",
        "    # Can't pass in the column_name like this\n",
        "    # query = f\"\"\"\n",
        "    #   SELECT\n",
        "    #     COUNT(@column_name) AS num_instances\n",
        "    #   FROM\n",
        "    #     {table_id_fullname}\n",
        "    #   WHERE\n",
        "    #     @column_name IN UNNEST(@pat_id_change);\n",
        "    #     \"\"\" \n",
        "    # job_config = bigquery.QueryJobConfig(query_parameters=[\n",
        "    #                                                       bigquery.ArrayQueryParameter(\"pat_id_change\", \"STRING\", [str(pat_id)]), \n",
        "    #                                                       bigquery.ScalarQueryParameter(\"column_name\", \"STRING\", column_name)\n",
        "    #                                                       ])\n",
        "    result = client.query(query, job_config=job_config)\n",
        "    df_result = result.to_dataframe()\n",
        "    count = df_result['num_instances'][0]\n",
        "  else: \n",
        "    count = 0 \n",
        "  print ('count: ' + str(count))\n",
        "\n",
        "  ############################################################################\n",
        "  ### If the count > 0, series id exists, so replace with current time     ### \n",
        "  ### If count = 0, either series id doesn't exist or table doesn't exist, ###\n",
        "  ### so create it                                                         ###\n",
        "  ############################################################################\n",
        "\n",
        "  if (count > 0): \n",
        "      client = bigquery.Client(project=project_name)\n",
        "      if (column_name==\"PatientID\"):\n",
        "        query = f\"\"\"\n",
        "            UPDATE \n",
        "              {table_id_fullname}\n",
        "            SET \n",
        "              time = @value_to_add\n",
        "            WHERE \n",
        "              PatientID = @pat_id_change;\n",
        "            \"\"\" \n",
        "      elif (column_name==\"SeriesInstanceUID\"):\n",
        "        query = f\"\"\"\n",
        "            UPDATE \n",
        "              {table_id_fullname}\n",
        "            SET \n",
        "              time = @value_to_add\n",
        "            WHERE \n",
        "              SeriesInstanceUID = @pat_id_change;\n",
        "            \"\"\" \n",
        "      job_config = bigquery.QueryJobConfig(query_parameters=[\n",
        "                                                            bigquery.ScalarQueryParameter(\"pat_id_change\", \"STRING\", str(pat_id)), \n",
        "                                                            bigquery.ScalarQueryParameter(\"value_to_add\", \"FLOAT\", value_to_add)\n",
        "                                                           ])   \n",
        "      # Did not work. \n",
        "      # query = f\"\"\"\n",
        "      #   UPDATE \n",
        "      #     {table_id_fullname}\n",
        "      #   SET \n",
        "      #     @value_name = @value_to_add\n",
        "      #   WHERE \n",
        "      #     @column_name IN UNNEST(@pat_id_change);\n",
        "      #   \"\"\"\n",
        "      # job_config = bigquery.QueryJobConfig(query_parameters=[\n",
        "      #                                                       bigquery.ArrayQueryParameter(\"pat_id_change\", \"STRING\", [str(pat_id)]), \n",
        "      #                                                       bigquery.ScalarQueryParameter(value_name, \"FLOAT\", value_to_add),\n",
        "      #                                                       bigquery.ScalarQueryParameter(\"column_name\", \"STRING\", column_name)\n",
        "      #                                                       ])\n",
        "      result = client.query(query, job_config=job_config)\n",
        "  else: \n",
        "    # add_to_df = format_dict_DK(dict_to_add, value_name)\n",
        "    add_to_df = format_dict_columns(dict_to_add, index_name=column_name, column_name=value_name) \n",
        "    table_id_fullname = '.'.join([project_name, dataset_id, table_id])\n",
        "    client = bigquery.Client(project=project_name)\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
        "    job = client.load_table_from_dataframe(add_to_df, table_id_fullname, job_config=job_config)\n",
        "\n",
        "  return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "fFGbEE4W55S5"
      },
      "outputs": [],
      "source": [
        "def save_time_and_num_instances_to_table(project_name, dataset_id, table_id, dict_to_add):\n",
        "\n",
        "  table_id_fullname = '.'.join([project_name, dataset_id,table_id])\n",
        "\n",
        "  add_to_df = pd.DataFrame.from_dict(data = dict_to_add, orient = \"index\")\n",
        "  add_to_df = add_to_df.reset_index()\n",
        "  # add_to_df = add_to_df.rename(columns = {\"index\" : 'SeriesInstanceUID', \n",
        "  #                                         0 : 'time', \n",
        "  #                                         1 : 'num_instances'})\n",
        "  add_to_df = add_to_df.rename(columns = {\"index\" : 'SeriesInstanceUID', \n",
        "                                        0 : 'num_instances', \n",
        "                                        1 : \"nnunet_inference_model_2d\", \n",
        "                                        2 : \"nnunet_total_time_model_2d\"})\n",
        "\n",
        "  table_id_fullname = '.'.join([project_name, dataset_id, table_id])\n",
        "  client = bigquery.Client(project=project_name)\n",
        "  job_config = bigquery.LoadJobConfig()\n",
        "  job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
        "  job = client.load_table_from_dataframe(add_to_df, table_id_fullname, job_config=job_config)\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "nF9cFrUlx-dk"
      },
      "outputs": [],
      "source": [
        "def modify_dicomseg_json_file(dicomseg_json_path, dicomseg_json_path_modified, SegmentAlgorithmName):\n",
        "\n",
        "  \"\"\"\n",
        "  This function writes out a new metadata json file for the DICOM Segmentation object. \n",
        "  It sets the SegmentAlgorithmName to the one provided as input. \n",
        "\n",
        "  Arguments:\n",
        "    dicomseg_json_path          : path of the original dicomseg json file \n",
        "    dicomseg_json_path_modified : the new json file to write to disk \n",
        "    SegmentAlgorithmName        : the field to replace\n",
        "    \n",
        "  Returns:\n",
        "    The json file is written out to dicomseg_json_path_modified \n",
        "\n",
        "  \"\"\"\n",
        "  f = open(dicomseg_json_path)\n",
        "  meta_json = json.load(f)\n",
        "\n",
        "  meta_json_modified = copy.deepcopy(meta_json)\n",
        "  num_regions = len(meta_json_modified['segmentAttributes'])\n",
        "  for n in range(0,num_regions): \n",
        "    meta_json_modified['segmentAttributes'][n][0]['SegmentAlgorithmName'] = SegmentAlgorithmName\n",
        "\n",
        "  with open(dicomseg_json_path_modified, 'w') as f: \n",
        "    json.dump(meta_json_modified, f)\n",
        "\n",
        "  return \n",
        "\n",
        "  # dicomseg_json_uri = \"s3://idc-medima-paper/nnunet/data/dicomseg_metadata.json\"\n",
        "  # dicomseg_json_path = \"/content/data/dicomseg_metadata.json\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "jR_qle-8rgDg"
      },
      "outputs": [],
      "source": [
        "def check_value_exists_in_table(table_id_fullname, project_name, series_id, field_name):\n",
        "  \n",
        "  \"\"\"\n",
        "  This function checks if a value exists in a table. \n",
        "\n",
        "  Arguments:\n",
        "    table_id_fullname    : the full name of the table (project_name.dataset_id.table_id)\n",
        "    project_name         : project name \n",
        "    series_id            : series id to check if the field exists \n",
        "    field_name           : the field name to check for the series \n",
        "    \n",
        "  Returns:\n",
        "    value_found          : 1 if the value is found for the field and the series_id, 0 otherwise \n",
        "\n",
        "  \"\"\"\n",
        "    \n",
        "  client = bigquery.Client(project=project_name)\n",
        "  # query = f\"\"\"\n",
        "  #       SELECT \n",
        "  #         *\n",
        "  #       FROM \n",
        "  #         {table_id_fullname}\n",
        "  #       WHERE \n",
        "  #         SeriesInstanceUID = {series_id} \n",
        "  #         AND {field_name} IS NOT NULL \n",
        "  #     \"\"\"\n",
        "  # job_config = bigquery.QueryJobConfig()\n",
        "  query = f\"\"\"\n",
        "      SELECT \n",
        "        *\n",
        "      FROM \n",
        "        {table_id_fullname}\n",
        "      WHERE\n",
        "        SeriesInstanceUID = @series_id AND \n",
        "        @field_name IS NOT NULL \n",
        "    \"\"\"\n",
        "  job_config = bigquery.QueryJobConfig(query_parameters = [bigquery.ScalarQueryParameter(\"series_id\", \"STRING\", series_id), \n",
        "                                                          bigquery.ScalarQueryParameter(\"field_name\", \"STRING\", field_name)])\n",
        "  result = client.query(query, job_config=job_config)\n",
        "  series = result.to_dataframe()\n",
        "  \n",
        "  if (len(series)==0):\n",
        "    value_found = 0 \n",
        "  else: \n",
        "    value_found = 1  \n",
        "\n",
        "  return value_found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq3qnUI2v1Yr"
      },
      "source": [
        "# Running over multiple models and subset - Putting everything together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4fpAvMzv1Yr"
      },
      "source": [
        "Paths for nnUNet and BPR "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "CQkS3621v1Yr"
      },
      "outputs": [],
      "source": [
        "############ nnUNet paths #############\n",
        "\n",
        "data_base_path = \"/content/data\"\n",
        "raw_base_path = \"/content/data/raw/tmp\"\n",
        "sorted_base_path = \"/content/data/raw/nlst/dicom\"\n",
        "\n",
        "processed_base_path = \"/content/data/processed/nlst/\"\n",
        "processed_nifti_path = os.path.join(processed_base_path, \"nii\")\n",
        "\n",
        "processed_nrrd_path_nnunet = os.path.join(processed_base_path, \"nnunet\", \"nrrd\")\n",
        "processed_dicomseg_path_nnunet = os.path.join(processed_base_path, \"nnunet\", \"dicomseg\")\n",
        "processed_dicompm_path_nnunet = os.path.join(processed_base_path, \"nnunet\", \"dicompm\")\n",
        "\n",
        "model_input_folder_nnunet = \"/content/data/nnunet/model_input/\"\n",
        "model_output_folder_nnunet = \"/content/data/nnunet/nnunet_output/\"\n",
        "\n",
        "# bucket_output_base_uri_nnunet = os.path.join(bucket_base_uri, \"nnunet/nnunet_output/nlst\")\n",
        "# bucket_output_base_uri_nnunet = os.path.join(bucket_base_uri, \"nnunet/nnunet_output/nlst_50_series\")\n",
        "bucket_output_base_uri_nnunet = os.path.join(bucket_base_uri, \"nnunet\", \"nnunet_output\", nlst_sub)\n",
        "\n",
        "\n",
        "# # -----------------\n",
        "# # nnU-Net pipeline parameters\n",
        "\n",
        "# # choose from: \"2d\", \"3d_lowres\", \"3d_fullres\", \"3d_cascade_fullres\"\n",
        "# # nnunet_model = \"3d_fullres\"\n",
        "# nnunet_model = \"2d\"\n",
        "# use_tta = True\n",
        "# export_prob_maps = True\n",
        "\n",
        "# experiment_folder_name = nnunet_model + \"-tta\" if use_tta == True else nnunet_model + \"-no_tta\"\n",
        "# bucket_experiment_folder_uri_nnunet = os.path.join(bucket_output_base_uri_nnunet, experiment_folder_name)\n",
        "\n",
        "# bucket_log_folder_uri_nnunet = os.path.join(bucket_experiment_folder_uri_nnunet, 'log')\n",
        "\n",
        "# bucket_nifti_folder_uri_nnunet = os.path.join(bucket_experiment_folder_uri_nnunet, 'nii')\n",
        "# bucket_softmax_pred_folder_uri_nnunet = os.path.join(bucket_experiment_folder_uri_nnunet, 'softmax_pred')\n",
        "\n",
        "# bucket_dicomseg_folder_uri_nnunet = os.path.join(bucket_experiment_folder_uri_nnunet, 'dicomseg')\n",
        "\n",
        "# # -----------------\n",
        "# # save run information\n",
        "\n",
        "# yaml_fn = \"run_params.yaml\"\n",
        "# yaml_out_path = os.path.join(data_base_path, yaml_fn)\n",
        "\n",
        "# settings_dict = dict()\n",
        "# settings_dict[\"bucket\"] = dict()\n",
        "# settings_dict[\"bucket\"][\"name\"] = bucket_name\n",
        "# settings_dict[\"bucket\"][\"base_uri\"] = bucket_base_uri\n",
        "# settings_dict[\"bucket\"][\"output_base_uri\"] = bucket_output_base_uri_nnunet\n",
        "# settings_dict[\"bucket\"][\"experiment_folder_uri\"] = bucket_experiment_folder_uri_nnunet\n",
        "# settings_dict[\"bucket\"][\"nifti_folder_uri\"] = bucket_nifti_folder_uri_nnunet\n",
        "# settings_dict[\"bucket\"][\"softmax_pred_folder_uri\"] = bucket_softmax_pred_folder_uri_nnunet\n",
        "# settings_dict[\"bucket\"][\"dicomseg_folder_uri\"] = bucket_dicomseg_folder_uri_nnunet\n",
        "# settings_dict[\"bucket\"][\"log_folder_uri\"] = bucket_log_folder_uri_nnunet\n",
        "\n",
        "# settings_dict[\"inference\"] = dict()\n",
        "# settings_dict[\"inference\"][\"model\"] = nnunet_model\n",
        "# settings_dict[\"inference\"][\"use_tta\"] = use_tta\n",
        "# settings_dict[\"inference\"][\"export_prob_maps\"] = export_prob_maps\n",
        "\n",
        "# with open(yaml_out_path, 'w') as fp:\n",
        "#   yaml.dump(settings_dict, fp, default_flow_style = False)\n",
        "\n",
        "# gs_uri_yaml_file = os.path.join(bucket_log_folder_uri_nnunet, yaml_fn)\n",
        "\n",
        "# #!gsutil -m cp $yaml_out_path $gs_uri_yaml_file\n",
        "# !/content/s5cmd --endpoint-url https://storage.googleapis.com cp $yaml_out_path $gs_uri_yaml_file\n",
        "\n",
        "######### BPR paths ############\n",
        "\n",
        "processed_json_path = os.path.join(processed_base_path, \"bpr\", \"json\")\n",
        "\n",
        "model_input_folder_bpr = \"/content/data/bpr/model_input/\"\n",
        "model_input_folder_tmp_bpr = \"/content/data/bpr/model_input_tmp\"\n",
        "model_output_folder_bpr = \"/content/data/bpr/bpr_output/\"\n",
        "model_output_folder_tmp_bpr = \"/content/data/bpr/bpr_output_tmp\"\n",
        "\n",
        "# bucket_output_base_uri_bpr = os.path.join(bucket_base_uri, \"bpr/bpr_output/nlst\")\n",
        "# bucket_output_base_uri_bpr = os.path.join(bucket_base_uri, \"bpr/bpr_output/nlst_50_series\")\n",
        "bucket_output_base_uri_bpr = os.path.join(bucket_base_uri, \"bpr\", \"bpr_output\", nlst_sub)\n",
        "\n",
        "\n",
        "# -----------------\n",
        "# bpr parameters \n",
        "# experiment_folder_name = 'bpr'\n",
        "# bucket_experiment_folder_uri_bpr = os.path.join(bucket_output_base_uri_bpr, experiment_folder_name)\n",
        "\n",
        "# bucket_log_folder_uri_bpr = os.path.join(bucket_experiment_folder_uri_bpr, 'log')\n",
        "# bucket_nifti_folder_uri_bpr = os.path.join(bucket_experiment_folder_uri_bpr, 'nii')\n",
        "# bucket_json_folder_uri_bpr = os.path.join(bucket_experiment_folder_uri_bpr, 'json')\n",
        "\n",
        "bucket_log_folder_uri_bpr = os.path.join(bucket_output_base_uri_bpr, 'log')\n",
        "bucket_nifti_folder_uri_bpr = os.path.join(bucket_output_base_uri_bpr, 'nii')\n",
        "bucket_json_folder_uri_bpr = os.path.join(bucket_output_base_uri_bpr, 'json')\n",
        "\n",
        "######## Table names #####\n",
        "\n",
        "# inference_time_table_id_name_nnunet = \"nlst_nnunet_inference_time\"\n",
        "# total_time_table_id_name_nnunet = \"nlst_nnunet_total_time\"\n",
        "\n",
        "# inference_time_table_id_name_bpr = \"nlst_bpr_inference_time\"\n",
        "# total_time_table_id_name_bpr = \"nlst_bpr_total_time\"\n",
        "\n",
        "# multiple_models_time_table_id_name_nnunet = 'nlst_nnunet_multiple_models_time'\n",
        "# time_table_id_name_bpr = 'nlst_bpr_time'\n",
        "\n",
        "# dataset_id = 'dataset_nlst' \n",
        "# table_id_name = 'table_nlst' # holds the cohort information \n",
        "# # table_view_id_name = 'table_nlst_view'\n",
        "# table_view_id_name = 'nlst_revised_series_selection' \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Moh26auDv1Ys"
      },
      "source": [
        "## Create the cohort using Big Query using table view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8mUC5NCv1Ys"
      },
      "outputs": [],
      "source": [
        "# Andrey's original query \n",
        "\n",
        "# WITH\n",
        "#  nlst_instances_per_series AS (\n",
        "#  SELECT\n",
        "#    SeriesInstanceUID,\n",
        "#    COUNT(DISTINCT(SOPInstanceUID)) AS num_instances,\n",
        "#    MAX(SAFE_CAST(SliceThickness AS float64)) AS max_SliceThickness,\n",
        "#    STRING_AGG(DISTINCT(SAFE_CAST(\"LOCALIZER\" IN UNNEST(ImageType) AS string)),\"\") AS has_localizer\n",
        "#  FROM\n",
        "#    `bigquery-public-data.idc_current.dicom_all`\n",
        "#  WHERE\n",
        "#    collection_id = \"nlst\"\n",
        "#    AND Modality = \"CT\"\n",
        "#  GROUP BY\n",
        "#    SeriesInstanceUID)\n",
        "# SELECT\n",
        "#  ANY_VALUE(dicom_all.PatientID) AS PatientID,\n",
        "#  ANY_VALUE(StudyInstanceUID) AS StudyInstanceUID,\n",
        "#  dicom_all.SeriesInstanceUID,\n",
        "#  any_value(array_to_string(ImageType,\"/\")) as ImageType,\n",
        "#  ANY_VALUE(nlst_instances_per_series.num_instances) AS num_instances,\n",
        "#  ANY_VALUE(CONCAT(\"https://viewer.imaging.datacommons.cancer.gov/viewer/\",dicom_all.StudyInstanceUID,\"?seriesInstanceUID=\",dicom_all.SeriesInstanceUID)) AS idc_url\n",
        "# FROM\n",
        "#  `bigquery-public-data.idc_current.dicom_all` AS dicom_all\n",
        "# JOIN\n",
        "#  nlst_instances_per_series\n",
        "# ON\n",
        "#  dicom_all.SeriesInstanceUID = nlst_instances_per_series.SeriesInstanceUID\n",
        "# WHERE\n",
        "#  nlst_instances_per_series.num_instances > 50\n",
        "#  AND max_SliceThickness <= 3\n",
        "#  AND has_localizer = \"false\"\n",
        "# GROUP BY\n",
        "#  SeriesInstanceUID\n",
        "# ORDER BY\n",
        "#  num_instances DESC\n",
        "\n",
        "# The query I slightly modified from Andrey's original query \n",
        "\n",
        "# query = \"\"\"\n",
        "#     WITH\n",
        "#     nlst_instances_per_series AS (\n",
        "#       SELECT\n",
        "#         SeriesInstanceUID,  \n",
        "#         COUNT(DISTINCT(SOPInstanceUID)) AS num_instances,\n",
        "#         COUNT(DISTINCT(ARRAY_TO_STRING(ImagePositionPatient,\"/\"))) AS position_count,\n",
        "#         COUNT(DISTINCT(ARRAY_TO_STRING(ImageOrientationPatient,\"/\"))) AS orientation_count,\n",
        "#         MAX(SAFE_CAST(SliceThickness AS float64)) AS max_SliceThickness,\n",
        "#         STRING_AGG(DISTINCT(SAFE_CAST(\"LOCALIZER\" IN UNNEST(ImageType) AS string)),\"\") AS has_localizer\n",
        "#       FROM\n",
        "#         `bigquery-public-data.idc_current.dicom_all`\n",
        "#       WHERE\n",
        "#         collection_id = \"nlst\"\n",
        "#         AND Modality = \"CT\"\n",
        "#       GROUP BY\n",
        "#         SeriesInstanceUID)\n",
        "\n",
        "#     SELECT \n",
        "#       ANY_VALUE(dicom_all.PatientID) AS PatientID,\n",
        "#       ANY_VALUE(StudyInstanceUID) AS StudyInstanceUID,\n",
        "#       dicom_all.SeriesInstanceUID,\n",
        "#       ANY_VALUE(nlst_instances_per_series.num_instances) AS num_instances,\n",
        "#       gcs_url \n",
        "#     FROM\n",
        "#       `bigquery-public-data.idc_current.dicom_all` AS dicom_all\n",
        "#     JOIN\n",
        "#       nlst_instances_per_series\n",
        "#     ON\n",
        "#       dicom_all.SeriesInstanceUID = nlst_instances_per_series.SeriesInstanceUID\n",
        "#     WHERE\n",
        "#       nlst_instances_per_series.num_instances > 50\n",
        "#       AND nlst_instances_per_series.num_instances/nlst_instances_per_series.position_count = 1\n",
        "#       AND nlst_instances_per_series.orientation_count = 1\n",
        "#       AND max_SliceThickness <= 3\n",
        "#       AND has_localizer = \"false\"\n",
        "#     GROUP BY\n",
        "#       SeriesInstanceUID,\n",
        "#       gcs_url\n",
        "#     ORDER BY\n",
        "#       num_instances DESC\n",
        "\n",
        "#  \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6DAo4u3v1Ys"
      },
      "outputs": [],
      "source": [
        "## Old query 6-13-22 ## \n",
        "# query = \"\"\"\n",
        "#   WITH\n",
        "\n",
        "#   nlst_instances_per_series AS (\n",
        "#     SELECT\n",
        "#       DISTINCT(StudyInstanceUID),\n",
        "#       SeriesInstanceUID,\n",
        "#       COUNT(DISTINCT(SOPInstanceUID)) AS num_instances,\n",
        "#       COUNT(DISTINCT(ARRAY_TO_STRING(ImagePositionPatient,\"/\"))) AS position_count,\n",
        "#       COUNT(DISTINCT(ARRAY_TO_STRING(ImageOrientationPatient,\"/\"))) AS orientation_count,\n",
        "#       MIN(SAFE_CAST(SliceThickness AS float64)) AS min_SliceThickness,\n",
        "#       MAX(SAFE_CAST(SliceThickness AS float64)) AS max_SliceThickness,\n",
        "#       MIN(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS float64)) as min_SliceLocation, \n",
        "#       MAX(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS float64)) as max_SliceLocation,\n",
        "#       STRING_AGG(DISTINCT(SAFE_CAST(\"LOCALIZER\" IN UNNEST(ImageType) AS string)),\"\") AS has_localizer\n",
        "#     FROM\n",
        "#       `bigquery-public-data.idc_current.dicom_all`\n",
        "#     WHERE\n",
        "#       collection_id = \"nlst\"\n",
        "#       AND Modality = \"CT\"\n",
        "#     GROUP BY\n",
        "#       StudyInstanceUID,\n",
        "#       SeriesInstanceUID\n",
        "#       ),\n",
        "\n",
        "#   nlst_values_per_series AS (\n",
        "#     SELECT \n",
        "#     dicom_all.StudyInstanceUID AS StudyInstanceUID,\n",
        "#     ANY_VALUE(dicom_all.PatientID) AS PatientID,\n",
        "#     ANY_VALUE(dicom_all.SeriesInstanceUID) AS SeriesInstanceUID,\n",
        "#     ANY_VALUE(nlst_instances_per_series.num_instances) AS num_instances,\n",
        "#     ANY_VALUE(nlst_instances_per_series.max_SliceThickness) AS SliceThickness,\n",
        "#     ANY_VALUE((nlst_instances_per_series.max_SliceLocation - nlst_instances_per_series.min_SliceLocation)) AS PatientHeightScanned,\n",
        "#     ANY_VALUE(CONCAT(\"https://viewer.imaging.datacommons.cancer.gov/viewer/\",dicom_all.StudyInstanceUID,\"?seriesInstanceUID=\",dicom_all.SeriesInstanceUID)) AS idc_url\n",
        "#   FROM\n",
        "#     `bigquery-public-data.idc_current.dicom_all` AS dicom_all\n",
        "#   JOIN\n",
        "#     nlst_instances_per_series\n",
        "#   ON\n",
        "#     dicom_all.SeriesInstanceUID = nlst_instances_per_series.SeriesInstanceUID\n",
        "#   WHERE\n",
        "#     min_SliceThickness >= 1.5 \n",
        "#     AND max_SliceThickness <= 3.5 \n",
        "#     AND nlst_instances_per_series.num_instances > 100\n",
        "#     AND nlst_instances_per_series.num_instances/nlst_instances_per_series.position_count = 1\n",
        "#     AND nlst_instances_per_series.orientation_count = 1\n",
        "#     AND has_localizer = \"false\"\n",
        "#   GROUP BY\n",
        "#     StudyInstanceUID\n",
        "#   )\n",
        "\n",
        "#   SELECT \n",
        "#     dicom_all.PatientID,\n",
        "#     dicom_all.StudyInstanceUID,\n",
        "#     dicom_all.SeriesInstanceUID,\n",
        "#     dicom_all.SOPInstanceUID,\n",
        "#     dicom_all.gcs_url,\n",
        "#     nlst_values_per_series.num_instances,\n",
        "#     nlst_values_per_series.SliceThickness,\n",
        "#     nlst_values_per_series.PatientHeightScanned,\n",
        "#     nlst_values_per_series.idc_url \n",
        "#   FROM\n",
        "#     `bigquery-public-data.idc_current.dicom_all` AS dicom_all\n",
        "#   JOIN\n",
        "#     nlst_values_per_series \n",
        "#   ON\n",
        "#     dicom_all.SeriesInstanceUID = nlst_values_per_series.SeriesInstanceUID \n",
        "#    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI2-Vs-Hv1Ys"
      },
      "source": [
        "The query below filters the series according to the criteria of slice thickness, image orientation and image position, etc. \n",
        "\n",
        "One series is then picked per study. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "WslhcFNP5FfQ"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "  WITH\n",
        "    nlst_instances_per_series AS (\n",
        "    SELECT\n",
        "      StudyInstanceUID,\n",
        "      SeriesInstanceUID,\n",
        "      COUNT(DISTINCT(SOPInstanceUID)) AS num_instances,\n",
        "      COUNT(DISTINCT(ARRAY_TO_STRING(ImagePositionPatient,\"/\"))) AS position_count,\n",
        "      COUNT(DISTINCT(ARRAY_TO_STRING(ImageOrientationPatient,\"/\"))) AS orientation_count,\n",
        "      MIN(SAFE_CAST(SliceThickness AS float64)) AS min_SliceThickness,\n",
        "      MAX(SAFE_CAST(SliceThickness AS float64)) AS max_SliceThickness,\n",
        "      MIN(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS float64)) AS min_SliceLocation,\n",
        "      MAX(SAFE_CAST(ImagePositionPatient[SAFE_OFFSET(2)] AS float64)) AS max_SliceLocation,\n",
        "      STRING_AGG(DISTINCT(SAFE_CAST(\"LOCALIZER\" IN UNNEST(ImageType) AS string)),\"\") AS has_localizer\n",
        "    FROM\n",
        "      `bigquery-public-data.idc_current.dicom_all`\n",
        "    WHERE\n",
        "      collection_id = \"nlst\"\n",
        "      AND Modality = \"CT\"\n",
        "    GROUP BY\n",
        "      StudyInstanceUID,\n",
        "      SeriesInstanceUID ),\n",
        "\n",
        "    nlst_values_per_series AS (\n",
        "    SELECT\n",
        "      ANY_VALUE(dicom_all.PatientID) AS PatientID,\n",
        "      ANY_VALUE(dicom_all.StudyInstanceUID) AS StudyInstanceUID,\n",
        "      dicom_all.SeriesInstanceUID AS SeriesInstanceUID,\n",
        "      ANY_VALUE(nlst_instances_per_series.num_instances) AS num_instances\n",
        "    FROM\n",
        "      `bigquery-public-data.idc_current.dicom_all` AS dicom_all\n",
        "    JOIN\n",
        "      nlst_instances_per_series\n",
        "    ON\n",
        "      dicom_all.SeriesInstanceUID = nlst_instances_per_series.SeriesInstanceUID\n",
        "    WHERE\n",
        "      nlst_instances_per_series.min_SliceThickness >= 1.5\n",
        "      AND nlst_instances_per_series.max_SliceThickness <= 3.5\n",
        "      AND nlst_instances_per_series.num_instances > 100\n",
        "      AND nlst_instances_per_series.num_instances/nlst_instances_per_series.position_count = 1\n",
        "      AND nlst_instances_per_series.orientation_count = 1\n",
        "      AND has_localizer = \"false\"\n",
        "    GROUP BY\n",
        "      dicom_all.SeriesInstanceUID ),\n",
        "\n",
        "    select_single_series_from_study AS (\n",
        "    SELECT\n",
        "      dicom_all.StudyInstanceUID,\n",
        "      dicom_all.SeriesInstanceUID,\n",
        "      ANY_VALUE(nlst_values_per_series.num_instances) AS num_instances,\n",
        "      ANY_VALUE(CONCAT(\"https://viewer.imaging.datacommons.cancer.gov/viewer/\",dicom_all.StudyInstanceUID,\"?seriesInstanceUID=\",dicom_all.SeriesInstanceUID)) AS idc_url\n",
        "    FROM\n",
        "      `bigquery-public-data.idc_current.dicom_all` AS dicom_all\n",
        "    JOIN\n",
        "      nlst_values_per_series\n",
        "    ON\n",
        "      dicom_all.SeriesInstanceUID = nlst_values_per_series.SeriesInstanceUID\n",
        "    GROUP BY\n",
        "      dicom_all.StudyInstanceUID,\n",
        "      dicom_all.SeriesInstanceUID )\n",
        "\n",
        "    SELECT \n",
        "      dicom_all.SeriesInstanceUID,\n",
        "      select_single_series_from_study.num_instances,\n",
        "      select_single_series_from_study.idc_url,\n",
        "      dicom_all.gcs_url\n",
        "    FROM \n",
        "    `bigquery-public-data.idc_current.dicom_all` AS dicom_all\n",
        "    JOIN \n",
        "      select_single_series_from_study \n",
        "    ON \n",
        "      dicom_all.SeriesInstanceUID = select_single_series_from_study.SeriesInstanceUID \n",
        "    order by dicom_all.SeriesInstanceUID\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMj8bOeav1Ys"
      },
      "source": [
        "Create the dataset in the project if it doesn't already exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXsPjLP7tzPA",
        "outputId": "3976e6a2-c67f-43d6-d493-9812a33f1192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset dataset_nlst already exists.\n"
          ]
        }
      ],
      "source": [
        "# Check if the dataset exists within the project \n",
        "dataset_exists = dataset_exists_in_project(project_name, dataset_id)\n",
        "\n",
        "# If it does not exist, create the dataset \n",
        "if not dataset_exists: \n",
        "  print ('creating dataset: ' + str(dataset_id))\n",
        "  create_dataset(project_name, dataset_id)\n",
        "else:\n",
        "  print ('dataset ' + str(dataset_id) + ' already exists.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvJS8odNt1jB"
      },
      "source": [
        "Create the view if it doesn't already exist. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dTAWZzGwqrH",
        "outputId": "2eff2741-a21a-400e-c871-618df2b5674d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idc-external-018.dataset_nlst.nlst_revised_series_selection\n",
            "view idc-external-018.dataset_nlst.nlst_revised_series_selection already exists\n",
            "view_exists: 1\n"
          ]
        }
      ],
      "source": [
        "client = bigquery.Client(project=project_name)\n",
        "\n",
        "view_id = '.'.join([project_name, dataset_id, table_view_id_name])\n",
        "print (view_id)\n",
        "\n",
        "view = bigquery.Table(view_id)\n",
        "view.view_query = query\n",
        "\n",
        "try:\n",
        "  view = client.create_table(view)\n",
        "  view_exists = 0 \n",
        "except:\n",
        "  print ('view ' + str(view) + ' already exists')\n",
        "  view_exists = 1 \n",
        "print ('view_exists: ' + str(view_exists))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2ANZpF-v1Yt"
      },
      "source": [
        "## Set the run parameters "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR18J9pAv1Yt"
      },
      "source": [
        "Get the cohort from the view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAANBf3V5xoI",
        "outputId": "a5d9baa3-c37b-4f8c-e100-f7481b4ab4e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elapsed time: 4.7946367263793945\n",
            "num of unique series should be 25 -- the number of unique series is: 25\n"
          ]
        }
      ],
      "source": [
        "### Get the cohort_df of the subset of series ids that we want ### \n",
        "\n",
        "# First query to get the list of series that we want \n",
        "table_view_id_name_full = '.'.join([project_name, dataset_id, table_view_id_name])\n",
        "client = bigquery.Client(project=project_name)\n",
        "query_view = f\"\"\"\n",
        "  SELECT \n",
        "    DISTINCT(SeriesInstanceUID)\n",
        "  FROM\n",
        "    {table_view_id_name_full}\n",
        "  LIMIT 25 OFFSET @patient_offset;\n",
        "  \"\"\"\n",
        "\n",
        "start_time = time.time()\n",
        "job_config = bigquery.QueryJobConfig(query_parameters=[\n",
        "                                                       bigquery.ScalarQueryParameter(\"patient_offset\", \"INTEGER\", int(patient_offset))\n",
        "                                                       ])\n",
        "result = client.query(query_view, job_config=job_config) \n",
        "series_df = result.to_dataframe(create_bqstorage_client=True)\n",
        "end_time = time.time()\n",
        "print ('elapsed time: ' + str(end_time-start_time)) # 2.7 seconds. \n",
        "series_id_list = list(series_df['SeriesInstanceUID'].values)\n",
        "print ('num of unique series should be 25 -- the number of unique series is: ' + str(len(set(series_id_list))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "dOtqlAQM9bXv",
        "outputId": "5584e8e2-5b86-4f4a-e0e5-68fea4b11ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elapsed time: 4.407650470733643\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      SeriesInstanceUID  num_instances  \\\n",
              "0     1.2.840.113654.2.55.13051140015783791366008702...            184   \n",
              "1     1.2.840.113654.2.55.13051140015783791366008702...            184   \n",
              "2     1.2.840.113654.2.55.13051140015783791366008702...            184   \n",
              "3     1.2.840.113654.2.55.13051140015783791366008702...            184   \n",
              "4     1.2.840.113654.2.55.13051140015783791366008702...            184   \n",
              "...                                                 ...            ...   \n",
              "4256  1.3.6.1.4.1.14519.5.2.1.7009.9004.962027259939...            183   \n",
              "4257  1.3.6.1.4.1.14519.5.2.1.7009.9004.962027259939...            183   \n",
              "4258  1.3.6.1.4.1.14519.5.2.1.7009.9004.962027259939...            183   \n",
              "4259  1.3.6.1.4.1.14519.5.2.1.7009.9004.962027259939...            183   \n",
              "4260  1.3.6.1.4.1.14519.5.2.1.7009.9004.962027259939...            183   \n",
              "\n",
              "                                                idc_url  \\\n",
              "0     https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "1     https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "2     https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "3     https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "4     https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "...                                                 ...   \n",
              "4256  https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "4257  https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "4258  https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "4259  https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "4260  https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "\n",
              "                                                gcs_url  \n",
              "0     gs://public-datasets-idc/43628019-935a-4a42-a8...  \n",
              "1     gs://public-datasets-idc/7649449a-e7d3-4eda-97...  \n",
              "2     gs://public-datasets-idc/61bdd9e1-459e-4b93-85...  \n",
              "3     gs://public-datasets-idc/8b411348-a307-4eef-b1...  \n",
              "4     gs://public-datasets-idc/f5563510-5ad2-47d8-97...  \n",
              "...                                                 ...  \n",
              "4256  gs://public-datasets-idc/a7465a57-76c1-4d8a-8b...  \n",
              "4257  gs://public-datasets-idc/db73b23a-cc0a-4dda-b1...  \n",
              "4258  gs://public-datasets-idc/064b379e-3f88-4647-a4...  \n",
              "4259  gs://public-datasets-idc/cd08acad-9969-42e0-99...  \n",
              "4260  gs://public-datasets-idc/afecef2c-dfb4-48fb-83...  \n",
              "\n",
              "[4261 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-023791ec-e9e8-43dd-8e2a-6189c6aa4ea4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SeriesInstanceUID</th>\n",
              "      <th>num_instances</th>\n",
              "      <th>idc_url</th>\n",
              "      <th>gcs_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.2.840.113654.2.55.13051140015783791366008702...</td>\n",
              "      <td>184</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/43628019-935a-4a42-a8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.2.840.113654.2.55.13051140015783791366008702...</td>\n",
              "      <td>184</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/7649449a-e7d3-4eda-97...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.2.840.113654.2.55.13051140015783791366008702...</td>\n",
              "      <td>184</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/61bdd9e1-459e-4b93-85...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.2.840.113654.2.55.13051140015783791366008702...</td>\n",
              "      <td>184</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/8b411348-a307-4eef-b1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.2.840.113654.2.55.13051140015783791366008702...</td>\n",
              "      <td>184</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/f5563510-5ad2-47d8-97...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4256</th>\n",
              "      <td>1.3.6.1.4.1.14519.5.2.1.7009.9004.962027259939...</td>\n",
              "      <td>183</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/a7465a57-76c1-4d8a-8b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4257</th>\n",
              "      <td>1.3.6.1.4.1.14519.5.2.1.7009.9004.962027259939...</td>\n",
              "      <td>183</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/db73b23a-cc0a-4dda-b1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4258</th>\n",
              "      <td>1.3.6.1.4.1.14519.5.2.1.7009.9004.962027259939...</td>\n",
              "      <td>183</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/064b379e-3f88-4647-a4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4259</th>\n",
              "      <td>1.3.6.1.4.1.14519.5.2.1.7009.9004.962027259939...</td>\n",
              "      <td>183</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/cd08acad-9969-42e0-99...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4260</th>\n",
              "      <td>1.3.6.1.4.1.14519.5.2.1.7009.9004.962027259939...</td>\n",
              "      <td>183</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/afecef2c-dfb4-48fb-83...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4261 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-023791ec-e9e8-43dd-8e2a-6189c6aa4ea4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-023791ec-e9e8-43dd-8e2a-6189c6aa4ea4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-023791ec-e9e8-43dd-8e2a-6189c6aa4ea4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "# Then get the subset cohort df of those series \n",
        "client = bigquery.Client(project=project_name)\n",
        "query_view = f\"\"\"\n",
        "  SELECT \n",
        "    *\n",
        "  FROM\n",
        "    {table_view_id_name_full}\n",
        "  WHERE\n",
        "    SeriesInstanceUID IN UNNEST(@series_id_list);\n",
        "  \"\"\"\n",
        "\n",
        "start_time = time.time()\n",
        "job_config = bigquery.QueryJobConfig(query_parameters=[\n",
        "                                                       bigquery.ArrayQueryParameter(\"series_id_list\", \"STRING\", series_id_list)\n",
        "                                                       ])\n",
        "result = client.query(query_view, job_config=job_config) \n",
        "cohort_df = result.to_dataframe(create_bqstorage_client=True)\n",
        "end_time = time.time()\n",
        "print ('elapsed time: ' + str(end_time-start_time)) #  8.6 seconds.\n",
        "cohort_df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bRgSTm1jDHR",
        "outputId": "d4a526aa-49e0-4888-b86a-85b4d5096879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created table idc-external-018.dataset_nlst.nlst_nnunet_time_temp\n"
          ]
        }
      ],
      "source": [
        "# nnuNet table creation \n",
        "# If table of inference/total time results doesn't exist, create the schema based on the above \n",
        "\n",
        "from google.cloud.exceptions import NotFound\n",
        "client = bigquery.Client(project=project_name)\n",
        "try:\n",
        "  client.get_table(nnunet_table_id_fullname)\n",
        "  nnunet_table_exists = 1 \n",
        "except NotFound: \n",
        "  nnunet_table_exists = 0 \n",
        "\n",
        "if (nnunet_table_exists==0):\n",
        "\n",
        "  schema = [\n",
        "      bigquery.SchemaField(\"SeriesInstanceUID\", \"STRING\"), # mode=\"REQUIRED\"\n",
        "      bigquery.SchemaField(\"num_instances\", \"INTEGER\"), # optional\n",
        "  ] \n",
        "  for nnunet_model in model_list: \n",
        "    experiment_folder_name = nnunet_model + \"-tta\" if use_tta == True else nnunet_model + \"-no_tta\"\n",
        "    experiment_folder_name = experiment_folder_name.replace('-', '_')\n",
        "    schema.append(bigquery.SchemaField('inference_time_' + experiment_folder_name, \"FLOAT\"))\n",
        "    schema.append(bigquery.SchemaField('total_time_' + experiment_folder_name, \"FLOAT\"))\n",
        "\n",
        "  table = bigquery.Table(nnunet_table_id_fullname, schema=schema)\n",
        "  table = client.create_table(table)  # Make an API request.\n",
        "  print(\n",
        "      \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
        "  )\n",
        "\n",
        "else: \n",
        "  print (\"Table \" + str(nnunet_table_id_fullname) + ' already exists.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x50QwuroFGo",
        "outputId": "92b9d992-c131-4970-8119-4686efb1efdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created table idc-external-018.dataset_nlst.nlst_bpr_time_temp\n"
          ]
        }
      ],
      "source": [
        "# bpr table creation\n",
        "# If table of inference/total time results doesn't exist, create the schema\n",
        "from google.cloud.exceptions import NotFound\n",
        "client = bigquery.Client(project=project_name)\n",
        "try:\n",
        "  client.get_table(bpr_table_id_fullname)\n",
        "  bpr_table_exists = 1 \n",
        "except NotFound: \n",
        "  bpr_table_exists = 0 \n",
        "\n",
        "if (bpr_table_exists==0):\n",
        "\n",
        "  schema = [\n",
        "      bigquery.SchemaField(\"SeriesInstanceUID\", \"STRING\"), # mode=\"REQUIRED\"\n",
        "      bigquery.SchemaField(\"num_instances\", \"INTEGER\"), # optional\n",
        "      bigquery.SchemaField('inference_time_bpr', \"FLOAT\"),\n",
        "      bigquery.SchemaField('total_time_bpr', \"FLOAT\")\n",
        "  ] \n",
        "  table = bigquery.Table(bpr_table_id_fullname, schema=schema)\n",
        "  table = client.create_table(table)  # Make an API request.\n",
        "  print(\n",
        "      \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
        "  )\n",
        "\n",
        "else: \n",
        "  print (\"Table \" + str(bpr_table_id_fullname) + ' already exists.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Am4M1_dv1Yu"
      },
      "source": [
        "## Running the Per-series Analysis over multiple models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqY5fRzEyt3-"
      },
      "outputs": [],
      "source": [
        "# complete_series_id_list = series_id_list\n",
        "# series_id_list = complete_series_id_list[0:]\n",
        "\n",
        "# print ('series_id_list: ' + str(series_id_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlVeFqY92mkd"
      },
      "outputs": [],
      "source": [
        "# idx = 0\n",
        "# series_id = series_id_list[0]\n",
        "# model_idx = 0 \n",
        "# nnunet_model = '2d'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "QpfEOYyfvUkf"
      },
      "outputs": [],
      "source": [
        "# Test over 1 series \n",
        "# series_id_list = series_id_list[0:1]\n",
        "# Test over 2 series \n",
        "# series_id_list = series_id_list[1:3]\n",
        "# ['1.3.6.1.4.1.14519.5.2.1.7009.9004.760444618030368893044716329710'] # the one with the memory issue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKVGrSJS2eZH",
        "outputId": "297985bb-34d1-4e5e-f1e0-dcbdbd968fb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp /content/data/run_params.yaml s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/2d-tta/log/run_params.yaml\n",
            "Processing series 1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403\n",
            "Searching `s3://idc-medima-paper-dk/` for: \n",
            "nnunet/nnunet_output/nlst_25_series_06_16_22/2d-tta/nii/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.nii.gz\n",
            "\n",
            "value_found inference_time nnunet: 0\n",
            "Done download in 2.06011 seconds.\n",
            "\n",
            "Sorting DICOM files...\n",
            "100% 153/153 [00:00<00:00, 422.98it/s]\n",
            "Files sorted\n",
            "Done sorting in 0.821031 seconds.\n",
            "Sorted DICOM data saved at: /content/data/raw/nlst/dicom/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403\n",
            "Removing un-sorted data at /content/data/raw/tmp/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403...\n",
            "... Done.\n",
            "\n",
            "Running 'plastimatch convert' with the specified arguments:\n",
            "  --input /content/data/raw/nlst/dicom/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/CT\n",
            "  --output-img /content/data/processed/nlst/nnunet/nrrd/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_CT_orig.nrrd\n",
            "... Done.\n",
            "\n",
            "Running 'plastimatch resample' with the specified arguments:\n",
            "  --input /content/data/processed/nlst/nnunet/nrrd/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_CT_orig.nrrd\n",
            "  --output /content/data/processed/nlst/nnunet/nrrd/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_CT.nrrd\n",
            "  --spacing 1 1 2.5\n",
            "... Done.\n",
            "\n",
            "Running 'plastimatch convert' with the specified arguments:\n",
            "  --input /content/data/raw/nlst/dicom/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/CT\n",
            "  --output-img /content/data/processed/nlst/nii/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_CT.nii.gz\n",
            "... Done.\n",
            "cp /content/data/processed/nlst/nii/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_CT.nii.gz s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/2d-tta/dicomseg/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.nii.gz\n",
            "\n",
            "Running 'plastimatch resample' with the specified arguments:\n",
            "  --input /content/data/processed/nlst/nii/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_CT.nii.gz\n",
            "  --output /content/data/processed/nlst/nii/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_CT_resample.nii.gz\n",
            "  --spacing 1 1 2.5\n",
            "... Done.\n",
            "Copying /content/data/processed/nlst/nii/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_CT_resample.nii.gz\n",
            "to /content/data/nnunet/model_input/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_0000.nii.gz...\n",
            "... Done.\n",
            "Running `nnUNet_predict` with `2d` model...\n",
            "Processing file at /content/data/nnunet/model_input/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_0000.nii.gz...\n",
            "\n",
            "\n",
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
            "\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "\n",
            "nnUNet_raw_data_base is not defined and nnU-Net can only be used on data for which preprocessed files are already present on your system. nnU-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\n",
            "nnUNet_preprocessed is not defined and nnU-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\n",
            "using model stored in  /content/data/nnunet/nnunet_output/nnUNet/2d/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1\n",
            "This model expects 1 input modalities for each image\n",
            "Found 1 unique case ids, here are some examples: ['1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403']\n",
            "If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc\n",
            "number of cases: 1\n",
            "number of cases that still need to be predicted: 1\n",
            "emptying cuda cache\n",
            "loading parameters for folds, None\n",
            "folds is None so we will automatically look for output folders (not using 'all'!)\n",
            "found the following folds:  ['/content/data/nnunet/nnunet_output/nnUNet/2d/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0', '/content/data/nnunet/nnunet_output/nnUNet/2d/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1', '/content/data/nnunet/nnunet_output/nnUNet/2d/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_2', '/content/data/nnunet/nnunet_output/nnUNet/2d/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_3', '/content/data/nnunet/nnunet_output/nnUNet/2d/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_4']\n",
            "using the following model files:  ['/content/data/nnunet/nnunet_output/nnUNet/2d/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model', '/content/data/nnunet/nnunet_output/nnUNet/2d/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model', '/content/data/nnunet/nnunet_output/nnUNet/2d/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model', '/content/data/nnunet/nnunet_output/nnUNet/2d/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model', '/content/data/nnunet/nnunet_output/nnUNet/2d/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model']\n",
            "starting preprocessing generator\n",
            "starting prediction...\n",
            "preprocessing /content/data/nnunet/nnunet_output/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.nii.gz\n",
            "using preprocessor PreprocessorFor2D\n",
            "before crop: (1, 122, 350, 350) after crop: (1, 122, 350, 350) spacing: [2.5 1.  1. ] \n",
            "\n",
            "no separate z, order 3\n",
            "no separate z, order 1\n",
            "before: {'spacing': array([2.5, 1. , 1. ]), 'spacing_transposed': array([2.5, 1. , 1. ]), 'data.shape (data is transposed)': (1, 122, 350, 350)} \n",
            "after:  {'spacing': array([2.5       , 0.97656202, 0.97656202]), 'data.shape (data is resampled)': (1, 122, 358, 358)} \n",
            "\n",
            "normalization...\n",
            "normalization done\n",
            "(1, 122, 358, 358)\n",
            "This worker has ended successfully, no errors to report\n",
            "predicting /content/data/nnunet/nnunet_output/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "debug: mirroring True mirror_axes (0, 1)\n",
            "inference done. Now waiting for the segmentation export to finish...\n",
            "force_separate_z: None interpolation order: 1\n",
            "separate z: False lowres axis None\n",
            "no separate z, order 1\n",
            "postprocessing...\n",
            "Done in 101.668 seconds.\n",
            "\n",
            "Running 'plastimatch resample' with the specified arguments:\n",
            "  --input /content/data/nnunet/nnunet_output/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.nii.gz\n",
            "  --output /content/data/nnunet/nnunet_output/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_resample.nii.gz\n",
            "  --fixed /content/data/processed/nlst/nii/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_CT.nii.gz\n",
            "  --interpolation nn\n",
            "... Done.\n",
            "cp /content/data/nnunet/nnunet_output/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_resample.nii.gz s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/2d-tta/nii/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.nii.gz\n",
            "\n",
            "Running 'plastimatch convert' with the specified arguments:\n",
            "  --input /content/data/nnunet/nnunet_output/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.nii.gz\n",
            "  --output-img /content/data/processed/nlst/nnunet/nrrd/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_pred_segthor.nrrd\n",
            "... Done.\n",
            "\n",
            "Running 'plastimatch resample' with the specified arguments:\n",
            "  --input /content/data/processed/nlst/nnunet/nrrd/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_pred_segthor.nrrd\n",
            "  --output /content/data/processed/nlst/nnunet/nrrd/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_pred_segthor_resampled.nrrd\n",
            "  --fixed /content/data/processed/nlst/nii/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_CT.nii.gz\n",
            "  --interpolation nn\n",
            "... Done.\n",
            "cp /content/data/dicomseg_metadata_2d-tta.json s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/2d-tta/dicomseg_metadata_2d-tta.json\n",
            "dcmqi repository URL: git@github.com:QIICR/dcmqi.git revision: e25cb30 tag: latest-2-ge25cb30\n",
            "Loaded segmentation from /content/data/processed/nlst/nnunet/nrrd/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_pred_segthor_resampled.nrrd\n",
            "Searching recursively /content/data/raw/nlst/dicom/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/CT for DICOM files\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.452273021329681013196164566916 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.164249485331293393621410457295 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.320452133438035465547668875348 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.312959914227781612392485789008 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.846108019447382758565346836111 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.142631870823497909717282622370 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.304208747779810504303717534800 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.458733033331690178181700771360 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.196621322972595365263352809764 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.281874588021591866207765846021 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.954139825137378586143960487818 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.317883972589534994844985783062 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.511179091899908141192645233455 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.730951560751754761696010764988 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.150095020438132998502493318107 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.162473252698141644782474042995 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.276659244146716283976328446094 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.199952183022787889873481656837 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.210350429317917402042368739762 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.158530922904250591279653359121 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.309582856739404529050669408709 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.217931953035241813048953472024 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.332302685462863156570199023414 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.505589759939667014855097670043 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.168823646237992752413433650161 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.307026205588906158537270097553 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.329366147162255864716682426208 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.163601621180535047019935493621 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.691593929317053590140477048135 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.205442491820120992919173333160 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.121498288977909746596727375626 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.125284965617219218194464052365 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.222482425959375256995975846769 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.181723708176388780732048716911 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.100015046265647211391263078735 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.174736904323847367664013450150 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.205711591402947557750409350579 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.269471456892507967596313790958 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.714538900365486638598042941003 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.268920190401746892078887154634 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.250586891033691777996201927994 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.236075263719579276105086814084 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.205495954160067267812220084560 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.269641673702731556932823625402 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.322989761281427407808780325127 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.114419667859899194234122946191 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.247287469691651675628860190226 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.188101405696179418093303822605 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.736662482373177339127519070053 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.677309862221836713587072087088 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.179852669849093300383728901069 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.297479748881261692041786594472 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.222718653289411623771028179074 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.870173276301768591602321825688 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.233760713291687055271732883245 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.591022218077233963235549896806 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.162077631999794343194954931204 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.328457135009991444523171079102 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.323964100456363998894977598693 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.338588182038324250629187506231 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.860989229867166205621310050217 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.278468864858966246672804281180 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.790156723581480554238129011608 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.302296304989678738207592981397 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.291191180349162514166512406767 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.250455550438179214938509406172 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.334226922470928725368630286313 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.496809936140823016124817549143 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.719660987522658361160547243270 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.276606382297042198153248257528 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.191009797925741195845108320774 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.187210360443703621219346776709 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.277180599967286406739959791880 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.101339609371495941961631143335 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.161263741337089488955342701511 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.165707995679691698422650545189 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.313795028990866651917280919828 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.263162317222199348203659585804 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.199152630279383050672471199778 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.202818186464248835085568996536 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.128205953026311227053676926265 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.270741031268098872575314752413 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.250030760015332379408836881534 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.557220852095115575719595660418 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.174610965215055166731125963327 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.289494768279984168559652791680 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.264046606243675038055211772635 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.123225324119500753017515591773 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.174713259807076378875572419927 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.530794192165319147233002539598 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.145999595234199592627955119281 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.158159309048892262725167054292 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.223621808509308987413627280302 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.142283675827065104102197082181 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.194999247332637914557736385768 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.897581864745430233687632659006 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.168064913972860310655876094292 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.519125379002780043824511331726 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.245709992511744884752957268056 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.135148238169373313540476702606 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.310206609404609973219404397600 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.219094953517505230986295071986 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.328670154778567010510535286909 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.278434230599436796606259818762 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.665245919737591445173404779802 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.174736531133448357855624796934 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.278073255033560356919553726820 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.551162341202454901436664530984 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.152090643820118341865350450883 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.129471877974915677429277884632 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.265466823859832178021822687507 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.203840418038168500589781528519 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.115285489493433453651996667731 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.337436853835652335383643182257 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.247681252972615986755036908441 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.625805288769875478908377417977 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.262345757788179856531523564256 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.671206572237804054505301981553 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.254041656465498148289381794034 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.114189128809648227207090414579 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.230981803127416315408516896376 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.105989985560106062895009937606 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.293618810377868980694745734188 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.280931290525431297351167621252 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.937566432372425404091595310429 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.173877460883599392080333445948 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.179481289637678615662427767194 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.322525018900019193117916029309 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.183964036070704421396727857956 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.185528219869363565198272579365 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.146066236180065412561280363710 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.194666042173638385512046047425 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.158784374287196330458068401686 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.394309329929469512143931981447 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.249951981913169005014111991981 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.372492473010321488412408895950 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.220877498018231232688820349393 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.448332642215570079676047291197 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.287556081728082681109637053885 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.567554790838608571442919504908 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.286947816315390467984287207522 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.207535258634752009743525761685 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.233376543093571723557313354211 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.193243779701005331054120626497 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.476850549692770182499085514654 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.231433311648332726339924820606 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.428243885778898868580255047776 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.300729674688517508078058850938 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.258709720600203147896278004414 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.289819104208020345969236461256 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.101311335552046860998212100307 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.906288605687438579297003953792 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.237988510013467854513391328348 mapped\n",
            "153 of 153 slices mapped to source DICOM images\n",
            "Found 5 label(s)\n",
            "Processing label 1\n",
            "Total non-empty slices that will be encoded in SEG for label 1 is 117\n",
            " (inclusive from 35 to 152)\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Processing label 2\n",
            "Total non-empty slices that will be encoded in SEG for label 2 is 52\n",
            " (inclusive from 35 to 87)\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Processing label 3\n",
            "Total non-empty slices that will be encoded in SEG for label 3 is 58\n",
            " (inclusive from 94 to 152)\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Processing label 4\n",
            "Total non-empty slices that will be encoded in SEG for label 4 is 108\n",
            " (inclusive from 17 to 125)\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Saved segmentation as /content/data/processed/nlst/nnunet/dicomseg/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_SEG.dcm\n",
            "cp /content/data/processed/nlst/nnunet/dicomseg/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_SEG.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/2d-tta/dicomseg/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_SEG.dcm\n",
            "removing files from: /content/data/processed/nlst/nnunet/dicomseg/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403\n",
            "rm: cannot remove '/content/data/nnunet/nnunet_output/*.npz': No such file or directory\n",
            "rm: cannot remove '/content/data/processed/nlst/nnunet/nrrd/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/pred_softmax/*.nrrd': No such file or directory\n",
            "rm: cannot remove '/content/data/processed/nlst/nnunet/nrrd/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/pred_softmax_resampled/*.nrrd': No such file or directory\n",
            "End-to-end processing of 1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403 completed in 170.778 seconds.\n",
            "\n",
            "Searching `s3://idc-medima-paper-dk/` for: \n",
            "bpr/bpr_output/nlst_25_series_06_16_22/json/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.json\n",
            "\n",
            "value_found inference_time_bpr: 0\n",
            "Copying /content/data/processed/nlst/nii/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_CT.nii.gz\n",
            "to /content/data/bpr/model_input/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.nii.gz...\n",
            "... Done.\n",
            "Create body-part meta data file: 1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.json\n",
            "Done in 8.48704 seconds.\n",
            "cp /content/data/bpr/bpr_output/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.json s3://idc-medima-paper-dk/bpr/bpr_output/nlst_25_series_06_16_22/json/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.json\n",
            "Copy json file to bucket: 0.7970964908599854\n",
            "rm: cannot remove '/content/data/processed/bpr/nii': No such file or directory\n",
            "End-to-end processing of 1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403 completed in 13.9114 seconds.\n",
            "\n",
            "cp /content/data/run_params.yaml s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/log/run_params.yaml\n",
            "Processing series 1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403\n",
            "Searching `s3://idc-medima-paper-dk/` for: \n",
            "nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/nii/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.nii.gz\n",
            "\n",
            "value_found inference_time nnunet: 1\n",
            "Running `nnUNet_predict` with `3d_fullres` model...\n",
            "Processing file at /content/data/nnunet/model_input/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_0000.nii.gz...\n",
            "\n",
            "\n",
            "Please cite the following paper when using nnUNet:\n",
            "\n",
            "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
            "\n",
            "\n",
            "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
            "\n",
            "nnUNet_raw_data_base is not defined and nnU-Net can only be used on data for which preprocessed files are already present on your system. nnU-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\n",
            "nnUNet_preprocessed is not defined and nnU-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\n",
            "using model stored in  /content/data/nnunet/nnunet_output/nnUNet/3d_fullres/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1\n",
            "This model expects 1 input modalities for each image\n",
            "Found 1 unique case ids, here are some examples: ['1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403']\n",
            "If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc\n",
            "number of cases: 1\n",
            "number of cases that still need to be predicted: 1\n",
            "emptying cuda cache\n",
            "loading parameters for folds, None\n",
            "folds is None so we will automatically look for output folders (not using 'all'!)\n",
            "found the following folds:  ['/content/data/nnunet/nnunet_output/nnUNet/3d_fullres/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0', '/content/data/nnunet/nnunet_output/nnUNet/3d_fullres/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1', '/content/data/nnunet/nnunet_output/nnUNet/3d_fullres/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_2', '/content/data/nnunet/nnunet_output/nnUNet/3d_fullres/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_3', '/content/data/nnunet/nnunet_output/nnUNet/3d_fullres/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_4']\n",
            "using the following model files:  ['/content/data/nnunet/nnunet_output/nnUNet/3d_fullres/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model', '/content/data/nnunet/nnunet_output/nnUNet/3d_fullres/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model', '/content/data/nnunet/nnunet_output/nnUNet/3d_fullres/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model', '/content/data/nnunet/nnunet_output/nnUNet/3d_fullres/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model', '/content/data/nnunet/nnunet_output/nnUNet/3d_fullres/Task055_SegTHOR/nnUNetTrainerV2__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model']\n",
            "starting preprocessing generator\n",
            "starting prediction...\n",
            "preprocessing /content/data/nnunet/nnunet_output/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.nii.gz\n",
            "using preprocessor GenericPreprocessor\n",
            "before crop: (1, 122, 350, 350) after crop: (1, 122, 350, 350) spacing: [2.5 1.  1. ] \n",
            "\n",
            "no separate z, order 3\n",
            "no separate z, order 1\n",
            "before: {'spacing': array([2.5, 1. , 1. ]), 'spacing_transposed': array([2.5, 1. , 1. ]), 'data.shape (data is transposed)': (1, 122, 350, 350)} \n",
            "after:  {'spacing': array([2.5       , 0.97656202, 0.97656202]), 'data.shape (data is resampled)': (1, 122, 358, 358)} \n",
            "\n",
            "(1, 122, 358, 358)\n",
            "This worker has ended successfully, no errors to report\n",
            "predicting /content/data/nnunet/nnunet_output/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.nii.gz\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "step_size: 0.5\n",
            "do mirror: True\n",
            "data shape: (1, 122, 358, 358)\n",
            "patch size: [ 64 192 160]\n",
            "steps (x, y, and z): [[0, 29, 58], [0, 83, 166], [0, 66, 132, 198]]\n",
            "number of tiles: 36\n",
            "computing Gaussian\n",
            "prediction done\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "step_size: 0.5\n",
            "do mirror: True\n",
            "data shape: (1, 122, 358, 358)\n",
            "patch size: [ 64 192 160]\n",
            "steps (x, y, and z): [[0, 29, 58], [0, 83, 166], [0, 66, 132, 198]]\n",
            "number of tiles: 36\n",
            "using precomputed Gaussian\n",
            "prediction done\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "step_size: 0.5\n",
            "do mirror: True\n",
            "data shape: (1, 122, 358, 358)\n",
            "patch size: [ 64 192 160]\n",
            "steps (x, y, and z): [[0, 29, 58], [0, 83, 166], [0, 66, 132, 198]]\n",
            "number of tiles: 36\n",
            "using precomputed Gaussian\n",
            "prediction done\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "step_size: 0.5\n",
            "do mirror: True\n",
            "data shape: (1, 122, 358, 358)\n",
            "patch size: [ 64 192 160]\n",
            "steps (x, y, and z): [[0, 29, 58], [0, 83, 166], [0, 66, 132, 198]]\n",
            "number of tiles: 36\n",
            "using precomputed Gaussian\n",
            "prediction done\n",
            "debug: mirroring True mirror_axes (0, 1, 2)\n",
            "step_size: 0.5\n",
            "do mirror: True\n",
            "data shape: (1, 122, 358, 358)\n",
            "patch size: [ 64 192 160]\n",
            "steps (x, y, and z): [[0, 29, 58], [0, 83, 166], [0, 66, 132, 198]]\n",
            "number of tiles: 36\n",
            "using precomputed Gaussian\n",
            "prediction done\n",
            "inference done. Now waiting for the segmentation export to finish...\n",
            "force_separate_z: None interpolation order: 1\n",
            "separate z: False lowres axis None\n",
            "no separate z, order 1\n",
            "postprocessing...\n",
            "Done in 818.364 seconds.\n",
            "\n",
            "Running 'plastimatch resample' with the specified arguments:\n",
            "  --input /content/data/nnunet/nnunet_output/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.nii.gz\n",
            "  --output /content/data/nnunet/nnunet_output/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_resample.nii.gz\n",
            "  --fixed /content/data/processed/nlst/nii/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_CT.nii.gz\n",
            "  --interpolation nn\n",
            "... Done.\n",
            "cp /content/data/nnunet/nnunet_output/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_resample.nii.gz s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/nii/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.nii.gz\n",
            "\n",
            "Running 'plastimatch convert' with the specified arguments:\n",
            "  --input /content/data/nnunet/nnunet_output/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403.nii.gz\n",
            "  --output-img /content/data/processed/nlst/nnunet/nrrd/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_pred_segthor.nrrd\n",
            "... Done.\n",
            "\n",
            "Running 'plastimatch resample' with the specified arguments:\n",
            "  --input /content/data/processed/nlst/nnunet/nrrd/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_pred_segthor.nrrd\n",
            "  --output /content/data/processed/nlst/nnunet/nrrd/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_pred_segthor_resampled.nrrd\n",
            "  --fixed /content/data/processed/nlst/nii/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_CT.nii.gz\n",
            "  --interpolation nn\n",
            "... Done.\n",
            "cp /content/data/dicomseg_metadata_3d_fullres-tta.json s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/dicomseg_metadata_3d_fullres-tta.json\n",
            "dcmqi repository URL: git@github.com:QIICR/dcmqi.git revision: e25cb30 tag: latest-2-ge25cb30\n",
            "Loaded segmentation from /content/data/processed/nlst/nnunet/nrrd/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_pred_segthor_resampled.nrrd\n",
            "Searching recursively /content/data/raw/nlst/dicom/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/CT for DICOM files\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.452273021329681013196164566916 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.164249485331293393621410457295 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.320452133438035465547668875348 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.312959914227781612392485789008 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.846108019447382758565346836111 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.142631870823497909717282622370 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.304208747779810504303717534800 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.458733033331690178181700771360 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.196621322972595365263352809764 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.281874588021591866207765846021 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.954139825137378586143960487818 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.317883972589534994844985783062 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.511179091899908141192645233455 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.730951560751754761696010764988 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.150095020438132998502493318107 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.162473252698141644782474042995 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.276659244146716283976328446094 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.199952183022787889873481656837 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.210350429317917402042368739762 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.158530922904250591279653359121 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.309582856739404529050669408709 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.217931953035241813048953472024 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.332302685462863156570199023414 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.505589759939667014855097670043 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.168823646237992752413433650161 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.307026205588906158537270097553 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.329366147162255864716682426208 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.163601621180535047019935493621 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.691593929317053590140477048135 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.205442491820120992919173333160 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.121498288977909746596727375626 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.125284965617219218194464052365 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.222482425959375256995975846769 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.181723708176388780732048716911 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.100015046265647211391263078735 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.174736904323847367664013450150 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.205711591402947557750409350579 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.269471456892507967596313790958 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.714538900365486638598042941003 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.268920190401746892078887154634 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.250586891033691777996201927994 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.236075263719579276105086814084 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.205495954160067267812220084560 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.269641673702731556932823625402 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.322989761281427407808780325127 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.114419667859899194234122946191 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.247287469691651675628860190226 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.188101405696179418093303822605 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.736662482373177339127519070053 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.677309862221836713587072087088 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.179852669849093300383728901069 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.297479748881261692041786594472 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.222718653289411623771028179074 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.870173276301768591602321825688 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.233760713291687055271732883245 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.591022218077233963235549896806 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.162077631999794343194954931204 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.328457135009991444523171079102 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.323964100456363998894977598693 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.338588182038324250629187506231 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.860989229867166205621310050217 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.278468864858966246672804281180 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.790156723581480554238129011608 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.302296304989678738207592981397 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.291191180349162514166512406767 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.250455550438179214938509406172 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.334226922470928725368630286313 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.496809936140823016124817549143 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.719660987522658361160547243270 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.276606382297042198153248257528 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.191009797925741195845108320774 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.187210360443703621219346776709 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.277180599967286406739959791880 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.101339609371495941961631143335 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.161263741337089488955342701511 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.165707995679691698422650545189 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.313795028990866651917280919828 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.263162317222199348203659585804 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.199152630279383050672471199778 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.202818186464248835085568996536 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.128205953026311227053676926265 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.270741031268098872575314752413 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.250030760015332379408836881534 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.557220852095115575719595660418 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.174610965215055166731125963327 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.289494768279984168559652791680 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.264046606243675038055211772635 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.123225324119500753017515591773 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.174713259807076378875572419927 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.530794192165319147233002539598 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.145999595234199592627955119281 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.158159309048892262725167054292 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.223621808509308987413627280302 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.142283675827065104102197082181 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.194999247332637914557736385768 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.897581864745430233687632659006 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.168064913972860310655876094292 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.519125379002780043824511331726 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.245709992511744884752957268056 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.135148238169373313540476702606 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.310206609404609973219404397600 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.219094953517505230986295071986 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.328670154778567010510535286909 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.278434230599436796606259818762 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.665245919737591445173404779802 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.174736531133448357855624796934 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.278073255033560356919553726820 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.551162341202454901436664530984 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.152090643820118341865350450883 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.129471877974915677429277884632 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.265466823859832178021822687507 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.203840418038168500589781528519 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.115285489493433453651996667731 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.337436853835652335383643182257 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.247681252972615986755036908441 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.625805288769875478908377417977 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.262345757788179856531523564256 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.671206572237804054505301981553 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.254041656465498148289381794034 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.114189128809648227207090414579 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.230981803127416315408516896376 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.105989985560106062895009937606 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.293618810377868980694745734188 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.280931290525431297351167621252 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.937566432372425404091595310429 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.173877460883599392080333445948 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.179481289637678615662427767194 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.322525018900019193117916029309 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.183964036070704421396727857956 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.185528219869363565198272579365 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.146066236180065412561280363710 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.194666042173638385512046047425 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.158784374287196330458068401686 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.394309329929469512143931981447 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.249951981913169005014111991981 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.372492473010321488412408895950 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.220877498018231232688820349393 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.448332642215570079676047291197 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.287556081728082681109637053885 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.567554790838608571442919504908 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.286947816315390467984287207522 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.207535258634752009743525761685 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.233376543093571723557313354211 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.193243779701005331054120626497 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.476850549692770182499085514654 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.231433311648332726339924820606 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.428243885778898868580255047776 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.300729674688517508078058850938 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.258709720600203147896278004414 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.289819104208020345969236461256 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.101311335552046860998212100307 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.906288605687438579297003953792 mapped\n",
            "SOPInstanceUID1.3.6.1.4.1.14519.5.2.1.7009.9004.237988510013467854513391328348 mapped\n",
            "153 of 153 slices mapped to source DICOM images\n",
            "Found 5 label(s)\n",
            "Processing label 1\n",
            "Total non-empty slices that will be encoded in SEG for label 1 is 120\n",
            " (inclusive from 32 to 152)\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Processing label 2\n",
            "Total non-empty slices that will be encoded in SEG for label 2 is 55\n",
            " (inclusive from 34 to 89)\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Processing label 3\n",
            "Total non-empty slices that will be encoded in SEG for label 3 is 58\n",
            " (inclusive from 94 to 152)\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Processing label 4\n",
            "Total non-empty slices that will be encoded in SEG for label 4 is 109\n",
            " (inclusive from 16 to 125)\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Added source image item\n",
            "Saved segmentation as /content/data/processed/nlst/nnunet/dicomseg/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_SEG.dcm\n",
            "cp /content/data/processed/nlst/nnunet/dicomseg/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_SEG.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/dicomseg/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403_SEG.dcm\n",
            "removing files from: /content/data/processed/nlst/nnunet/dicomseg/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403\n",
            "rm: cannot remove '/content/data/nnunet/nnunet_output/*.npz': No such file or directory\n",
            "rm: cannot remove '/content/data/processed/nlst/nnunet/nrrd/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/pred_softmax/*.nrrd': No such file or directory\n",
            "rm: cannot remove '/content/data/processed/nlst/nnunet/nrrd/1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403/pred_softmax_resampled/*.nrrd': No such file or directory\n",
            "End-to-end processing of 1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633855123925781347403 completed in 833.801 seconds.\n",
            "\n",
            "*****deleting files for the next series run ******\n"
          ]
        }
      ],
      "source": [
        "# Run over each series, do the checking if processed in loop\n",
        "for idx, series_id in enumerate(series_id_list):\n",
        "\n",
        "  # Run over each model \n",
        "  for model_idx, nnunet_model in enumerate(model_list):\n",
        "\n",
        "\n",
        "    # -----------------\n",
        "    # nnU-Net pipeline parameters\n",
        "\n",
        "    experiment_folder_name = nnunet_model + \"-tta\" if use_tta == True else nnunet_model + \"-no_tta\"\n",
        "    bucket_experiment_folder_uri_nnunet = os.path.join(bucket_output_base_uri_nnunet, experiment_folder_name)\n",
        "\n",
        "    bucket_log_folder_uri_nnunet = os.path.join(bucket_experiment_folder_uri_nnunet, 'log')\n",
        "\n",
        "    bucket_nifti_folder_uri_nnunet = os.path.join(bucket_experiment_folder_uri_nnunet, 'nii')\n",
        "    bucket_softmax_pred_folder_uri_nnunet = os.path.join(bucket_experiment_folder_uri_nnunet, 'softmax_pred')\n",
        "\n",
        "    bucket_dicomseg_folder_uri_nnunet = os.path.join(bucket_experiment_folder_uri_nnunet, 'dicomseg')\n",
        "\n",
        "    # -----------------\n",
        "    # save run information\n",
        "\n",
        "    yaml_fn = \"run_params.yaml\"\n",
        "    yaml_out_path = os.path.join(data_base_path, yaml_fn)\n",
        "\n",
        "    settings_dict = dict()\n",
        "    settings_dict[\"bucket\"] = dict()\n",
        "    settings_dict[\"bucket\"][\"name\"] = bucket_name\n",
        "    settings_dict[\"bucket\"][\"base_uri\"] = bucket_base_uri\n",
        "    settings_dict[\"bucket\"][\"output_base_uri\"] = bucket_output_base_uri_nnunet\n",
        "    settings_dict[\"bucket\"][\"experiment_folder_uri\"] = bucket_experiment_folder_uri_nnunet\n",
        "    settings_dict[\"bucket\"][\"nifti_folder_uri\"] = bucket_nifti_folder_uri_nnunet\n",
        "    settings_dict[\"bucket\"][\"softmax_pred_folder_uri\"] = bucket_softmax_pred_folder_uri_nnunet\n",
        "    settings_dict[\"bucket\"][\"dicomseg_folder_uri\"] = bucket_dicomseg_folder_uri_nnunet\n",
        "    settings_dict[\"bucket\"][\"log_folder_uri\"] = bucket_log_folder_uri_nnunet\n",
        "\n",
        "    settings_dict[\"inference\"] = dict()\n",
        "    settings_dict[\"inference\"][\"model\"] = nnunet_model\n",
        "    settings_dict[\"inference\"][\"use_tta\"] = use_tta\n",
        "    settings_dict[\"inference\"][\"export_prob_maps\"] = export_prob_maps\n",
        "\n",
        "    with open(yaml_out_path, 'w') as fp:\n",
        "      yaml.dump(settings_dict, fp, default_flow_style = False)\n",
        "\n",
        "    gs_uri_yaml_file = os.path.join(bucket_log_folder_uri_nnunet, yaml_fn)\n",
        "\n",
        "    #!gsutil -m cp $yaml_out_path $gs_uri_yaml_file\n",
        "    !/content/s5cmd --endpoint-url https://storage.googleapis.com cp $yaml_out_path $gs_uri_yaml_file\n",
        "\n",
        "    #  -----------------\n",
        "    # init\n",
        "\n",
        "    start_total_nnunet = time.time()\n",
        "\n",
        "    # init every single time, as the most recent logs are loaded from the bucket\n",
        "    inference_time_dict_nnunet = dict()\n",
        "    total_time_dict_nnunet = dict()\n",
        "    inference_time_dict_bpr = dict()\n",
        "    total_time_dict_bpr = dict()\n",
        "\n",
        "    # clear_output(wait = True)\n",
        "\n",
        "    # print(\"(%g/%g) Processing series %s\"%(idx + 1, len(series_to_process_id_list), series_id))\n",
        "    print(\"Processing series %s\"%(series_id))\n",
        "\n",
        "    series_df = cohort_df[cohort_df[\"SeriesInstanceUID\"] == series_id]\n",
        "    num_instances = series_df['num_instances'].to_list()[0]\n",
        "\n",
        "    has_segmask_already = False\n",
        "\n",
        "    dicomseg_fn = series_id + \"_SEG.dcm\"\n",
        "\n",
        "    input_nifti_fn = series_id + \"_0000.nii.gz\"\n",
        "    input_nifti_path = os.path.join(model_input_folder_nnunet, input_nifti_fn)\n",
        "\n",
        "    pred_nifti_fn = series_id + \".nii.gz\"\n",
        "    pred_nifti_path = os.path.join(model_output_folder_nnunet, pred_nifti_fn)\n",
        "\n",
        "    pred_softmax_folder_name = \"pred_softmax\"\n",
        "    pred_softmax_folder_path = os.path.join(processed_nrrd_path_nnunet, series_id, pred_softmax_folder_name)\n",
        "\n",
        "    # -----------------\n",
        "    # GS URI definition\n",
        "\n",
        "    # gs URI at which the *nii.gz object is or will be stored in the bucket\n",
        "    gs_uri_nifti_file = os.path.join(bucket_nifti_folder_uri_nnunet, pred_nifti_fn)\n",
        "\n",
        "    # gs URI at which the folder storing the *.nrrd softmax probabilities is or will be stored in the bucket\n",
        "    gs_uri_softmax_pred_folder = os.path.join(bucket_softmax_pred_folder_uri_nnunet, series_id)\n",
        "\n",
        "    # gs URI at which the DICOM SEG object is or will be stored in the bucket\n",
        "    gs_uri_dicomseg_file = os.path.join(bucket_dicomseg_folder_uri_nnunet, dicomseg_fn)\n",
        "\n",
        "    # DK added - gs URI at which the CT to nii file is or will be stored in the bucket \n",
        "    gs_uri_ct_nifti_file = os.path.join(bucket_dicomseg_folder_uri_nnunet, pred_nifti_fn)\n",
        "\n",
        "\n",
        "    # -----------------\n",
        "    # cross-load the CT data from the IDC buckets, run the preprocessing\n",
        "\n",
        "    # check whether the NIfTI seg mask exists already\n",
        "    has_segmask_already = file_exists_in_bucket(project_name = project_name,\n",
        "                                                bucket_name = bucket_name,\n",
        "                                                file_gs_uri = gs_uri_nifti_file)\n",
        "\n",
        "    # Query to see if the series for the specific model has been run \n",
        "    field_name = 'inference_time_' + experiment_folder_name.replace('-', '_')\n",
        "    value_found = check_value_exists_in_table(nnunet_table_id_fullname, project_name, series_id, field_name)\n",
        "    print ('value_found inference_time nnunet: ' + str(value_found))\n",
        "\n",
        "    # if the seg mask doesn't exist, and the value doesn't exist in the table, do the processing \n",
        "    # if has_segmask_already == False or value_found == 0: \n",
        "    # if True: # for now \n",
        "    if has_segmask_already == False: \n",
        "\n",
        "      # if the raw segmentation file exists in the output directory but the DICOM SEG\n",
        "      # doesn't, skip the inference phase. Data still need to be downloaded because\n",
        "      # the DICOM folder is essential in the DICOM SEG generation process\n",
        "      # If the download_path already exists, we don't need to download it again for this series \n",
        "      \n",
        "      # download_path = os.path.join(raw_base_path, series_id)\n",
        "      # if not os.path.exists(download_path):\n",
        "      download_path = os.path.join(sorted_base_path, series_id) # should only be deleted after all models are run for the series, not after bpr like before.\n",
        "      if not os.path.exists(download_path):\n",
        "      # if True: # for now\n",
        "        start_time_download_series_data = time.time()\n",
        "        download_series_data_s5cmd(raw_base_path = raw_base_path,\n",
        "                                    sorted_base_path = sorted_base_path,\n",
        "                                    series_df = series_df,\n",
        "                                    remove_raw = True)\n",
        "        elapsed_time_download_series_data = time.time()-start_time_download_series_data\n",
        "\n",
        "        # DICOM CT to NRRD - good to have for a number of reasons\n",
        "        # pypla_dicom_ct_to_nrrd(sorted_base_path = sorted_base_path,\n",
        "        #                         processed_nrrd_path = processed_nrrd_path_nnunet,\n",
        "        #                         pat_id = series_id, \n",
        "        #                         verbose = True)\n",
        "        pypla_dicom_ct_to_nrrd_resample(sorted_base_path = sorted_base_path,\n",
        "                                        processed_nrrd_path = processed_nrrd_path_nnunet,\n",
        "                                        pat_id = series_id, \n",
        "                                        verbose = True)\n",
        "\n",
        "        # DICOM CT to NIfTI - required for the processing\n",
        "        start_time_ct_to_nii = time.time()\n",
        "        pypla_dicom_ct_to_nifti(sorted_base_path = sorted_base_path,\n",
        "                                processed_nifti_path = processed_nifti_path,\n",
        "                                pat_id = series_id, \n",
        "                                verbose = True)\n",
        "        elapsed_time_ct_to_nii = time.time()-start_time_ct_to_nii \n",
        "\n",
        "        # added DK\n",
        "        # upload to bucket \n",
        "        ct_nifti_path = os.path.join(processed_nifti_path,series_id,series_id+\"_CT.nii.gz\")\n",
        "        # !gsutil -m cp $ct_nifti_path $gs_uri_ct_nifti_file \n",
        "        !/content/s5cmd --endpoint-url https://storage.googleapis.com cp $ct_nifti_path $gs_uri_ct_nifti_file\n",
        "\n",
        "        # # prepare the `model_input` folder for the inference phase\n",
        "        # prep_input_data(processed_nifti_path = processed_nifti_path,\n",
        "        #                 model_input_folder = model_input_folder_nnunet,\n",
        "        #                 pat_id = series_id)\n",
        "        # prepare the `model_input` folder for the inference phase\n",
        "        # resample the ct image to 1x1x2.5 before using in inference \n",
        "        prep_input_data_resample(processed_nifti_path = processed_nifti_path,\n",
        "                                  model_input_folder = model_input_folder_nnunet,\n",
        "                                  pat_id = series_id, \n",
        "                                  verbose = True)\n",
        "\n",
        "\n",
        "\n",
        "      start_inference_nnunet = time.time()\n",
        "      # run the DL-based prediction\n",
        "      process_patient_nnunet(model_input_folder = model_input_folder_nnunet,\n",
        "                            model_output_folder = model_output_folder_nnunet, \n",
        "                            nnunet_model = nnunet_model, use_tta = use_tta,\n",
        "                            export_prob_maps = export_prob_maps, verbose = True)\n",
        "\n",
        "      elapsed_inference_nnunet = time.time() - start_inference_nnunet\n",
        "      # inference_time_dict_nnunet[series_id] = elapsed_inference_nnunet\n",
        "\n",
        "      # Need to create a CT nrrd file that is resampled to 1x1x2.5 -- the header of this \n",
        "      # is used as input to create the individual nrrd segments. \n",
        "      # The nrrd in /content/data/processed/nlst/nnunet/nrrd/1.3.6.1.4.1.14519.5.2.1.7009.9004.120988899216969247850007613978/1.3.6.1.4.1.14519.5.2.1.7009.9004.120988899216969247850007613978_CT.nrrd\n",
        "      # is in the original space. \n",
        "\n",
        "      # get the dim from the original file \n",
        "      ct_nifti_path = os.path.join(processed_nifti_path,series_id,series_id+\"_CT.nii.gz\")\n",
        "      nii = nib.load(ct_nifti_path)\n",
        "      spacing = [nii.header['pixdim'][1], nii.header['pixdim'][2], nii.header['pixdim'][3]]\n",
        "      # spacing_str = \" \".join([spacing[0], spacing[1], spacing[2]])\n",
        "      spacing_str = \" \".join([str(s) for s in spacing])\n",
        "\n",
        "      if export_prob_maps: \n",
        "        # convert the softmax predictions to NRRD files\n",
        "        numpy_to_nrrd(model_output_folder = model_output_folder_nnunet,\n",
        "                      processed_nrrd_path = processed_nrrd_path_nnunet,\n",
        "                      pat_id = series_id,\n",
        "                      output_folder_name = pred_softmax_folder_name)\n",
        "        \n",
        "        ### Resample the nrrd files back to the original space before copying to the bucket ### \n",
        "        # This should be linear interpolation because we are resampling the softmax predictions.\n",
        "        # get list of files that need to be converted \n",
        "        input_folder_path = os.path.join(processed_nrrd_path_nnunet,series_id,\"pred_softmax\")\n",
        "        output_folder_path = os.path.join(processed_nrrd_path_nnunet,series_id,\"pred_softmax_resample\")\n",
        "        if not os.path.isdir(output_folder_path):\n",
        "          os.mkdir(output_folder_path)\n",
        "        nrrd_files_input = [os.path.join(input_folder_path,f) for f in os.listdir(input_folder_path)]\n",
        "        nrrd_files_output = [os.path.join(output_folder_path,f) for f in os.listdir(input_folder_path)]\n",
        "        # convert each file \n",
        "        for input_file, output_file in zip(nrrd_files_input, nrrd_files_output):\n",
        "          # convert_args_ct = {\"input\" : input_file,\n",
        "          #                     \"output\" : output_file,\n",
        "          #                     \"spacing\": spacing_str,\n",
        "          #                     \"interpolation\": \"linear\"}\n",
        "          convert_args_ct = {\"input\" : input_file,\n",
        "                            \"output\" : output_file,\n",
        "                            \"fixed\": ct_nifti_path,\n",
        "                            \"interpolation\": \"linear\"}\n",
        "          resample_log_file = os.path.join(output_folder_path, 'resample.log')\n",
        "          pypla.resample(verbose = True,\n",
        "                        path_to_log_file = resample_log_file,\n",
        "                        **convert_args_ct)\n",
        "\n",
        "      # copy the nnU-Net *.npz softmax probabilities in the chosen bucket\n",
        "      # !gsutil -m cp $pred_softmax_folder_path/* $gs_uri_softmax_pred_folder\n",
        "      if (export_prob_maps):\n",
        "        # !/content/s5cmd --endpoint-url https://storage.googleapis.com cp $pred_softmax_folder_path/ $gs_uri_softmax_pred_folder/\n",
        "        !/content/s5cmd --endpoint-url https://storage.googleapis.com cp $output_folder_path/ $gs_uri_softmax_pred_folder/\n",
        "      \n",
        "\n",
        "      ### Resample the nii.gz file to original space before copying to bucket \n",
        "      # copy the nnU-Net *.nii.gz binary masks in the chosen bucket\n",
        "      # !gsutil -m cp $pred_nifti_path $gs_uri_nifti_file\n",
        "      # !/content/s5cmd --endpoint-url https://storage.googleapis.com cp $pred_nifti_path $gs_uri_nifti_file\n",
        "      pred_nifti_path_resample = os.path.join(model_output_folder_nnunet, series_id + '_resample.nii.gz')\n",
        "      # convert_args_ct = {\"input\" : pred_nifti_path,\n",
        "      #                     \"output\" : pred_nifti_path_resample,\n",
        "      #                     \"spacing\": spacing_str, \n",
        "      #                     \"interpolation\": \"nn\"}\n",
        "      convert_args_ct = {\"input\" : pred_nifti_path,\n",
        "                          \"output\" : pred_nifti_path_resample,\n",
        "                          \"fixed\": ct_nifti_path, \n",
        "                          \"interpolation\": \"nn\"}\n",
        "      pred_nifti_path_resample_log_file = os.path.join(model_output_folder_nnunet, 'resample.log')\n",
        "      pypla.resample(verbose = True,\n",
        "                    path_to_log_file = pred_nifti_path_resample_log_file,\n",
        "                    **convert_args_ct)\n",
        "      !/content/s5cmd --endpoint-url https://storage.googleapis.com cp $pred_nifti_path_resample $gs_uri_nifti_file\n",
        "      \n",
        "      \n",
        "      # remove the NIfTI file the prediction was computed from\n",
        "      # !rm $input_nifti_path\n",
        "\n",
        "        \n",
        "      # -----------------\n",
        "      # post-processing\n",
        "      pypla_postprocess(processed_nrrd_path = processed_nrrd_path_nnunet,\n",
        "                        model_output_folder = model_output_folder_nnunet,\n",
        "                        pat_id = series_id)\n",
        "      \n",
        "      ### Resample the nrrd file back to the original space, before converting to dicomseg ### \n",
        "      pred_nrrd_path_input = os.path.join(processed_nrrd_path_nnunet, series_id, series_id + \"_pred_segthor.nrrd\")\n",
        "      pred_nrrd_path_output = os.path.join(processed_nrrd_path_nnunet, series_id, series_id + \"_pred_segthor_resampled.nrrd\")\n",
        "      resample_log_file = os.path.join(processed_nrrd_path_nnunet, series_id, series_id + '_resample_log.log')\n",
        "      # convert_args_ct = {\"input\" : pred_nrrd_path_input,\n",
        "      #                     \"output\" : pred_nrrd_path_output,\n",
        "      #                     \"spacing\": spacing_str, \n",
        "      #                     \"interpolation\": \"nn\"}\n",
        "      convert_args_ct = {\"input\" : pred_nrrd_path_input,\n",
        "                        \"output\" : pred_nrrd_path_output,\n",
        "                        \"fixed\": ct_nifti_path, \n",
        "                        \"interpolation\": \"nn\"}\n",
        "      pypla.resample(verbose = True,\n",
        "              path_to_log_file = resample_log_file,\n",
        "              **convert_args_ct)\n",
        "\n",
        "      \n",
        "      # Modify the dicomseg_json file so that the SegmentAlgorithmName is representative of the model and other parameters \n",
        "      # Writes out the json file \n",
        "      SegmentAlgorithmName = experiment_folder_name \n",
        "      dicomseg_json_path_modified = \"/content/data/dicomseg_metadata_\" + SegmentAlgorithmName + '.json'\n",
        "      modify_dicomseg_json_file(dicomseg_json_path, dicomseg_json_path_modified, SegmentAlgorithmName)\n",
        "      # upload the json file \n",
        "      gs_uri_dicomseg_json_file = os.path.join(bucket_experiment_folder_uri_nnunet, 'dicomseg_metadata_' + SegmentAlgorithmName + '.json')\n",
        "      !/content/s5cmd --endpoint-url https://storage.googleapis.com cp $dicomseg_json_path_modified $gs_uri_dicomseg_json_file\n",
        "\n",
        "      # nrrd_to_dicomseg(sorted_base_path = sorted_base_path,\n",
        "      #                  processed_base_path = processed_base_path,\n",
        "      #                  dicomseg_json_path = dicomseg_json_path_modified, # dicomseg_json_path\n",
        "      #                  pat_id = series_id)\n",
        "      nrrd_to_dicomseg_resampled(sorted_base_path = sorted_base_path,\n",
        "                  processed_base_path = processed_base_path,\n",
        "                  dicomseg_json_path = dicomseg_json_path_modified, # dicomseg_json_path\n",
        "                  pat_id = series_id)\n",
        "\n",
        "      pred_dicomseg_path = os.path.join(processed_dicomseg_path_nnunet, series_id, dicomseg_fn)\n",
        "\n",
        "      # !gsutil -m cp $pred_dicomseg_path $gs_uri_dicomseg_file\n",
        "      !/content/s5cmd --endpoint-url https://storage.googleapis.com cp $pred_dicomseg_path $gs_uri_dicomseg_file\n",
        "\n",
        "      # delete the files in the pat_dir_dicomseg_path\n",
        "      processed_dicomseg_path = os.path.join(processed_base_path, \"nnunet\", \"dicomseg\")\n",
        "      pat_dir_dicomseg_path = os.path.join(processed_dicomseg_path, series_id)\n",
        "      print ('removing files from: ' + pat_dir_dicomseg_path)\n",
        "      !rm -r $pat_dir_dicomseg_path/*\n",
        "      !rmdir $pat_dir_dicomseg_path\n",
        "\n",
        "      # remove files from the nnunet output prediction \n",
        "      !rm /content/data/nnunet/nnunet_output/*.nii.gz\n",
        "      !rm /content/data/nnunet/nnunet_output/*.npz\n",
        "      !rm /content/data/nnunet/nnunet_output/*.pkl\n",
        "      !rm /content/data/nnunet/nnunet_output/*.json\n",
        "\n",
        "      # # delete the files in the processed_nrrd_path_nnunet\n",
        "      # # but don't want to delete the _CT.nrrd file, the rest are the segments\n",
        "      # print ('removing files excluding CT.nrrd from: ' + processed_nrrd_path_nnunet)\n",
        "      # # !rm -r $processed_nrrd_path_nnunet/*\n",
        "      processed_nrrd_path_nnunet_segments = os.path.join(processed_nrrd_path_nnunet,series_id,'pred_softmax')\n",
        "      !rm $processed_nrrd_path_nnunet_segments/*.nrrd\n",
        "      !rm $processed_nrrd_path_nnunet/$series_id/*.log\n",
        "      !rm $processed_nrrd_path_nnunet/$series_id/*_pred_segthor*.nrrd\n",
        "      # delete the files in the resampled directory \n",
        "      processed_nrrd_path_nnunet_segments_resampled = os.path.join(processed_nrrd_path_nnunet,series_id,'pred_softmax_resampled')\n",
        "      !rm $processed_nrrd_path_nnunet_segments_resampled/*.nrrd\n",
        "\n",
        "      elapsed_total_nnunet = time.time() - start_total_nnunet\n",
        "\n",
        "      print(\"End-to-end processing of %s completed in %g seconds.\\n\"%(series_id, elapsed_total_nnunet))\n",
        "\n",
        "      ### Inference/total time ### \n",
        "      # dict_to_add = dict()\n",
        "      # dict_to_add[series_id] = [num_instances, elapsed_inference_nnunet, elapsed_total_nnunet]\n",
        "      # save_time_and_num_instances_to_table(project_name, dataset_id, multiple_models_time_table_id_name_nnunet, dict_to_add)\n",
        "      ### Using INSERT ### \n",
        "      column = nnunet_model + \"-tta\" if use_tta == True else nnunet_model + \"-no_tta\"\n",
        "      column = column.replace('-', '_')\n",
        "      column_inference = 'inference_time_' + column\n",
        "      column_total = 'total_time_' + column\n",
        "      SeriesInstanceUID_column = \"SeriesInstanceUID\"\n",
        "      num_instances_column = \"num_instances\"\n",
        "      client = bigquery.Client(project=project_name)\n",
        "      query = f\"\"\"\n",
        "        INSERT INTO\n",
        "          {nnunet_table_id_fullname}({SeriesInstanceUID_column}, \n",
        "                                    {num_instances_column}, \n",
        "                                    {column_inference},\n",
        "                                    {column_total})\n",
        "        VALUES\n",
        "          (@SeriesInstanceUID_value, @num_instances_value, @inference_time_value, @total_time_value)\n",
        "          \"\"\"\n",
        "      job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ScalarQueryParameter(\"SeriesInstanceUID_value\", \"STRING\", series_id), \n",
        "                                                            bigquery.ScalarQueryParameter(\"num_instances_value\", \"INTEGER\", int(num_instances)), \n",
        "                                                            bigquery.ScalarQueryParameter(\"inference_time_value\", \"FLOAT\", elapsed_inference_nnunet), \n",
        "                                                            bigquery.ScalarQueryParameter(\"total_time_value\", \"FLOAT\", elapsed_total_nnunet)])\n",
        "      result = client.query(query, job_config=job_config)\n",
        "\n",
        "\n",
        "    ### only run BPR once ### \n",
        "\n",
        "    if (model_idx==0):\n",
        "\n",
        "      # -----------------\n",
        "      # BPR \n",
        "\n",
        "      start_total_bpr = time.time()\n",
        "\n",
        "      input_nifti_fn = series_id + \".nii.gz\"\n",
        "      input_nifti_path = os.path.join(model_input_folder_bpr, input_nifti_fn)\n",
        "\n",
        "      pred_json_fn = series_id + \".json\"\n",
        "      pred_json_path = os.path.join(model_output_folder_bpr, pred_json_fn)\n",
        "\n",
        "      # gs URI at which the *.json object is or will be stored in the bucket\n",
        "      gs_uri_json_file = os.path.join(bucket_json_folder_uri_bpr, pred_json_fn)\n",
        "      has_json_already = file_exists_in_bucket(project_name = project_name,\n",
        "                                              bucket_name = bucket_name,\n",
        "                                              file_gs_uri = gs_uri_json_file)\n",
        "\n",
        "      # Query to see if the series for the specific model has been run \n",
        "      field_name = 'inference_time_bpr' \n",
        "      value_found = check_value_exists_in_table(bpr_table_id_fullname, project_name, series_id, field_name)\n",
        "      print ('value_found inference_time_bpr: ' + str(value_found))\n",
        "\n",
        "      # -----------------\n",
        "      # DL-inference for BPR \n",
        "      # json not found and timing entry doesn't exist in table \n",
        "      # if not has_json_already or value_found == 0:\n",
        "      if has_json_already == False: \n",
        "\n",
        "        # If ct dicom directory does not exist, download the ct files \n",
        "        ct_dicom_path = os.path.join(sorted_base_path, series_id) \n",
        "        if not os.path.isdir(ct_dicom_path):\n",
        "          start_time_download_series_data = time.time()\n",
        "          download_series_data_s5cmd(raw_base_path = raw_base_path,\n",
        "                                    sorted_base_path = sorted_base_path,\n",
        "                                    series_df = series_df,\n",
        "                                    remove_raw = True)\n",
        "          elapsed_time_download_series_data = time.time()-start_time_download_series_data\n",
        "\n",
        "        # If ct nifti doesn't exist, create it \n",
        "        pat_dir_nifti_path = os.path.join(processed_nifti_path, series_id)\n",
        "        ct_nifti_path = os.path.join(pat_dir_nifti_path, series_id + \"_CT.nii.gz\")\n",
        "        if not os.path.exists(ct_nifti_path):\n",
        "          start_time_ct_to_nii = time.time()\n",
        "          pypla_dicom_ct_to_nifti(sorted_base_path = sorted_base_path,\n",
        "                                  processed_nifti_path = processed_nifti_path,\n",
        "                                  pat_id = series_id, \n",
        "                                  verbose = True)\n",
        "          elapsed_time_ct_to_nii = time.time()-start_time_ct_to_nii \n",
        "\n",
        "        # prepare the `model_input` folder for the inference phase\n",
        "        prep_input_data_bpr(processed_nifti_path = processed_nifti_path,\n",
        "                            model_input_folder = model_input_folder_bpr,\n",
        "                            pat_id = series_id)\n",
        "\n",
        "        # single patient needs a temporary folder, as bpr inference takes as input\n",
        "        # a directory\n",
        "\n",
        "        model_input_folder_series = os.path.join(model_input_folder_bpr,series_id+'.nii.gz')\n",
        "        !cp $model_input_folder_series $model_input_folder_tmp_bpr\n",
        "\n",
        "        start_inference_bpr = time.time()\n",
        "        process_patient_bpr(model_input_folder = model_input_folder_tmp_bpr,\n",
        "                            model_output_folder = model_output_folder_tmp_bpr,\n",
        "                            model = \"/content/models/bpr_model/public_bpr_model/\")\n",
        "        elapsed_inference_bpr = time.time() - start_inference_bpr\n",
        "        inference_time_dict_bpr[series_id] = elapsed_inference_bpr\n",
        "\n",
        "        # !cp $model_output_folder_tmp/* $model_output_folder\n",
        "        !cp $model_output_folder_tmp_bpr/* $model_output_folder_bpr\n",
        "\n",
        "        # copy the bpr *.json prediction in the chosen bucket \n",
        "        s_time = time.time()\n",
        "        # !gsutil -m cp $pred_json_path $gs_uri_json_file \n",
        "        !/content/s5cmd --endpoint-url https://storage.googleapis.com cp $pred_json_path $gs_uri_json_file \n",
        "\n",
        "        e_time = time.time()\n",
        "        print ('Copy json file to bucket: ' + str(e_time-s_time))\n",
        "\n",
        "        # -----------------\n",
        "        # remove files from BPR \n",
        "\n",
        "        # remove the file from the model_input_tmp folder\n",
        "        model_input_folder_tmp_series = os.path.join(model_input_folder_tmp_bpr,series_id+'.nii.gz')\n",
        "        !rm $model_input_folder_tmp_series\n",
        "\n",
        "        # remove the file from the model_output_tmp folder \n",
        "        model_output_folder_tmp_series = os.path.join(model_output_folder_tmp_bpr,series_id+'.json')\n",
        "        !rm $model_output_folder_tmp_series\n",
        "\n",
        "        # remove the NIfTI file the prediction was computed from\n",
        "        !rm $input_nifti_path\n",
        "\n",
        "        # Remove the raw patient data \n",
        "        # sorted_base_path_remove = os.path.join(sorted_base_path,series_id)\n",
        "        # !rm -r $sorted_base_path_remove \n",
        "\n",
        "        # Delete json from bpr_output folder \n",
        "        model_output_folder_series = os.path.join(model_output_folder_bpr,series_id+'.json')\n",
        "        !rm $model_output_folder_series\n",
        "\n",
        "        # Delete the files in /content/data/processed/bpr/nii\n",
        "        # !rm -r $processed_nifti_path/* ## this was for nnUNet \n",
        "        !rm -r /content/data/processed/bpr/nii\n",
        "        \n",
        "        # should be in the if statement for if it has a json file. \n",
        "        # because only then do you want to add the value to the table.\n",
        "        elapsed_total_bpr = time.time() - start_total_bpr\n",
        "        total_time_dict_bpr[series_id] = elapsed_total_bpr\n",
        "        print(\"End-to-end processing of %s completed in %g seconds.\\n\"%(series_id, elapsed_total_bpr))\n",
        "\n",
        "        # -----------------\n",
        "        # save inference time/total time information \n",
        "\n",
        "        # elapsed_total_bpr = (time.time() - start_total_bpr) + elapsed_time_download_series_data + elapsed_time_ct_to_nii\n",
        "        \n",
        "\n",
        "        # ### Inference time ### \n",
        "        # dict_to_add = dict()\n",
        "        # dict_to_add[series_id] = [elapsed_inference_bpr, num_instances]\n",
        "        # save_time_and_num_instances_to_table(project_name, dataset_id, inference_time_table_id_name_bpr, dict_to_add)\n",
        "        # ### Total time ###\n",
        "        # dict_to_add = dict()\n",
        "        # dict_to_add[series_id] = [elapsed_total_bpr, num_instances]\n",
        "        # save_time_and_num_instances_to_table(project_name, dataset_id, total_time_table_id_name_bpr, dict_to_add)\n",
        "\n",
        "        # dict_to_add = dict()\n",
        "        # dict_to_add[series_id] = [num_instances, elapsed_inference_bpr, elapsed_total_bpr]\n",
        "        # add_to_df = pd.DataFrame.from_dict(data = dict_to_add, orient = \"index\")\n",
        "        # add_to_df = add_to_df.reset_index()\n",
        "        # add_to_df = add_to_df.rename(columns = {\"index\" : 'SeriesInstanceUID', \n",
        "        #                                         0 : 'num_instances', \n",
        "        #                                         1 : 'inference_time_bpr', \n",
        "        #                                         2 : 'total_time_bpr'})\n",
        "        # client = bigquery.Client(project=project_name)\n",
        "        # job_config = bigquery.LoadJobConfig()\n",
        "        # job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
        "        # job = client.load_table_from_dataframe(add_to_df, bpr_table_id_fullname, job_config=job_config)\n",
        "\n",
        "        ### Using INSERT ### \n",
        "        column_inference = 'inference_time_bpr'\n",
        "        column_total = 'total_time_bpr'\n",
        "        SeriesInstanceUID_column = \"SeriesInstanceUID\"\n",
        "        num_instances_column = \"num_instances\"\n",
        "        client = bigquery.Client(project=project_name)\n",
        "        query = f\"\"\"\n",
        "          INSERT INTO\n",
        "            {bpr_table_id_fullname}({SeriesInstanceUID_column}, \n",
        "                                    {num_instances_column}, \n",
        "                                    {column_inference},\n",
        "                                    {column_total})\n",
        "          VALUES\n",
        "            (@SeriesInstanceUID_value, @num_instances_value, @inference_time_value, @total_time_value)\n",
        "            \"\"\"\n",
        "        job_config = bigquery.QueryJobConfig(query_parameters=[bigquery.ScalarQueryParameter(\"SeriesInstanceUID_value\", \"STRING\", series_id), \n",
        "                                                              bigquery.ScalarQueryParameter(\"num_instances_value\", \"INTEGER\", int(num_instances)), \n",
        "                                                              bigquery.ScalarQueryParameter(\"inference_time_value\", \"FLOAT\", elapsed_inference_bpr), \n",
        "                                                              bigquery.ScalarQueryParameter(\"total_time_value\", \"FLOAT\", elapsed_total_bpr)])\n",
        "        result = client.query(query, job_config=job_config)\n",
        "\n",
        "\n",
        "  # Remove some files/folders to get ready for the next series run\n",
        "  print ('*****deleting files for the next series run ******')\n",
        "  if os.listdir('/content/data/raw/nlst/dicom'):\n",
        "    !rm -r /content/data/raw/nlst/dicom/*  \n",
        "  if os.listdir('/content/data/processed/nlst/nii'):\n",
        "    !rm -r /content/data/processed/nlst/nii/*\n",
        "  # remove from model input \n",
        "  !rm /content/data/nnunet/model_input/*\n",
        "\n",
        "  # Don't need to remove this -- alreadty removed above by '/content/data/raw/nlst/dicom\n",
        "  # Remove the raw series data to get ready for the next series run \n",
        "  # sorted_base_path_remove = os.path.join(sorted_base_path,series_id)\n",
        "  # !rm -r $sorted_base_path_remove \n",
        "\n",
        "  # Remove the series data from /content/data/processed/nlst/nnunet/nrrd/\n",
        "  !rm -r /content/data/processed/nlst/nnunet/nrrd/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAmWN39zv1Yu"
      },
      "source": [
        "# GCP Healthcare steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCVAL4bWv1Yu"
      },
      "source": [
        "Save the CT dicom files and SEG files to a DICOM store. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIyZTVKvv1Yu"
      },
      "source": [
        "Create the dataset if it doesn't already exist. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "-X3q8z-6v1Yu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944a388e-9a7a-4d40-99be-dfeafbafd57b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets that exist for project_id idc-external-018, location us-central1: ['SlicerDICOMWeb_test', 'bpr_dataset', 'dataset_nlst', 'mpreview_dataset', 'mpreview_dataset_ncigt', 'mpreview_dataset_prostatex', 'project-week', 'project-week-dataset', '', '', 'To take a quick anonymous survey, run:', '  $ gcloud survey', '']\n",
            "datasets that exist for project_id idc-external-018, location us-central1: ['SlicerDICOMWeb_test', 'bpr_dataset', 'dataset_nlst', 'mpreview_dataset', 'mpreview_dataset_ncigt', 'mpreview_dataset_prostatex', 'project-week', 'project-week-dataset']\n"
          ]
        }
      ],
      "source": [
        "# First let's list the datasets that we already have for our particular project_id and location_id\n",
        "datasets = !gcloud healthcare datasets list --project $project_name --location $location_id --format=\"value(ID)\" \n",
        "print ('datasets that exist for project_id ' + str(project_name) + ', location ' + str(location_id) + ': ' + str(datasets))\n",
        " \n",
        "# If the dataset doesn't exist, create it \n",
        "if not (dataset_id in datasets):\n",
        "  try:\n",
        "    !gcloud healthcare datasets create --project $project_name $dataset_id --location $location_id\n",
        "  except OSError as err:\n",
        "    print(\"Error: %s : %s\" % (\"Unable to create dataset\", err.strerror)) \n",
        "\n",
        "# As a check, we'll list the datasets again.\n",
        "datasets = !gcloud healthcare datasets list --project $project_name --location $location_id --format=\"value(ID)\" \n",
        "print ('datasets that exist for project_id ' + str(project_name) + ', location ' + str(location_id) + ': ' + str(datasets))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7cDtQEwv1Yu"
      },
      "source": [
        "Create a dicom store if it doesn't already exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "A9baFv4iv1Yu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9473398-4804-4e1a-c4aa-53ece78162c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datastores that exist for project_id idc-external-018, location us-central1, dataset dataset_nlst: ['datastore_nlst_25_series_06_15_22']\n",
            "Created dicomStore [datastore_nlst_25_series_06_16_22].\n",
            "\n",
            "datastores that exist for project_id idc-external-018, location us-central1, dataset dataset_nlst: ['datastore_nlst_25_series_06_15_22', 'datastore_nlst_25_series_06_16_22']\n"
          ]
        }
      ],
      "source": [
        "# First list the datastores that exist in the dataset\n",
        "datastores = !gcloud healthcare dicom-stores list --project $project_name --dataset $dataset_id --format=\"value(ID)\"\n",
        "print ('datastores that exist for project_id ' + str(project_name) + ', location ' + str(location_id) + ', dataset ' + str(dataset_id) + ': ' + str(datastores))\n",
        "\n",
        "# If the dicom_store_id doesn't exist, create it\n",
        "if not (datastore_id in datastores):\n",
        "  try:\n",
        "    !gcloud healthcare dicom-stores create $datastore_id --project $project_name --dataset $dataset_id --format=\"value(ID)\" \n",
        "  except OSError as err:\n",
        "    print(\"Error: %s : %s\" % (\"Unable to create datastore\", err.strerror)) \n",
        "\n",
        "# As a check, we'll list the datastores again.\n",
        "datastores = !gcloud healthcare dicom-stores list --project $project_name --dataset $dataset_id --format=\"value(ID)\"\n",
        "print ('datastores that exist for project_id ' + str(project_name) + ', location ' + str(location_id) + ', dataset ' + str(dataset_id) + ': ' + str(datastores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z77xqyuXv1Yv"
      },
      "source": [
        "Save the SEG files to the dicom store for each model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_output_base_uri_nnunet = os.path.join(bucket_base_uri, \"nnunet\", \"nnunet_output\", nlst_sub) # need to run the Set the run parameters. "
      ],
      "metadata": {
        "id": "-PYmEsGHhyPT"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "GWTOOzhcv1Yv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d59f2e21-a9b4-42e9-8eab-0ffee07df25b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/2d-tta/dicomseg\n",
            "Request issued for: [datastore_nlst_25_series_06_16_22]\n",
            "name: projects/idc-external-018/locations/us-central1/datasets/dataset_nlst/dicomStores/datastore_nlst_25_series_06_16_22\n",
            "s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/dicomseg\n",
            "Request issued for: [datastore_nlst_25_series_06_16_22]\n",
            "name: projects/idc-external-018/locations/us-central1/datasets/dataset_nlst/dicomStores/datastore_nlst_25_series_06_16_22\n"
          ]
        }
      ],
      "source": [
        "# Save SEG files to the dicom-store\n",
        "\n",
        "for model_idx, nnunet_model in enumerate(model_list):\n",
        "\n",
        "  experiment_folder_name = nnunet_model + \"-tta\" if use_tta == True else nnunet_model + \"-no_tta\"\n",
        "  bucket_experiment_folder_uri_nnunet = os.path.join(bucket_output_base_uri_nnunet, experiment_folder_name)\n",
        "  bucket_dicomseg_folder_uri_nnunet = os.path.join(bucket_experiment_folder_uri_nnunet, 'dicomseg')\n",
        "\n",
        "  print (bucket_dicomseg_folder_uri_nnunet)\n",
        "  # my_bucket = \"idc-medima-paper-dk/nnunet/nnunet_output/nlst/2d-tta/dicomseg\"\n",
        "  # my_bucket = gs_uri_dicomseg_file.split('s3://')[1] \n",
        "  my_bucket = bucket_dicomseg_folder_uri_nnunet.split('s3://')[1]\n",
        "  !gcloud healthcare dicom-stores import gcs $datastore_id \\\n",
        "                                              --dataset=$dataset_id \\\n",
        "                                              --location=$location_id \\\n",
        "                                              --project=$project_name \\\n",
        "                                              --gcs-uri=gs://$my_bucket/**.dcm\n",
        "                                            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA4EnBWzv1Yv"
      },
      "source": [
        "For each series, download the data and then copy to the bucket. This takes about 2 seconds per series"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of series_id \n",
        "\n",
        "### Get the cohort_df of the subset of series ids that we want ### \n",
        "\n",
        "# First query to get the list of series that we want \n",
        "table_view_id_name_full = '.'.join([project_name, dataset_id, table_view_id_name])\n",
        "client = bigquery.Client(project=project_name)\n",
        "query_view = f\"\"\"\n",
        "  SELECT \n",
        "    DISTINCT(SeriesInstanceUID)\n",
        "  FROM\n",
        "    {table_view_id_name_full}\n",
        "  LIMIT 25 OFFSET @patient_offset;\n",
        "  \"\"\"\n",
        "\n",
        "start_time = time.time()\n",
        "job_config = bigquery.QueryJobConfig(query_parameters=[\n",
        "                                                       bigquery.ScalarQueryParameter(\"patient_offset\", \"INTEGER\", int(patient_offset))\n",
        "                                                       ])\n",
        "result = client.query(query_view, job_config=job_config) \n",
        "series_df = result.to_dataframe(create_bqstorage_client=True)\n",
        "end_time = time.time()\n",
        "print ('elapsed time: ' + str(end_time-start_time)) # 2.7 seconds. \n",
        "series_id_list = list(series_df['SeriesInstanceUID'].values)\n",
        "print ('num of unique series should be 25 -- the number of unique series is: ' + str(len(set(series_id_list))))\n",
        "\n",
        "# series_id_list = series_id_list[0:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW90viiLgvft",
        "outputId": "16890a68-747e-4ed4-c6d2-6ddb154c5926"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elapsed time: 4.546273946762085\n",
            "num of unique series should be 25 -- the number of unique series is: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Then get the subset cohort df of those series \n",
        "client = bigquery.Client(project=project_name)\n",
        "query_view = f\"\"\"\n",
        "  SELECT \n",
        "    *\n",
        "  FROM\n",
        "    {table_view_id_name_full}\n",
        "  WHERE\n",
        "    SeriesInstanceUID IN UNNEST(@series_id_list);\n",
        "  \"\"\"\n",
        "\n",
        "start_time = time.time()\n",
        "job_config = bigquery.QueryJobConfig(query_parameters=[\n",
        "                                                       bigquery.ArrayQueryParameter(\"series_id_list\", \"STRING\", series_id_list)\n",
        "                                                       ])\n",
        "result = client.query(query_view, job_config=job_config) \n",
        "cohort_df = result.to_dataframe(create_bqstorage_client=True)\n",
        "end_time = time.time()\n",
        "print ('elapsed time: ' + str(end_time-start_time)) #  8.6 seconds.\n",
        "cohort_df "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "uTFNAgmohdiI",
        "outputId": "11b2d493-c937-4fa1-cde5-59d046746138"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elapsed time: 6.800174713134766\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     SeriesInstanceUID  num_instances  \\\n",
              "0    1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...            153   \n",
              "1    1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...            153   \n",
              "2    1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...            153   \n",
              "3    1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...            153   \n",
              "4    1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...            153   \n",
              "..                                                 ...            ...   \n",
              "148  1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...            153   \n",
              "149  1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...            153   \n",
              "150  1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...            153   \n",
              "151  1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...            153   \n",
              "152  1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...            153   \n",
              "\n",
              "                                               idc_url  \\\n",
              "0    https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "1    https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "2    https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "3    https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "4    https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "..                                                 ...   \n",
              "148  https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "149  https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "150  https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "151  https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "152  https://viewer.imaging.datacommons.cancer.gov/...   \n",
              "\n",
              "                                               gcs_url  \n",
              "0    gs://public-datasets-idc/58c8fb4d-d7b9-4cb5-b1...  \n",
              "1    gs://public-datasets-idc/c5f7970d-737b-49ca-b5...  \n",
              "2    gs://public-datasets-idc/edc53df8-2330-4488-a6...  \n",
              "3    gs://public-datasets-idc/a1e63e15-7fa1-4c60-a3...  \n",
              "4    gs://public-datasets-idc/e0d7a2e1-3ec3-450b-a3...  \n",
              "..                                                 ...  \n",
              "148  gs://public-datasets-idc/0cc8bc4a-d145-49f5-a5...  \n",
              "149  gs://public-datasets-idc/b94d7b29-6c52-4321-af...  \n",
              "150  gs://public-datasets-idc/62a16f1d-c561-4050-b4...  \n",
              "151  gs://public-datasets-idc/810944bd-5565-4230-ab...  \n",
              "152  gs://public-datasets-idc/a3896b2c-f15b-4924-85...  \n",
              "\n",
              "[153 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4e34cb0e-ae77-4228-904a-d4ab5b9e1371\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SeriesInstanceUID</th>\n",
              "      <th>num_instances</th>\n",
              "      <th>idc_url</th>\n",
              "      <th>gcs_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...</td>\n",
              "      <td>153</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/58c8fb4d-d7b9-4cb5-b1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...</td>\n",
              "      <td>153</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/c5f7970d-737b-49ca-b5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...</td>\n",
              "      <td>153</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/edc53df8-2330-4488-a6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...</td>\n",
              "      <td>153</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/a1e63e15-7fa1-4c60-a3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...</td>\n",
              "      <td>153</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/e0d7a2e1-3ec3-450b-a3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...</td>\n",
              "      <td>153</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/0cc8bc4a-d145-49f5-a5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...</td>\n",
              "      <td>153</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/b94d7b29-6c52-4321-af...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...</td>\n",
              "      <td>153</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/62a16f1d-c561-4050-b4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...</td>\n",
              "      <td>153</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/810944bd-5565-4230-ab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>1.3.6.1.4.1.14519.5.2.1.7009.9004.132120322633...</td>\n",
              "      <td>153</td>\n",
              "      <td>https://viewer.imaging.datacommons.cancer.gov/...</td>\n",
              "      <td>gs://public-datasets-idc/a3896b2c-f15b-4924-85...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>153 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e34cb0e-ae77-4228-904a-d4ab5b9e1371')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4e34cb0e-ae77-4228-904a-d4ab5b9e1371 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4e34cb0e-ae77-4228-904a-d4ab5b9e1371');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "y_HMqxCKv1Yv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f0256f-27f6-4525-c89a-5eb4243b435e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0\n",
            "Done download in 2.09328 seconds.\n",
            "cp /content/data/raw/tmp/25af8faf-8ff5-43df-9bd6-c4ce82411220.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/25af8faf-8ff5-43df-9bd6-c4ce82411220.dcm\n",
            "cp /content/data/raw/tmp/c5a8b520-6bc1-492e-a08f-59ae08c3599e.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/c5a8b520-6bc1-492e-a08f-59ae08c3599e.dcm\n",
            "cp /content/data/raw/tmp/3d551f9b-03c2-415a-a30f-2ee7f3c4af77.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/3d551f9b-03c2-415a-a30f-2ee7f3c4af77.dcm\n",
            "cp /content/data/raw/tmp/b2a8c746-2423-4be7-b690-729fce5b8de3.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/b2a8c746-2423-4be7-b690-729fce5b8de3.dcm\n",
            "cp /content/data/raw/tmp/b0ba2014-541a-4b4a-bd14-9f819ad36296.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/b0ba2014-541a-4b4a-bd14-9f819ad36296.dcm\n",
            "cp /content/data/raw/tmp/b94d7b29-6c52-4321-afeb-a1c4e966d167.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/b94d7b29-6c52-4321-afeb-a1c4e966d167.dcm\n",
            "cp /content/data/raw/tmp/bbc8025d-2b93-4a31-9f65-d32533d97ff0.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/bbc8025d-2b93-4a31-9f65-d32533d97ff0.dcm\n",
            "cp /content/data/raw/tmp/fa767cfe-6644-4c85-91f3-2775ef2cdda4.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/fa767cfe-6644-4c85-91f3-2775ef2cdda4.dcm\n",
            "cp /content/data/raw/tmp/a6582b9a-aef1-47c6-913e-169207dd408d.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/a6582b9a-aef1-47c6-913e-169207dd408d.dcm\n",
            "cp /content/data/raw/tmp/21c6e0fc-7412-4438-aab5-c191b16de27f.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/21c6e0fc-7412-4438-aab5-c191b16de27f.dcm\n",
            "cp /content/data/raw/tmp/c79f3f95-f167-4e80-9b69-f6a9b363f3b2.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/c79f3f95-f167-4e80-9b69-f6a9b363f3b2.dcm\n",
            "cp /content/data/raw/tmp/ce543af6-9fa4-4d7e-89dd-6981313aae83.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/ce543af6-9fa4-4d7e-89dd-6981313aae83.dcm\n",
            "cp /content/data/raw/tmp/a9feac00-0cd6-4c1a-b9c3-e61f976c3d9e.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/a9feac00-0cd6-4c1a-b9c3-e61f976c3d9e.dcm\n",
            "cp /content/data/raw/tmp/c089335d-7a2b-4217-b677-4d6b3f562ca7.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/c089335d-7a2b-4217-b677-4d6b3f562ca7.dcm\n",
            "cp /content/data/raw/tmp/ace0a697-2544-4d3e-822e-7f49ad1ca493.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/ace0a697-2544-4d3e-822e-7f49ad1ca493.dcm\n",
            "cp /content/data/raw/tmp/b70ef308-9d69-47a2-a48e-b48db5e0ff6d.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/b70ef308-9d69-47a2-a48e-b48db5e0ff6d.dcm\n",
            "cp /content/data/raw/tmp/b1b4ec74-e065-446e-a183-63d92a05ec64.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/b1b4ec74-e065-446e-a183-63d92a05ec64.dcm\n",
            "cp /content/data/raw/tmp/b22f9bbd-f790-4373-8bf4-05cd7eb9eea0.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/b22f9bbd-f790-4373-8bf4-05cd7eb9eea0.dcm\n",
            "cp /content/data/raw/tmp/814d9d1e-251f-4990-954d-40b9f48750c4.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/814d9d1e-251f-4990-954d-40b9f48750c4.dcm\n",
            "cp /content/data/raw/tmp/beb98fa3-894c-481d-9f94-5012186fed2a.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/beb98fa3-894c-481d-9f94-5012186fed2a.dcm\n",
            "cp /content/data/raw/tmp/d180ee32-094c-4e03-afa0-51532a909ecf.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/d180ee32-094c-4e03-afa0-51532a909ecf.dcm\n",
            "cp /content/data/raw/tmp/ad669685-26f4-4bda-b5de-77d13e30b60e.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/ad669685-26f4-4bda-b5de-77d13e30b60e.dcm\n",
            "cp /content/data/raw/tmp/f0a058ae-e882-4f6d-aef2-d97190242898.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/f0a058ae-e882-4f6d-aef2-d97190242898.dcm\n",
            "cp /content/data/raw/tmp/e86eecfa-d1a2-4bd5-8a22-8598e40b82ed.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/e86eecfa-d1a2-4bd5-8a22-8598e40b82ed.dcm\n",
            "cp /content/data/raw/tmp/edc53df8-2330-4488-a6f5-25ca633eefd7.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/edc53df8-2330-4488-a6f5-25ca633eefd7.dcm\n",
            "cp /content/data/raw/tmp/c77d02d4-bd57-4c28-9a88-74ce966b0773.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/c77d02d4-bd57-4c28-9a88-74ce966b0773.dcm\n",
            "cp /content/data/raw/tmp/ce0518e9-87f7-4669-b352-366563f8b4df.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/ce0518e9-87f7-4669-b352-366563f8b4df.dcm\n",
            "cp /content/data/raw/tmp/c16b1be1-d62c-4b67-9c27-8cf665f3f9ec.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/c16b1be1-d62c-4b67-9c27-8cf665f3f9ec.dcm\n",
            "cp /content/data/raw/tmp/c219f72c-2877-4933-a987-2c5103a5091a.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/c219f72c-2877-4933-a987-2c5103a5091a.dcm\n",
            "cp /content/data/raw/tmp/f004ff3d-ddba-4b2f-8411-ed9ea81f4cb7.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/f004ff3d-ddba-4b2f-8411-ed9ea81f4cb7.dcm\n",
            "cp /content/data/raw/tmp/dbbcf0d8-64e4-488d-9d40-8691c9e42993.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/dbbcf0d8-64e4-488d-9d40-8691c9e42993.dcm\n",
            "cp /content/data/raw/tmp/c1e0a600-8edc-404e-814c-4051fc214c10.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/c1e0a600-8edc-404e-814c-4051fc214c10.dcm\n",
            "cp /content/data/raw/tmp/d6f70f8c-8d84-4f13-88e4-43eb32f4a98f.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/d6f70f8c-8d84-4f13-88e4-43eb32f4a98f.dcm\n",
            "cp /content/data/raw/tmp/c9c049cd-d106-4867-bfe7-f98a71c979d9.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/c9c049cd-d106-4867-bfe7-f98a71c979d9.dcm\n",
            "cp /content/data/raw/tmp/b112e62c-fb16-4d6a-8dc4-5f26ad3ddc55.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/b112e62c-fb16-4d6a-8dc4-5f26ad3ddc55.dcm\n",
            "cp /content/data/raw/tmp/e357ca26-8ea4-402b-ab66-de52d0d2bbbb.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/e357ca26-8ea4-402b-ab66-de52d0d2bbbb.dcm\n",
            "cp /content/data/raw/tmp/f8b56be3-ad00-44c9-86e4-1f5b242837c5.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/f8b56be3-ad00-44c9-86e4-1f5b242837c5.dcm\n",
            "cp /content/data/raw/tmp/74f7f106-713c-4609-8fbd-d5215e7e6a39.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/74f7f106-713c-4609-8fbd-d5215e7e6a39.dcm\n",
            "cp /content/data/raw/tmp/3262dc68-2437-4424-915f-7fb672d5b41a.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/3262dc68-2437-4424-915f-7fb672d5b41a.dcm\n",
            "cp /content/data/raw/tmp/ae787d8c-70f3-4d01-9ee3-8b37fe5ae6e5.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/ae787d8c-70f3-4d01-9ee3-8b37fe5ae6e5.dcm\n",
            "cp /content/data/raw/tmp/3e1d76f1-95b7-4d13-b19e-1038476bf513.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/3e1d76f1-95b7-4d13-b19e-1038476bf513.dcm\n",
            "cp /content/data/raw/tmp/1d873e45-9c2d-44ce-b596-5ccd36ecac38.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/1d873e45-9c2d-44ce-b596-5ccd36ecac38.dcm\n",
            "cp /content/data/raw/tmp/cf280951-b35b-4c65-afd6-de9070efa15e.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/cf280951-b35b-4c65-afd6-de9070efa15e.dcm\n",
            "cp /content/data/raw/tmp/a242c7b4-ed61-40bd-93dd-a25f514b1c7e.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/a242c7b4-ed61-40bd-93dd-a25f514b1c7e.dcm\n",
            "cp /content/data/raw/tmp/d9168353-86b1-4ed0-9a32-3da281d5bb64.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/d9168353-86b1-4ed0-9a32-3da281d5bb64.dcm\n",
            "cp /content/data/raw/tmp/f02cbce0-9fc5-414d-aa39-968bde39c2a2.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/f02cbce0-9fc5-414d-aa39-968bde39c2a2.dcm\n",
            "cp /content/data/raw/tmp/58c8fb4d-d7b9-4cb5-b12a-2cb67906ac4e.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/58c8fb4d-d7b9-4cb5-b12a-2cb67906ac4e.dcm\n",
            "cp /content/data/raw/tmp/c7e6d88c-9efe-4cb1-a401-005957938d7d.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/c7e6d88c-9efe-4cb1-a401-005957938d7d.dcm\n",
            "cp /content/data/raw/tmp/179c9e6c-b6d2-4320-bca6-2721e6246194.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/179c9e6c-b6d2-4320-bca6-2721e6246194.dcm\n",
            "cp /content/data/raw/tmp/e3b76ba0-2c95-448d-9b08-e689c77c023a.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/e3b76ba0-2c95-448d-9b08-e689c77c023a.dcm\n",
            "cp /content/data/raw/tmp/dba55c88-4cb6-4832-9b5a-080c6ede18e9.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/dba55c88-4cb6-4832-9b5a-080c6ede18e9.dcm\n",
            "cp /content/data/raw/tmp/44163341-5b4b-4159-b6f2-6df559ed9037.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/44163341-5b4b-4159-b6f2-6df559ed9037.dcm\n",
            "cp /content/data/raw/tmp/f6581dce-015d-4dd8-a442-0f7c3de71a24.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/f6581dce-015d-4dd8-a442-0f7c3de71a24.dcm\n",
            "cp /content/data/raw/tmp/c5f7970d-737b-49ca-b5d4-ba875643135f.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/c5f7970d-737b-49ca-b5d4-ba875643135f.dcm\n",
            "cp /content/data/raw/tmp/a3a58231-60b9-44fa-ae4f-f7aaeab58506.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/a3a58231-60b9-44fa-ae4f-f7aaeab58506.dcm\n",
            "cp /content/data/raw/tmp/e7cddc21-4d4a-45f7-ae14-302f191d0595.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/e7cddc21-4d4a-45f7-ae14-302f191d0595.dcm\n",
            "cp /content/data/raw/tmp/d1cddbd1-27af-41d8-832c-021f67f42e2b.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/d1cddbd1-27af-41d8-832c-021f67f42e2b.dcm\n",
            "cp /content/data/raw/tmp/caa8cd32-b813-4a3f-970e-61285874838b.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/caa8cd32-b813-4a3f-970e-61285874838b.dcm\n",
            "cp /content/data/raw/tmp/6f0c455a-7a0c-4c00-a113-42b8fc54e602.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/6f0c455a-7a0c-4c00-a113-42b8fc54e602.dcm\n",
            "cp /content/data/raw/tmp/08a20d94-fb54-452d-807a-262902fab485.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/08a20d94-fb54-452d-807a-262902fab485.dcm\n",
            "cp /content/data/raw/tmp/3824022c-897a-4c50-87e2-9f0bec9ca70a.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/3824022c-897a-4c50-87e2-9f0bec9ca70a.dcm\n",
            "cp /content/data/raw/tmp/c9e421b5-631f-43cf-8d47-d577c27d1f51.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/c9e421b5-631f-43cf-8d47-d577c27d1f51.dcm\n",
            "cp /content/data/raw/tmp/05823820-05f2-43d2-add3-27a353a10ff9.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/05823820-05f2-43d2-add3-27a353a10ff9.dcm\n",
            "cp /content/data/raw/tmp/1da196ee-3edd-45fc-9b26-b22ad98fb93f.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/1da196ee-3edd-45fc-9b26-b22ad98fb93f.dcm\n",
            "cp /content/data/raw/tmp/062df572-8ac3-4150-87a7-d5fe2c32761d.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/062df572-8ac3-4150-87a7-d5fe2c32761d.dcm\n",
            "cp /content/data/raw/tmp/671d2c42-e55a-4b61-aa43-53af1978824a.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/671d2c42-e55a-4b61-aa43-53af1978824a.dcm\n",
            "cp /content/data/raw/tmp/3e71a64d-075b-467b-8fac-e4721d8a1500.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/3e71a64d-075b-467b-8fac-e4721d8a1500.dcm\n",
            "cp /content/data/raw/tmp/3df01a32-140b-44c7-bfaa-db409caf89b4.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/3df01a32-140b-44c7-bfaa-db409caf89b4.dcm\n",
            "cp /content/data/raw/tmp/49be99a5-f212-49f6-abf3-533ba9545bbe.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/49be99a5-f212-49f6-abf3-533ba9545bbe.dcm\n",
            "cp /content/data/raw/tmp/c3602b12-e52e-4a4b-8cfc-da49472c9ecd.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/c3602b12-e52e-4a4b-8cfc-da49472c9ecd.dcm\n",
            "cp /content/data/raw/tmp/763e0517-45c4-4c5e-9b8d-c05969be4082.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/763e0517-45c4-4c5e-9b8d-c05969be4082.dcm\n",
            "cp /content/data/raw/tmp/fd65b18d-7ce8-4637-b886-c3625ba080e3.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/fd65b18d-7ce8-4637-b886-c3625ba080e3.dcm\n",
            "cp /content/data/raw/tmp/c31c2ee6-1a32-4902-963f-b1c5477ca686.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/c31c2ee6-1a32-4902-963f-b1c5477ca686.dcm\n",
            "cp /content/data/raw/tmp/e52bff54-e7de-47ce-bdb6-c5c0332b49f3.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/e52bff54-e7de-47ce-bdb6-c5c0332b49f3.dcm\n",
            "cp /content/data/raw/tmp/e0d7a2e1-3ec3-450b-a3eb-1df8fc026523.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/e0d7a2e1-3ec3-450b-a3eb-1df8fc026523.dcm\n",
            "cp /content/data/raw/tmp/071f2547-e94f-4073-be2f-cb2a6776445e.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/071f2547-e94f-4073-be2f-cb2a6776445e.dcm\n",
            "cp /content/data/raw/tmp/eceb928f-b05c-48ba-b401-cf9d4c8aff42.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/eceb928f-b05c-48ba-b401-cf9d4c8aff42.dcm\n",
            "cp /content/data/raw/tmp/d481e047-598a-4761-8bca-980d8958efb5.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/d481e047-598a-4761-8bca-980d8958efb5.dcm\n",
            "cp /content/data/raw/tmp/92475663-7f9a-4898-90da-9ccc301bf964.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/92475663-7f9a-4898-90da-9ccc301bf964.dcm\n",
            "cp /content/data/raw/tmp/62a16f1d-c561-4050-b438-1a20eff40daa.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/62a16f1d-c561-4050-b438-1a20eff40daa.dcm\n",
            "cp /content/data/raw/tmp/810944bd-5565-4230-abd0-1673dff24889.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/810944bd-5565-4230-abd0-1673dff24889.dcm\n",
            "cp /content/data/raw/tmp/7ca288dc-41ca-4507-a1f6-5c0ce4b9cbce.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/7ca288dc-41ca-4507-a1f6-5c0ce4b9cbce.dcm\n",
            "cp /content/data/raw/tmp/722c64ad-f706-4d35-81b4-550959209f40.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/722c64ad-f706-4d35-81b4-550959209f40.dcm\n",
            "cp /content/data/raw/tmp/f8a9e046-15f7-41c9-b638-13d758461572.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/f8a9e046-15f7-41c9-b638-13d758461572.dcm\n",
            "cp /content/data/raw/tmp/84b653f7-cf95-48dc-8db7-0abc3770daa9.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/84b653f7-cf95-48dc-8db7-0abc3770daa9.dcm\n",
            "cp /content/data/raw/tmp/9bfc583c-93c9-4a99-8d7a-cae3254636a0.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/9bfc583c-93c9-4a99-8d7a-cae3254636a0.dcm\n",
            "cp /content/data/raw/tmp/13f88d93-3fef-4a36-842d-21b208d93e2f.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/13f88d93-3fef-4a36-842d-21b208d93e2f.dcm\n",
            "cp /content/data/raw/tmp/b72e119c-7e28-4373-9b61-6cb1a567e60e.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/b72e119c-7e28-4373-9b61-6cb1a567e60e.dcm\n",
            "cp /content/data/raw/tmp/a1e63e15-7fa1-4c60-a315-7880f66f28d3.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/a1e63e15-7fa1-4c60-a315-7880f66f28d3.dcm\n",
            "cp /content/data/raw/tmp/2584a735-23b0-4998-89fb-bf404fb88307.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/2584a735-23b0-4998-89fb-bf404fb88307.dcm\n",
            "cp /content/data/raw/tmp/9ba253ef-0f29-4e8b-b8bf-c6aa824542ff.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/9ba253ef-0f29-4e8b-b8bf-c6aa824542ff.dcm\n",
            "cp /content/data/raw/tmp/395f93b3-9c53-4874-9b4c-835971872775.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/395f93b3-9c53-4874-9b4c-835971872775.dcm\n",
            "cp /content/data/raw/tmp/93ffcd74-c528-4601-a7d9-54b01adeb8dc.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/93ffcd74-c528-4601-a7d9-54b01adeb8dc.dcm\n",
            "cp /content/data/raw/tmp/33522ff8-2c92-4538-8432-dcdc567c9de1.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/33522ff8-2c92-4538-8432-dcdc567c9de1.dcm\n",
            "cp /content/data/raw/tmp/c167a017-395c-452d-aa0f-c235ce860f17.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/c167a017-395c-452d-aa0f-c235ce860f17.dcm\n",
            "cp /content/data/raw/tmp/9bd6027b-a6f1-4ba1-9e74-fe194b507432.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/9bd6027b-a6f1-4ba1-9e74-fe194b507432.dcm\n",
            "cp /content/data/raw/tmp/aca7f9bc-a0ae-4d3a-9be5-b1019ee4e4f8.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/aca7f9bc-a0ae-4d3a-9be5-b1019ee4e4f8.dcm\n",
            "cp /content/data/raw/tmp/5c4fdfdb-ba1c-4834-9b3c-8bcbe5843ed8.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/5c4fdfdb-ba1c-4834-9b3c-8bcbe5843ed8.dcm\n",
            "cp /content/data/raw/tmp/319f57c8-1ccc-4240-865d-f95441533f3c.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/319f57c8-1ccc-4240-865d-f95441533f3c.dcm\n",
            "cp /content/data/raw/tmp/78ee2463-bdc5-4606-a210-65d15cad1245.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/78ee2463-bdc5-4606-a210-65d15cad1245.dcm\n",
            "cp /content/data/raw/tmp/330253f3-5f5b-4657-b21d-aa0e1b27fbfb.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/330253f3-5f5b-4657-b21d-aa0e1b27fbfb.dcm\n",
            "cp /content/data/raw/tmp/75eab54c-576c-4206-87a9-bdc1ea4bfef4.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/75eab54c-576c-4206-87a9-bdc1ea4bfef4.dcm\n",
            "cp /content/data/raw/tmp/681cf2ef-271a-45ab-a928-281dbad0d6eb.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/681cf2ef-271a-45ab-a928-281dbad0d6eb.dcm\n",
            "cp /content/data/raw/tmp/0cc8bc4a-d145-49f5-a5f7-c42517a6e2aa.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/0cc8bc4a-d145-49f5-a5f7-c42517a6e2aa.dcm\n",
            "cp /content/data/raw/tmp/82488010-4b3e-472f-9d9d-dd179734f9d6.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/82488010-4b3e-472f-9d9d-dd179734f9d6.dcm\n",
            "cp /content/data/raw/tmp/72df21eb-faa7-4972-8b0e-1231188f1c70.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/72df21eb-faa7-4972-8b0e-1231188f1c70.dcm\n",
            "cp /content/data/raw/tmp/0d59ffff-6295-48bf-8973-ec7690cd5280.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/0d59ffff-6295-48bf-8973-ec7690cd5280.dcm\n",
            "cp /content/data/raw/tmp/75d6474c-ca38-42af-ad7e-74409e2a24f2.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/75d6474c-ca38-42af-ad7e-74409e2a24f2.dcm\n",
            "cp /content/data/raw/tmp/94cc2107-124a-457b-bdfd-103561a83596.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/94cc2107-124a-457b-bdfd-103561a83596.dcm\n",
            "cp /content/data/raw/tmp/2e0c0156-7cdf-4acc-8b86-0252e4938f20.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/2e0c0156-7cdf-4acc-8b86-0252e4938f20.dcm\n",
            "cp /content/data/raw/tmp/6fbf1a2b-f0a9-4ef0-9ffd-cde629faa699.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/6fbf1a2b-f0a9-4ef0-9ffd-cde629faa699.dcm\n",
            "cp /content/data/raw/tmp/6efb862c-3824-4db5-8069-85f1648236fe.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/6efb862c-3824-4db5-8069-85f1648236fe.dcm\n",
            "cp /content/data/raw/tmp/8ca5c658-d952-412d-8621-d450ef7624ea.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/8ca5c658-d952-412d-8621-d450ef7624ea.dcm\n",
            "cp /content/data/raw/tmp/22903be6-d58b-4fb0-ac86-35bcb5172a73.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/22903be6-d58b-4fb0-ac86-35bcb5172a73.dcm\n",
            "cp /content/data/raw/tmp/6247430c-c9d1-497d-b590-5cbe34e8cd64.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/6247430c-c9d1-497d-b590-5cbe34e8cd64.dcm\n",
            "cp /content/data/raw/tmp/22947636-9609-494c-8e55-3ca0afa2457d.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/22947636-9609-494c-8e55-3ca0afa2457d.dcm\n",
            "cp /content/data/raw/tmp/0dde2ff4-7a02-4d5f-8787-db772a23d08f.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/0dde2ff4-7a02-4d5f-8787-db772a23d08f.dcm\n",
            "cp /content/data/raw/tmp/242a744b-7082-4820-bb48-4ab43592f7a9.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/242a744b-7082-4820-bb48-4ab43592f7a9.dcm\n",
            "cp /content/data/raw/tmp/1fe6dece-a9ce-4b01-9e6e-6540731cd5e4.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/1fe6dece-a9ce-4b01-9e6e-6540731cd5e4.dcm\n",
            "cp /content/data/raw/tmp/7e28f616-d63e-49bc-9495-d73abde5968a.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/7e28f616-d63e-49bc-9495-d73abde5968a.dcm\n",
            "cp /content/data/raw/tmp/42f041c1-09ae-46e3-873b-86af279b7b3f.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/42f041c1-09ae-46e3-873b-86af279b7b3f.dcm\n",
            "cp /content/data/raw/tmp/a3896b2c-f15b-4924-8564-9937715569bc.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/a3896b2c-f15b-4924-8564-9937715569bc.dcm\n",
            "cp /content/data/raw/tmp/557575f9-6d03-4478-9784-77222f3fca16.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/557575f9-6d03-4478-9784-77222f3fca16.dcm\n",
            "cp /content/data/raw/tmp/34df6c74-34c0-4f54-b234-639c7612d4a0.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/34df6c74-34c0-4f54-b234-639c7612d4a0.dcm\n",
            "cp /content/data/raw/tmp/8f9ca888-30d5-4c57-b144-58608b782fe9.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/8f9ca888-30d5-4c57-b144-58608b782fe9.dcm\n",
            "cp /content/data/raw/tmp/3090f764-4ac5-4fa3-9153-b58505be818b.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/3090f764-4ac5-4fa3-9153-b58505be818b.dcm\n",
            "cp /content/data/raw/tmp/78a0d9b3-2897-4bd1-911c-45f3a35e6433.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/78a0d9b3-2897-4bd1-911c-45f3a35e6433.dcm\n",
            "cp /content/data/raw/tmp/69a51a70-5f24-497a-9085-6cda97d2f1f8.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/69a51a70-5f24-497a-9085-6cda97d2f1f8.dcm\n",
            "cp /content/data/raw/tmp/287d8cfc-6703-49eb-a06c-8df195f03c5c.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/287d8cfc-6703-49eb-a06c-8df195f03c5c.dcm\n",
            "cp /content/data/raw/tmp/2f980ea0-e2c2-4bd0-8c0f-897ffb3208c1.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/2f980ea0-e2c2-4bd0-8c0f-897ffb3208c1.dcm\n",
            "cp /content/data/raw/tmp/50e1a9c4-6b11-4a0a-825b-ee684484c540.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/50e1a9c4-6b11-4a0a-825b-ee684484c540.dcm\n",
            "cp /content/data/raw/tmp/3b6c88b7-e599-412d-9335-70472d4cb80a.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/3b6c88b7-e599-412d-9335-70472d4cb80a.dcm\n",
            "cp /content/data/raw/tmp/9b761452-ada8-45f7-af5b-42981d9e9453.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/9b761452-ada8-45f7-af5b-42981d9e9453.dcm\n",
            "cp /content/data/raw/tmp/5fcbe8ef-d7a0-4687-a9ce-ff907e02a2a4.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/5fcbe8ef-d7a0-4687-a9ce-ff907e02a2a4.dcm\n",
            "cp /content/data/raw/tmp/8757c558-03c5-4ed3-8414-f3b8f4fea6b6.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/8757c558-03c5-4ed3-8414-f3b8f4fea6b6.dcm\n",
            "cp /content/data/raw/tmp/8b037b01-2a7d-4fdc-ac9e-66c6d0df08c3.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/8b037b01-2a7d-4fdc-ac9e-66c6d0df08c3.dcm\n",
            "cp /content/data/raw/tmp/08378817-e458-44a5-993f-3c92236504b9.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/08378817-e458-44a5-993f-3c92236504b9.dcm\n",
            "cp /content/data/raw/tmp/3f9d4d04-f1b0-4a8d-b7df-92442a86f1a1.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/3f9d4d04-f1b0-4a8d-b7df-92442a86f1a1.dcm\n",
            "cp /content/data/raw/tmp/556ff32b-6423-44eb-a7a5-2ce261fbfe00.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/556ff32b-6423-44eb-a7a5-2ce261fbfe00.dcm\n",
            "cp /content/data/raw/tmp/36252ca5-b222-4205-8c9d-a4421aa00ddb.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/36252ca5-b222-4205-8c9d-a4421aa00ddb.dcm\n",
            "cp /content/data/raw/tmp/0b2031c8-b097-4fd5-b920-069a5d4bc4ed.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/0b2031c8-b097-4fd5-b920-069a5d4bc4ed.dcm\n",
            "cp /content/data/raw/tmp/981ba98c-b6aa-419a-a5c9-b247c0e6e4ef.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/981ba98c-b6aa-419a-a5c9-b247c0e6e4ef.dcm\n",
            "cp /content/data/raw/tmp/6d858958-a09c-4481-ac32-dac8b41ed0c2.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/6d858958-a09c-4481-ac32-dac8b41ed0c2.dcm\n",
            "cp /content/data/raw/tmp/8a58bfa7-5112-4f20-8743-d17d846b8973.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/8a58bfa7-5112-4f20-8743-d17d846b8973.dcm\n",
            "cp /content/data/raw/tmp/5a67a5a4-953e-4376-94d2-4f1430692282.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/5a67a5a4-953e-4376-94d2-4f1430692282.dcm\n",
            "cp /content/data/raw/tmp/929c1675-7b89-42c6-8568-7618b990444f.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/929c1675-7b89-42c6-8568-7618b990444f.dcm\n",
            "cp /content/data/raw/tmp/8f4d3bc0-86ce-4247-804d-0213d955d7d1.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/8f4d3bc0-86ce-4247-804d-0213d955d7d1.dcm\n",
            "cp /content/data/raw/tmp/4fbb7a24-6f47-452f-b237-3fcda5b56dc3.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/4fbb7a24-6f47-452f-b237-3fcda5b56dc3.dcm\n",
            "cp /content/data/raw/tmp/367f1a54-598e-485d-a2f5-eb5fbcf8c480.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/367f1a54-598e-485d-a2f5-eb5fbcf8c480.dcm\n",
            "cp /content/data/raw/tmp/3af9902a-c210-4b87-abc2-96c48d0ea398.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/3af9902a-c210-4b87-abc2-96c48d0ea398.dcm\n",
            "cp /content/data/raw/tmp/79bb76fc-d0aa-4690-b024-1429459e114f.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/79bb76fc-d0aa-4690-b024-1429459e114f.dcm\n",
            "cp /content/data/raw/tmp/598e274a-fe25-4b31-9557-51141c754a88.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/598e274a-fe25-4b31-9557-51141c754a88.dcm\n",
            "cp /content/data/raw/tmp/8e3d67f8-8d5c-453f-92db-efb3b50f6151.dcm s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/8e3d67f8-8d5c-453f-92db-efb3b50f6151.dcm\n"
          ]
        }
      ],
      "source": [
        "# my_bucket_ct = \"s3://idc-medima-paper-dk/nnunet/nnunet_output/nlst/2d-tta/ct/\" \n",
        "my_bucket_ct = os.path.join(bucket_experiment_folder_uri_nnunet, 'ct/')\n",
        "\n",
        "# Get and create the download path \n",
        "download_path = \"/content/data/raw/tmp\"\n",
        "if not os.path.exists(download_path):\n",
        "  os.mkdir(download_path)\n",
        "\n",
        "# series_to_copy_list = list(set(cohort_df_subset[\"SeriesInstanceUID\"].to_list()))\n",
        "series_to_copy_list = series_id_list\n",
        "print (len(series_to_copy_list))\n",
        "\n",
        "for idx, series_id in enumerate(series_to_copy_list):\n",
        "\n",
        "  print (idx)\n",
        "  if not os.path.exists(download_path):\n",
        "    os.mkdir(download_path)\n",
        "\n",
        "  series_df = cohort_df[cohort_df[\"SeriesInstanceUID\"] == series_id]\n",
        "\n",
        "  gcsurl_temp = \"cp \" + series_df[\"gcs_url\"].str.replace(\"gs://\",\"s3://\") + \" \" + download_path \n",
        "  gs_file_path = \"gcs_paths_s3.txt\"\n",
        "  gcsurl_temp.to_csv(gs_file_path, header = False, index = False)\n",
        "\n",
        "  # Download using s5cmd \n",
        "  start_time = time.time()\n",
        "  download_cmd = [\"/content/s5cmd\",\"--endpoint-url\", \"https://storage.googleapis.com\", \"run\", gs_file_path]\n",
        "  proc = subprocess.Popen(download_cmd)\n",
        "  proc.wait()\n",
        "  elapsed = time.time() - start_time \n",
        "  print (\"Done download in %g seconds.\"%elapsed)\n",
        "\n",
        "  # need to run below.\n",
        "  !/content/s5cmd --endpoint-url https://storage.googleapis.com cp $download_path/ $my_bucket_ct/\n",
        "\n",
        "  !rm -rf $download_path/*\n",
        "  # shutil.rmtree(download_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "kGV2FT4XbjoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8304e3f1-97da-4f91-8c54-120651067f33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idc-medima-paper-dk/nnunet/nnunet_output/nlst_25_series_06_16_22/3d_fullres-tta/ct/\n",
            "Request issued for: [datastore_nlst_25_series_06_16_22]\n",
            "name: projects/idc-external-018/locations/us-central1/datasets/dataset_nlst/dicomStores/datastore_nlst_25_series_06_16_22\n"
          ]
        }
      ],
      "source": [
        "# Save corresponding CT files to the dicom-store from bucket\n",
        "\n",
        "# my_bucket_ct_gs = \"idc-medima-paper-dk/nnunet/nnunet_output/nlst/2d-tta/ct/\" \n",
        "my_bucket_ct_gs = my_bucket_ct.split('s3://')[1] \n",
        "print(my_bucket_ct_gs)\n",
        "!gcloud healthcare dicom-stores import gcs $datastore_id \\\n",
        "                                            --dataset=$dataset_id \\\n",
        "                                            --location=$location_id \\\n",
        "                                            --project=$project_name \\\n",
        "                                            --gcs-uri=gs://$my_bucket_ct_gs**.dcm"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RkKOcbyxs__u",
        "CZnoRi9Y7nEB",
        "P_Z2S0OdAtkz"
      ],
      "machine_shape": "hm",
      "name": "idc_nlst_nnunet_and_bpr-infer.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}